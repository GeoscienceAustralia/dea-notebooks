{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Vegetation Anomalies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Understanding how the vegetated landscape responds to longer-term environmental drivers such as the El Nino Southern Oscillation (ENSO) or climate change, requires the calculation of seasonal anomalies. Seasonal anomalies subtract the long-term seasonal mean from a time-series, thus removing seasonal variability and highlighting change related to longer-term drivers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook will calculate the seasonal anomaly for any given season and year. The long-term seasonal climatologies (both mean and standard deviation) for the vegetation index `NDVI` have been pre-calculated and are stored on disk. Given an AOI, season, and year, the script will calculate the seasonal mean for one of these indices and subtract the seasonal mean from the long-term climatology, resulting in a map of standardised vegetation anomalies for your AOI.  Optionally, the script will output a geotiff of the result. \n",
    "\n",
    "**IMPORTANT NOTES:** \n",
    "\n",
    "* It is a convention to establish climatologies based on a 30-year time range to account for inter-annual and inter-decadal modes of climate variability (often 1980-2010). As the landsat archive only goes back to 1987, the climatologies here have been calculated using the date-range `1988 - 2010` (inclusive).  While this is not ideal, a 22-year climatology should suffice to capture the bulk of inter-annual and inter-decadal variability, for example, both a major El Nino (1998) and a major La Nina (2010) are captured by this time-range.\n",
    "\n",
    "* Files & scripts for running datacube stats to calculate vegetation climatologies are located here: `'/g/data/r78/cb3058/dea-notebooks/vegetation_anomlies/dcstats'`. \n",
    "\n",
    "* The pre-computed climatologies are stored here: `/g/data/r78/cb3058/dea-notebooks/vegetation_anomalies/results/NSW_NDVI_climatologies_<mean>`.  The script below will use this string location to grab the data, so shifting the climatology mosaics to another location will require editing the `anomalies.py` script.\n",
    "\n",
    "* So far, NDVI climatolgies have been produced for the full extent of NSW only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical details\n",
    "\n",
    "* **Products used:** 'ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To run this analysis, go to the `Analysis Parameters` section and enter the relevant details, then run all the cells in the notebook. If running the analysis multiple times, only run the `Set up dask cluster` and `import libraries` cells once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.utils.cog import write_cog\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import os\n",
    "import datacube\n",
    "\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_plotting import display_map, map_shapefile\n",
    "from anomalies import calculate_anomalies\n",
    "from dea_datahandling import load_ard\n",
    "from dea_dask import create_local_dask_cluster\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up local dask cluster\n",
    "\n",
    "Dask will create a local cluster of cpus for running this analysis in parallel. If you'd like to see what the dask cluster is doing, click on the hyperlink that prints after you run the cell and you can watch the cluster run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44569</li>\n",
       "  <li><b>Dashboard: </b><a href='/proxy/8787/status' target='_blank'>/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>30.06 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44569' processes=1 threads=8, memory=30.06 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "The following cell sets the parameters, which define the area of interest and the season to conduct the analysis over. The parameters are:\n",
    "\n",
    "* `shp_fpath`: Provide a filepath to a shapefile that defines your AOI, if not using a shapefile then put `None` here.\n",
    "* `lat`, `lon`, `buffer`: If not using a shapefile to define the AOI, then use a latitide, longitude, and buffer to define a query 'box'.\n",
    "* `collection`: The landsat collection to load data from. either `'c3'` or `'c2'`\n",
    "* `year`: The year of interest, e.g. `'2018'`\n",
    "* `season`:  The season of interest, e.g `'DJF'`,`'JFM'`, `'FMA'` etc\n",
    "* `name` : A string value used to name the output geotiff, e.g 'NSW'\n",
    "* `dask_chunks` : dictionary of values to chunk the data using dask e.g. `{'x':3000, 'y':3000}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_fpath = \"data/NSW_and_ACT.shp\" \n",
    "# shp_fpath=\"/home/156/jbw156/goFARM property boundaries for NSW DPI/goFARM property boundaries for NSW DPI/Petro Station.shp\"\n",
    "lat, lon, buff = -34.958, 149.281, 1\n",
    "collection =  'c3'\n",
    "year = '2020'\n",
    "season = 'ASO'\n",
    "name='NSW'\n",
    "dask_chunks = {'x':3000, 'y':3000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine your area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_shapefile(gpd.read_file(shp_fpath), attribute='NSW_STAT_1')\n",
    "map_shapefile(gpd.read_file(shp_fpath), attribute='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_map(y=(lat-buff, lat + buff), x=(lon-buff, lon + buff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the anomaly for the AOI\n",
    "\n",
    "For large queries (e.g > 10,000 x 10,000 pixels), the code will take several minutes to run.  Queries larger than ~25,000 x 25,000 pixels may start to fail due to memory limitations (several (42,000 x 35,000 x 52) runs covering all of NSW has been successfully run on the VDI). Check the x,y dimensions in the lazily loaded output to get idea of how big your result will be before you run  the `.compute()` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data based on shapefile extent\n",
      "Finding datasets\n",
      "    ga_ls5t_ard_3\n",
      "    ga_ls7e_ard_3 (ignoring SLC-off observations)\n",
      "    ga_ls8c_ard_3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Lazily run calculations, this will check for errors before\n",
    "# we actually compute the results\n",
    "anomalies,obs = calculate_anomalies(shp_fpath=shp_fpath,\n",
    "                                query_box=(lat,lon,buff),\n",
    "                                collection=collection,\n",
    "                                year=year,\n",
    "                                season=season,\n",
    "                                dask_chunks=dask_chunks)\n",
    "\n",
    "print(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anomalies = anomalies.compute()\n",
    "# Write geotiff to a location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_geotiff('results/computed_anomalies/ndvi_' +year+season+'_'+name+'_standardised_anomalies.tif', anomalies)\n",
    "anomaliesA=anomalies.to_array()\n",
    "if collection == 'c2':\n",
    "    write_cog(geo_im=anomaliesA,fname='results/computed_anomalies/ndvi_c2_' +year+season+'_'+name+'_standardised_anomalies.tif', overwrite=True)\n",
    "if collection == 'c3':\n",
    "    write_cog(geo_im=anomaliesA,fname='results/computed_anomalies/ndvi_c3_' +year+season+'_'+name+'_standardised_anomalies.tif', overwrite=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export geotiff and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2=pd.DataFrame(meta[1])\n",
    "lastobs=obs.to_dataframe()\n",
    "lastobs.to_csv('results/computed_anomalies/'+year+season+'_NDVIanom_timestamps.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result\n",
    "\n",
    "If your AOI is very large, plotting the result can crash the notebook. In that case, its better to export the geotiff and view it in QGIS or ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalies.std_anomalies.plot(figsize=(10,10), vmin=-2.0, vmax=2.0, cmap='BrBG')\n",
    "\n",
    "# plt.title(season+ \", \" +year)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Seasonal_anomalies.ipynb to script\n",
      "[NbConvertApp] Writing 6991 bytes to Seasonal_anomalies.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script Seasonal_anomalies.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
