{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waterbody clustering\n",
    "\n",
    "This notebook investigates the clustering of waterbodies based on their time series surface areas and other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPython.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import fiona\n",
    "import numpy as np\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance\n",
    "import scipy.ndimage\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_shp_path = Path('/g/data/r78/cek156/dea-notebooks/Scientific_workflows/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodiesFINAL.shp')\n",
    "waterbody_csv_path = Path('/g/data/r78/cek156/dea-notebooks/Scientific_workflows/DEAWaterbodies/timeseries_aus_uid/')\n",
    "surface_area_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_shapes = gpd.read_file(waterbody_shp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "area                                                     11875\n",
       "perimeter                                              549.791\n",
       "UID                                                  q9cusmx7n\n",
       "FID                                                          0\n",
       "geometry     POLYGON ((-1538525.000000001 -3849499.99999999...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterbody_shapes.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an area of interest to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = gpd.GeoDataFrame(geometry=gpd.points_from_xy((142.1246, 149.1300), (-37.0161, -34.2801)))  # Mildura -> Canberra, Seymour -> Griffith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox.crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, y_min, x_max, y_max = bbox.to_crs('EPSG:3577').total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_shapes_ = waterbody_shapes.cx[x_min:x_max, y_min:y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295906 waterbodies total\n",
      "12535 in Mildura/Seymour/Canberra/Griffith area\n"
     ]
    }
   ],
   "source": [
    "print(len(waterbody_shapes), 'waterbodies total')\n",
    "print(len(waterbody_shapes_), 'in Mildura/Seymour/Canberra/Griffith area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_shapes = waterbody_shapes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join these with the BOM river regions. I grabbed these from the v2.1.1 Geofabric Reporting Regions and converted them from gdb + WGS84 to GeoJSON + Australian Albers in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverregions = gpd.read_file('bom_riverregions_v2p1p1.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_shapes = gpd.sjoin(waterbody_shapes, riverregions, how='left', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4442c8a9634d40fba68f439fcd99c679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_time_series = []\n",
    "for i, shape in tqdm(waterbody_shapes.iterrows(), total=len(waterbody_shapes)):\n",
    "    uid = shape.UID\n",
    "    csv_path = waterbody_csv_path / uid[:4] / f'{uid}.csv'\n",
    "    time_series = pd.read_csv(csv_path)\n",
    "    # Relabel the third column to something consistent, and rename all columns to something\n",
    "    # easier to access.\n",
    "    time_series.rename(columns={\n",
    "        'Observation Date': 'date',\n",
    "        'Wet pixel percentage': 'pc_wet',\n",
    "        time_series.columns[2]: 'px_wet',\n",
    "        }, inplace=True)\n",
    "    # Convert time strings into datetimes.\n",
    "    time_series.date = pd.to_datetime(time_series.date)\n",
    "    # Store the actual number of pixels too.\n",
    "    n_pixels = shape.geometry.area // (25 ** 2)\n",
    "    time_series.attrs['px_tot'] = n_pixels  # attrs is experimental.\n",
    "    all_time_series.append(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies = waterbody_shapes.set_index('UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012a164dec034b37a0579c6e06e412e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9c172aae48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = plt.figure(figsize=(10, 5)).add_subplot(1, 1, 1)\n",
    "waterbodies.plot(ax=ax, column='RivRegNum', cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_time_series) == len(waterbody_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to remove entries with NaN water levels (presumably cloud or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a29c044a170489e82780742e9865cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_time_series_ = []\n",
    "for t in tqdm(all_time_series):\n",
    "    nans = t.px_wet.isnull()\n",
    "    t = t[~nans].reset_index(drop=True)\n",
    "    all_time_series_.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = all_time_series_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies['water_history'] = all_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the rivers are throwing a spanner in the works a bit, and while the big lakes take up a *lot* of area we don't really care about them. We want to see dams, small lakes, and ponds! Let's use the Surface Hydrology Network to remove rivers. Claire has previously used this to remove major rivers but this led to inconsistent results where some large lakes were removed because they were part of the water network. However, in this case I don't actually care about those either: if they are rivers then they are gone, and lakes like Lake Hume should go too. We can always add them back in later (e.g. using an area threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HydroLines']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiona.listlayers('SurfaceHydrologyLinesNational.gdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = gpd.read_file('SurfaceHydrologyLinesNational.gdb', layer='HydroLines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.to_crs('EPSG:3577')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.cx[x_min:x_max, y_min:y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "watercourses = lines['FEATURETYPE'] == 'Watercourse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we strip everything that intersects with a watercourse, how much of our data does that remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = gpd.sjoin(waterbodies.drop(columns='index_right'), lines[watercourses], how='inner', op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.55% of waterbodies intersect watercourses\n"
     ]
    }
   ],
   "source": [
    "print('{:.02%} of waterbodies intersect watercourses'.format(joined.index.unique().shape[0] / waterbodies.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! But it's also consistent with the idea that the non-river waterbodies are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be7892cd44f4f4f951f607cd9c24cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9b7e730f28>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does a pretty good job of pulling out rivers (and lakes that are made from dammed rivers). What's left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_river = waterbodies.index.isin(joined.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2755484642999601"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_river.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies_not_river = waterbodies[~yes_river]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8396473288c1425db5ae9d3af18eaea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9b7d6349e8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterbodies_not_river.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of remaining waterbodies resemble rivers, but I'm fairly sure that these are billabongs and similar, which are particularly prevalent along the Murray River."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies_including_rivers = waterbodies[yes_river]\n",
    "waterbodies = waterbodies[~yes_river]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies = waterbodies.reset_index().set_index('UID')\n",
    "waterbodies_including_rivers = waterbodies_including_rivers.reset_index().set_index('UID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies.drop(columns='water_history').to_file('waterbodies_murray_norivers.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies_including_rivers.drop(columns='water_history').to_file('waterbodies_murray_onlyrivers.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define some kind of distance between two water level time series (henceforth \"water histories\"). These have different x values and lengths. A dilemma! One option is to interpolate so everything is the same length. We could also have some distance function that doesn't require the same x values. The former is simpler, and lets us use all our favourite distance measures, including all vector distances (e.g. cosine, Euclidean, Pearson correlation...) but requires assumptions on water behaviour. It also requires preprocessing the data to the same time steps, which will at minimum greatly increase the memory usage. The latter runs the risk of being slower. One option for the latter is dynamic time warping distance, but this requires a quadratic DP for each pair and can be pretty slow as a result, especially when there are many data points in each time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by interpolating to a common grid. How many elements should that grid have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4375f7e8a074c4c98a4ae28f16f68b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dates = set()\n",
    "for history in tqdm(waterbodies.water_history):\n",
    "    dates |= set(history.date.dt.round('1d').values.astype('datetime64[D]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of observations per waterbody: 666.9769849135557\n"
     ]
    }
   ],
   "source": [
    "print('average number of observations per waterbody:', waterbodies.water_history.map(lambda a: len(a)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique dates: 5253\n"
     ]
    }
   ],
   "source": [
    "print('unique dates:', len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.datetime64('1986-08-18'), numpy.datetime64('2020-07-19'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each water history we can add in all the dates between the first and most recent observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(min(dates), max(dates), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12389"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516b2453ca194147a85723abb33a9a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First round every date and set date to be the index.\n",
    "# Note that we also have to drop the timezone, which pandas assumes is UTC.\n",
    "# If pandas did not assume it was UTC - maybe it assumed UTC+11 for example - then this would also do\n",
    "# a conversion into UTC, which is probably not what we want.\n",
    "for history in tqdm(waterbodies.water_history):\n",
    "    history.date = history.date.dt.round('1d')\n",
    "    history.set_index('date', drop=True, inplace=True)\n",
    "    history.index = history.index.tz_convert(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_index = pd.DatetimeIndex(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280c8df3f4344ef88204362fc6e187f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "histories = []  # Storing reindexed dataframes back directly in waterbodies leads to some super bizarre behaviour where they are replaced entirely by nans.\n",
    "# So, storing them in a list instead.\n",
    "for i in tqdm(range(len(waterbodies))):\n",
    "    # Merge duplicate dates into one.\n",
    "    history = waterbodies.water_history[i].groupby('date').mean()\n",
    "    # Then reindex with the full list of dates.\n",
    "    histories.append(history.reindex(dt_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies.water_history = histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the water histories now having the same time index, they are all aligned. We now need to handle the lack of measurements at some times, and we will do this by linear interpolation as it is the least information thing we can do (besides setting them to the last observed value, which feels unphysical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd52949202f446edaea7a368aa9ca585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for history in tqdm(waterbodies.water_history):\n",
    "    history.interpolate(limit_direction='both', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is aligned! Put everything into a matrix, treating every time observation as an independent feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_matrix = np.zeros((len(waterbodies), len(dt_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b28a9ee89449338172a63673c17cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, history in tqdm(enumerate(waterbodies.water_history)):\n",
    "    history_matrix[i] = history.pc_wet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_matrix = np.nan_to_num(history_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('12388 days 00:00:00')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_index.max() - dt_index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's downsample this because we really don't need 5000+ time entries. We have 12000 days of data, which is 1700 weeks, so let's downsample by 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_matrix_original = history_matrix\n",
    "history_matrix_zoomed = scipy.ndimage.zoom(history_matrix, (1, 1 / 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_zoomed = scipy.ndimage.zoom(dt_index.values.astype('datetime64[D]').astype(int), 1 / 7).astype('datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df_original = gpd.GeoDataFrame(history_matrix_original, columns=dt_index, index=waterbodies.index, geometry=waterbodies.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df_zoomed = gpd.GeoDataFrame(history_matrix_zoomed, columns=dt_zoomed, index=waterbodies.index, geometry=waterbodies.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('time_axis_murray_zoomed_norivers.npy', dt_zoomed)\n",
    "np.save('time_axis_murray_full_norivers.npy', dt_index)\n",
    "np.save('history_murray_full_norivers.npy', history_matrix_original)\n",
    "np.save('history_murray_zoomed_norivers.npy', history_matrix_zoomed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring a dataset, it's always good to start with PCA! The first component is the mean, which is worth looking at regardless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5a17618eeb474683bbd04f72a9787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mean percentage of maximum extent')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "mean = np.mean(history_matrix, axis=0)\n",
    "std = np.std(history_matrix, axis=0)\n",
    "plt.plot(dt_index, mean, c='black')\n",
    "plt.fill_between(dt_index, mean - std, mean + std, color='black', alpha=0.2)\n",
    "# for d in dt_index[dt_index.month == 1]:\n",
    "#     plt.axvline(d, alpha=0.01, c='black')\n",
    "\n",
    "def plot_la_nina_el_nino():\n",
    "    for la_nina_from, la_nina_to in [('2010-04', '2012-03'), ('2008-08', '2009-04'), ('2007-06', '2008-02'), ('1998-05', '2001-03'), ('1988-04', '1989-07')]:\n",
    "        plt.axvspan(np.datetime64(la_nina_from), np.datetime64(la_nina_to), color='blue', alpha=0.2)\n",
    "    for el_nino_from, el_nino_to in [('2015-04', '2016-04'), ('2009-05', '2010-03'), ('2006-05', '2007-01'), ('2002-03', '2003-01'), ('1997-04', '1998-03'),\n",
    "                                     ('1994-03', '1995-01'), ('1993-04', '1994-02'), ('1991-03', '1991-11'), ('1987-05', '1988-03')]:\n",
    "        plt.axvspan(np.datetime64(el_nino_from), np.datetime64(el_nino_to), color='red', alpha=0.2)\n",
    "plot_la_nina_el_nino()\n",
    "    \n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean percentage of maximum extent')\n",
    "# plt.xlim(np.datetime64('2008-01'), np.datetime64('2012-01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've highlighted the dates in January in the dataset: these have lower water levels on average, which makes sense for the middle of summer in NSW and Victoria. I've also highlighted La Niña and El Niño events in blue and red respectively. They are weakly correlated with significant increases and decreases in mean water level. In particular, the very strong 2010-2012 La Niña corresponds with a particularly large increase in average water extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=50)\n",
    "pca_f = pca.fit_transform(history_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbodies.loc[waterbodies.RivRegNum.isnull(), 'RivRegNum'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfd066b6025438d912788d7b5e1c93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9b83f814e0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(pca_f[:, 0], pca_f[:, 1], s=2, edgecolor='None', c=waterbodies.RivRegNum.astype(int), cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no obvious correlations in 2-PCA-space. Let's try t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "\n",
    "tsne = sklearn.manifold.TSNE(verbose=True, perplexity=50)\n",
    "\n",
    "tsne_f = tsne.fit_transform(pca_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = dict(zip(waterbodies.RivRegNum.astype(int), waterbodies.RivRegName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "xs = np.arange(min(names), max(names))\n",
    "plt.scatter(tsne_f[:, 0], tsne_f[:, 1], s=(waterbodies.area / 0.5e3) ** 0.5,\n",
    "            edgecolor='None', c=waterbodies.RivRegNum.astype(int), cmap='tab20', norm=matplotlib.colors.BoundaryNorm(xs, len(xs) + 1))\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks(xs + 0.5)\n",
    "cb.set_ticklabels([names.get(i, '') for i in xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does have some obvious substructure, particularly when we colour it by position. But clustering results in mostly useless clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have been exported already, so we are good to try and cluster in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
