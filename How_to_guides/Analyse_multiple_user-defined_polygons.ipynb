{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and run analysis on multiple polygons <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatability:** Notebook currently compatible with both the `NCI` and `DEA Sandbox` environments\n",
    "* **Products used:** \n",
    "[ga_ls8c_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls8c_ard_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Many users need to run analyses on their own areas of interest. \n",
    "A common use case involves running the same analysis across multiple polygons in a vector file (e.g. ESRI Shapefile or GeoJSON). \n",
    "This notebook will demonstrate how to use a vector file and the Open Data Cube to extract satellite data from Digital Earth Australia corresponding to individual polygon geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "If we have a vector file containing multiple polygons, we can use the python package [geopandas](https://geopandas.org/) to open it as a `GeoDataFrame`. \n",
    "We can then iterate through each geometry and extract satellite data corresponding with the extent of each geometry. \n",
    "Further anlaysis can then be conducted on each resulting `xarray.Dataset`.\n",
    "\n",
    "We can retrieve data for each polygon, perform an analysis like calculating NDVI and plot the data.\n",
    "\n",
    "1. First we open the vector file as a `geopandas.GeoDataFrame`\n",
    "2. Iterate through each polygon in the `GeoDataFrame`, and extract satellite data from DEA\n",
    "3. Calculate NDVI as an example analysis on one of the extracted satellite timeseries\n",
    "4. Plot NDVI for the polygon extent\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Please note the use of `datacube.utils` package `geometry`: \n",
    "this is important for saving the coordinate reference system of the incoming shapefile in a format that the Digital Earth Australia query can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rioxarray in /env/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: scipy in /env/lib/python3.6/site-packages (from rioxarray) (1.5.3)\n",
      "Requirement already satisfied: pyproj>=2.2 in /env/lib/python3.6/site-packages (from rioxarray) (2.6.1.post1)\n",
      "Requirement already satisfied: xarray in /env/lib/python3.6/site-packages (from rioxarray) (0.16.1)\n",
      "Requirement already satisfied: rasterio in /env/lib/python3.6/site-packages (from rioxarray) (1.1.6)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /env/lib/python3.6/site-packages (from scipy->rioxarray) (1.18.5)\n",
      "Requirement already satisfied: setuptools>=38.4 in /env/lib/python3.6/site-packages (from xarray->rioxarray) (50.3.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /env/lib/python3.6/site-packages (from xarray->rioxarray) (1.1.2)\n",
      "Requirement already satisfied: click<8,>=4.0 in /env/lib/python3.6/site-packages (from rasterio->rioxarray) (7.1.2)\n",
      "Requirement already satisfied: affine in /env/lib/python3.6/site-packages (from rasterio->rioxarray) (2.3.0)\n",
      "Requirement already satisfied: click-plugins in /env/lib/python3.6/site-packages (from rasterio->rioxarray) (1.1.1)\n",
      "Requirement already satisfied: attrs in /env/lib/python3.6/site-packages (from rasterio->rioxarray) (20.2.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /env/lib/python3.6/site-packages (from rasterio->rioxarray) (0.7.0)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /env/lib/python3.6/site-packages (from rasterio->rioxarray) (1.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /env/lib/python3.6/site-packages (from pandas>=0.25->xarray->rioxarray) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /env/lib/python3.6/site-packages (from pandas>=0.25->xarray->rioxarray) (2.8.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /env/lib/python3.6/site-packages (from snuggs>=1.4.1->rasterio->rioxarray) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /env/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.25->xarray->rioxarray) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the rioxarray if required\n",
    "\n",
    "!pip install rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "import rasterio.crs\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils import geometry\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "# from dea_datahandling import load_ard\n",
    "from dea_bandindices import calculate_indices\n",
    "from dea_plotting import rgb, map_shapefile\n",
    "from dea_temporaltools import time_buffer\n",
    "from dea_spatialtools import xr_rasterize\n",
    "\n",
    "from dea_coastaltools import tidal_tag\n",
    "from dea_coastaltools import tidal_stats\n",
    "\n",
    "## Packages from the polygon labelling workflow (Sentinel_2_Cloud_Labelling.ipynb)\n",
    "\n",
    "# %matplotlib widget\n",
    "\n",
    "import functools\n",
    "import os\n",
    "# import sys\n",
    "# import datacube\n",
    "from datacube.storage.masking import make_mask\n",
    "import datacube.utils.cog\n",
    "# import geopandas as gpd\n",
    "import ipyleaflet\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.cm\n",
    "import matplotlib.colors\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import odc.ui\n",
    "from odc.ui import with_ui_cbk\n",
    "import rasterio.features\n",
    "import rioxarray\n",
    "from shapely.geometry import shape\n",
    "import skimage.color as colour\n",
    "import skimage.io\n",
    "import sklearn.metrics\n",
    "from tqdm.notebook import tqdm\n",
    "import xarray\n",
    "import pickle\n",
    "\n",
    "# sys.path.append(\"../Scripts\")\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_datahandling import load_ard, array_to_geotiff\n",
    "from dea_plotting import display_map\n",
    "# from dea_plotting import rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Connect to the datacube database to enable loading Digital Earth Australia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:43779</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/cp/proxy/8787/status' target='_blank'>/user/cp/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>14.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:43779' processes=1 threads=2, memory=14.18 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc = datacube.Datacube(app='Analyse_multiple_polygons')\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `time_of_interest` : Enter a time, in units YYYY-MM-DD, around which to load satellite data e.g. `'2019-01-01'`\n",
    "* `time_buff` : A buffer of a given duration (e.g. days) around the time_of_interest parameter, e.g. `'30 days'`\n",
    "* `vector_file` : A path to a vector file (ESRI Shapefile or GeoJSON)\n",
    "* `attribute_col` : A column in the vector file used to label the output `xarray` datasets containing satellite images. Each row of this column should have a unique identifier\n",
    "* `products` : A list of product names to load from the datacube e.g. `['ga_ls7e_ard_3', 'ga_ls8c_ard_3']`\n",
    "* `measurements` : A list of band names to load from the satellite product e.g. `['nbart_red', 'nbart_green']`\n",
    "* `resolution` : The spatial resolution of the loaded satellite data e.g. for Landsat, this is `(-30, 30)`\n",
    "* `output_crs` : The coordinate reference system/map projection to load data into, e.g. `'EPSG:3577'` to load data in the Albers Equal Area projection\n",
    "* `align` : How to align the x, y coordinates respect to each pixel. Landsat Collection 3 should be centre aligned `align = (15, 15)` if data is loaded in its native UTM zone projection, e.g. `'EPSG:32756'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the general query\n",
    "\n",
    "\n",
    "time = ('2013-01-01', '2020-08-01')\n",
    "products= ['ga_ls8c_ard_3']\n",
    "bands = ['nbart_red', 'nbart_green', 'nbart_blue', 'nbart_nir', 'nbart_swir_1']\n",
    "resolution = (-30, 30)\n",
    "output_crs = 'EPSG:3577'\n",
    "align = (0, 0)\n",
    "\n",
    "# Query\n",
    "query = {\n",
    "#     'x': lon, # Use query.update({str(key):variable}) for the classification coords\n",
    "#     'y': lat,\n",
    "    'time': time,\n",
    "    'measurements' : bands,\n",
    "    'output_crs': 'EPSG:3577',\n",
    "    'resolution': (-30, 30),\n",
    "    'group_by': 'solar_day'\n",
    "}\n",
    "\n",
    "# Designate dask chunks\n",
    "# It doesn't really matter how big the chunks we load are, as long as time ~ 1.\n",
    "chunks = {\"time\": 1, \"x\": 3000, \"y\": 3000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice: polygon/class source\n",
    "* ### If they are provided then use the *Predetermined polygons* workflow below\n",
    "* ### If they are to be user nominated then use the *Polygon classification* workflow below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Predetermined polygons*\n",
    "User defines:\n",
    "* vector polygon file\n",
    "\n",
    "Automatically defined:\n",
    "* coords (from vector file extents)\n",
    "* class ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this cell with predetermined vector file sets\n",
    "\n",
    "# vector_file = '../Supplementary_data/Analyse_multiple_polygons/multiple_polys.shp'\n",
    "vector_file = 'QISMCQ_polygons_testarea.shp'\n",
    "attribute_col = 'id'\n",
    "\n",
    "# Read in the polygon vector file\n",
    "gdf = gpd.read_file(vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attribute values for each class are as follows: [['Intertidal grass-herb-sedge-other succulent', 0], ['Intertidal mangroves and other trees & shrubs', 1], ['Intertidal consolidated substrate', 2], ['Intertidal seagrass', 3], ['Intertidal unconsolidated substrate', 4]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301f6c2e438f4ed984e0e6cc25418bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9df9ab2dba49309b0c78dad32b5ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.085816122499978, 151.55457361750007], controls=(ZoomControl(options=['position', 'zoom_in_text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attribute each class with an integer value\n",
    "val = (gdf['BRD_HAB'].unique()).tolist()\n",
    "\n",
    "num_list = []\n",
    "attr_key = []\n",
    "\n",
    "d = 0\n",
    "for x in range (len(gdf)):\n",
    "    for d in range (len(val)):        \n",
    "        if gdf['BRD_HAB'].values[x] == str(val[d]):\n",
    "            num_list.append(d)\n",
    "        # Create a key to interpret the integer attribute for each class\n",
    "        for y in num_list:\n",
    "            if y not in attr_key:\n",
    "                    attr_key.append(y)\n",
    "\n",
    "val = [[el] for el in val]\n",
    "for x in attr_key:\n",
    "    val[x].append(attr_key[x])\n",
    "        \n",
    "print ('The attribute values for each class are as follows: ' + str(val))\n",
    "\n",
    "# Update the geodataframe of vector polygons with the integer attribution for each class\n",
    "gdf['id'] = num_list\n",
    "\n",
    "# Map the shapefiles from imported vector set\n",
    "map_shapefile(gdf, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 1/394\n",
      "Intertidal grass-herb-sedge-other succulent\n",
      "0\n",
      "OBJECTID                                                     11\n",
      "CONSOL                                                        C\n",
      "DOM_TYPE                                                      1\n",
      "DOM_LABEL                   Grass-herb-sedge (undifferentiated)\n",
      "CO_TYPES                                                   None\n",
      "TIDE_ZONE                                            Intertidal\n",
      "BRD_HAB             Intertidal grass-herb-sedge-other succulent\n",
      "Shape_Leng                                           0.00706932\n",
      "Shape_Area                                          1.27758e-06\n",
      "geometry      POLYGON ((151.5643725000001 -24.12794199999996...\n",
      "id                                                            0\n",
      "Name: 0, dtype: object\n",
      "Finding datasets\n",
      "    ga_ls8c_ard_3\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 159 time steps\n"
     ]
    }
   ],
   "source": [
    "# Load data for predetermined polygons - may need some debugging to correspond to above cell\n",
    "\n",
    "# Dictionary to save results \n",
    "results = {}\n",
    "# results2 = {}\n",
    "\n",
    "# Loop through polygons in geodataframe and extract satellite data\n",
    "for index, row in gdf.iterrows():\n",
    "    \n",
    "    print(f'Feature: {index + 1}/{len(gdf)}')\n",
    "    print (gdf['BRD_HAB'].values[index])\n",
    "    print (str(index))\n",
    "    print (str(row))\n",
    "    \n",
    "    if not (str(row[attribute_col]) in results.keys()):\n",
    "        results[str(row[attribute_col])] = {}\n",
    "    \n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    \n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({'geopolygon': geom}) \n",
    "    \n",
    "    # Load landsat\n",
    "    ds = load_ard(dc=dc, \n",
    "                  products=products,\n",
    "                  # min_gooddata=0.99,  # only take uncloudy scenes\n",
    "                  ls7_slc_off = False,                  \n",
    "                  **query)\n",
    "    \n",
    "            ## Tidally tag datasets\n",
    "    ds = tidal_tag(ds, ebb_flow=True)\n",
    "    \n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "    \n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    ds = ds.where(mask)\n",
    "    \n",
    "    # Append results to a dictionary using the attribute\n",
    "    # column as an key\n",
    "#     results.update({str(row[attribute_col]): ds}) ## Original. I think it only saves one polygon per class\n",
    "#     results.update({str(row[attribute_col]):{str(index): ds}}) # New dict of dicts with each polygon recorded under each class.\n",
    "    results[str(row[attribute_col])][str(index)] = ds\n",
    "    \n",
    "#         ## Tidally tag datasets\n",
    "#     ds1 = tidal_tag(ds, ebb_flow=True)\n",
    "    \n",
    "#         # Generate a polygon mask to keep only data within the polygon\n",
    "#     mask1 = xr_rasterize(gdf.iloc[[index]], ds1)\n",
    "    \n",
    "#         # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "#     ds1 = ds1.where(mask1)\n",
    "    \n",
    "#     # Append results to a dictionary using the attribute column as key\n",
    "#     results2[str(row[attribute_col])][str(index)] = ds1 \n",
    "\n",
    "    print (row[attribute_col], index)\n",
    "    print ('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results dict (optional)\n",
    "\n",
    "\n",
    "try:\n",
    "    resultsdict = open('resultsdict _' + str(time[0]), 'wb')\n",
    "    pickle.dump(results, resultsdict)\n",
    "    resultsdict.close()\n",
    "# except:\n",
    "#     print(\"something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Open (pickled) results dict (optional)\n",
    "\n",
    "# pickle.load('resultsdict _' + str(time[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Polygon classification*\n",
    "User defines:\n",
    "* coords\n",
    "* dataset_name for file naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the coordinate extent for the area of interest\n",
    "\n",
    "# # Gulf of Carpentaria\n",
    "# # lon = (138.8998, 139.0989)\n",
    "# # lat = (-16.8341, -16.9264)\n",
    "\n",
    "# # Queensland East\n",
    "# lon = (150.65, 150.8)\n",
    "# lat = (-22.457, -22.654)\n",
    "\n",
    "# # Update the query\n",
    "# query.update({'x': lon})\n",
    "# query.update({'y': lat})\n",
    "\n",
    "# # Filename for saved datasets\n",
    "# dataset_name = 'Test_05'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confirm area of interest\n",
    "\n",
    "# # Display the area of interest given the coords from the query\n",
    "# display_map(x=query['x'], y=query['y'])#, crs=query['output_crs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets\n",
    "\n",
    "# # ds = dc.load(product=products,\n",
    "# # #              progress_cbk=with_ui_cbk(),  # Add a progress bar like this if you decide to use eager loading.\n",
    "# #              dask_chunks=chunks,\n",
    "# # #              measurements=bands,\n",
    "# #              **query)\n",
    "# # Load landsat\n",
    "# ds = load_ard(dc=dc, \n",
    "#               products=products,\n",
    "#               min_gooddata=0.99,  # only take uncloudy scenes             \n",
    "# #               group_by='solar_day',\n",
    "#               **query)\n",
    "\n",
    "#         ## Tidally tag datasets\n",
    "# ds = tidal_tag(ds, ebb_flow=True)\n",
    "\n",
    "# # Attribute and sort for tide height\n",
    "# lowest = ds.tide_height.quantile([0.05]).values\n",
    "# ds_lowtide = ds.where(ds.tide_height <= lowest, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load ITEMv2 and mask out everything above max tide height \n",
    "# # (later, update with dynamic DEACL high tide lines)\n",
    "\n",
    "# # Remove 'measurements' key from query as it is not a valid \n",
    "# # measurement of the ITEMv2 product\n",
    "\n",
    "# query.pop('measurements')\n",
    "# query.pop('time')\n",
    "\n",
    "# # Load ITEMv2 for the area of interest\n",
    "# ds_ITEM = dc.load(\n",
    "#     'item_v2',\n",
    "# #     dask_chunks = chunks,\n",
    "#     **query)\n",
    "\n",
    "# # Filter dataset for ITEM values greater than 0 and less than 9\n",
    "# ds_ITEM = ds_ITEM.where(ds_ITEM.relative > 0)\n",
    "# ds_ITEM = ds_ITEM.where(ds_ITEM.relative < 9)\n",
    "\n",
    "# # Remove the time dimension from ITEM to enable masking\n",
    "# ds_ITEM = ds_ITEM.squeeze(dim='time', drop=True)\n",
    "\n",
    "# # Optional: view the loaded, filtered ITEM dataset\n",
    "# # %matplotlib inline # Confirm whether plot shows using %mpl widget\n",
    "# ds_ITEM.relative.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITEM integer interpretation\n",
    "Single Band Integer Raster:\n",
    "\n",
    "0 – Always water\n",
    "\n",
    "1 – Exposed at lowest 0-10% of the observed tidal range\n",
    "\n",
    "2 – Exposed at 10-20% of the observed tidal range\n",
    "\n",
    "3 – Exposed at 20-30% of the observed tidal range\n",
    "\n",
    "4 – Exposed at 30-40% of the observed tidal range\n",
    "\n",
    "5 – Exposed at 40-50% of the observed tidal range\n",
    "\n",
    "6 – Exposed at 50-60% of the observed tidal range\n",
    "\n",
    "7 – Exposed at 60-70% of the observed tidal range\n",
    "\n",
    "8 – Exposed at 70-80% of the observed tidal range\n",
    "\n",
    "9 - Exposed at highest 80-100% of the observed tidal range (land)\n",
    "\n",
    "-6666 – No Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Three band colour combinations for viewing imagery\n",
    "# rgb_bands = bands[:3]\n",
    "# snr_bands = ['nbart_swir_1', 'nbart_nir', 'nbart_red'] # For false colour\n",
    "\n",
    "# # Add ITEM mask as a coordinate array to imagery ds\n",
    "# ds_lowtide.coords['ITEM'] = (('y', 'x'), ds_ITEM.relative)\n",
    "\n",
    "# # Select image to classify\n",
    "# # (view a filmstrip of low-tide images - with and without masking)\n",
    "\n",
    "# # Plot unmasked true colour filmstrip of low tide datasets\n",
    "# rgb(ds_lowtide, bands=['nbart_red', 'nbart_green', 'nbart_blue'], col='time')\n",
    "\n",
    "# # Plot unmasked false colour filmstrip of low tide datasets\n",
    "# rgb(ds_lowtide, bands=['nbart_swir_1', 'nbart_nir', 'nbart_red'], col='time')\n",
    "# #     plt.title('Index: ' + str(x) + ', Time: ' + str(ds_lowtide.time[x].values))\n",
    "\n",
    "# # Plot ITEM-masked true colour filmstrip of low tide datasets\n",
    "# rgb(ds_lowtide.where(ds_lowtide.ITEM<9), bands=['nbart_red', 'nbart_green', 'nbart_blue'], col='time')\n",
    "# # plt.title('title')\n",
    "\n",
    "# # Plot ITEM-masked false colour filmstrip of low tide datasets\n",
    "# rgb(ds_lowtide.where(ds_lowtide.ITEM<9), bands=['nbart_swir_1', 'nbart_nir', 'nbart_red'], col='time')\n",
    "# # plt.title('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot images as above with adjusted titles\n",
    "# for x in range (0,len(ds_lowtide.time)):\n",
    "#     rgb(ds_lowtide.isel(time=x), bands=['nbart_red', 'nbart_green', 'nbart_blue'], size=4)\n",
    "#     plt.title('Index: ' + str(x) + ', Time: ' + str(ds_lowtide.time[x].values))\n",
    "#     rgb(ds_lowtide.isel(time=x), bands=['nbart_swir_1', 'nbart_nir', 'nbart_red'], size=4)\n",
    "#     plt.title('Index: ' + str(x) + ', Time: ' + str(ds_lowtide.time[x].values))\n",
    "#     rgb(ds_lowtide.where(ds_lowtide.ITEM<9).isel(time=x), bands=['nbart_red', 'nbart_green', \n",
    "#                                                          'nbart_blue'], size=4)\n",
    "#     plt.title('Index: ' + str(x) + ', Time: ' + str(ds_lowtide.time[x].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classify image - ths cell may not be necessary\n",
    "\n",
    "# item_masked = ds_lowtide.where(ds_lowtide.ITEM<9)\n",
    "# # item_masked\n",
    "\n",
    "# # Based on the above imagery, choose a timeslice (based upon its index value in the figure title)\n",
    "# # to classify\n",
    "\n",
    "# index = 1\n",
    "# # ds_lowtide.isel(time=index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinched from Sentinel_2_Cloud_Labelling.ipynb\n",
    "Now let's label the nominated image. We'll use an ipyleaflet widget based on `Imagery_on_web_map.ipynb` and `interactive_polygons.ipynb`. \n",
    "\n",
    "Different classes are labelled separately below.\n",
    "\n",
    "## This code sets up the widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a list of low_tide images from which to classify\n",
    "# dates = ds_lowtide.time.values\n",
    "\n",
    "# selected_dates=[]\n",
    "# for n in dates:\n",
    "#     print (n)\n",
    "#     selected_dates.append(n)\n",
    "\n",
    "# print('Based on your index value identified above, the imagery to be classified will be: ' + str(selected_dates[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the functionality to get the interactive map, overlay it with nominated image/s,\n",
    "# # create the polygon classification tool and build buttons to flick between imagery colour combos\n",
    "\n",
    "# # Set up the interactive map\n",
    "# def get_interactive_map(times, index, cover_type='green', clamp=3000):\n",
    "#     # Set up the map.\n",
    "#     bbox = ds.geobox.extent.to_crs('EPSG:4326').boundingbox\n",
    "#     zoom = odc.ui.zoom_from_bbox(bbox)\n",
    "#     center = (bbox.bottom + bbox.top) * 0.5, (bbox.right + bbox.left) * 0.5\n",
    "#     m = ipyleaflet.Map(\n",
    "#         center=center,\n",
    "#         zoom=zoom,\n",
    "#         scroll_wheel_zoom=True,  # Allow zoom with the mouse scroll wheel\n",
    "#         layout=widgets.Layout(\n",
    "#             width='600px',   # Set Width of the map to 600 pixels, examples: \"100%\", \"5em\", \"300px\"\n",
    "#             height='600px',  # Set height of the map\n",
    "#         ))\n",
    "#     # Add a false colour image\n",
    "#     def add_image_layer_snr(time):\n",
    "#         # Add the false colour image.\n",
    "\n",
    "#         img_layer_snr = odc.ui.mk_image_overlay(\n",
    "#             item_masked.sel(time=time).drop('time').drop('tide_height').drop('ebb_flow').drop('ITEM')\\\n",
    "#                 .rio.reproject(dst_crs='EPSG:3857', shape=ds_lowtide.nbart_red.shape[1:], \n",
    "#                 resampling=rasterio.warp.Resampling.bilinear), \n",
    "#             bands=snr_bands, #select snr_bands for false colour, rgb_bands for true colour\n",
    "#             clamp=clamp,\n",
    "#             fmt='jpeg')\n",
    "\n",
    "#         m.add_layer(img_layer_snr)\n",
    "        \n",
    "#         return img_layer_snr\n",
    "#     # Add a true colour image\n",
    "#     def add_image_layer_rgb(time):\n",
    "#         # Add the true colour image.\n",
    "\n",
    "#         img_layer_rgb = odc.ui.mk_image_overlay(\n",
    "#             item_masked.sel(time=time).drop('time').drop('tide_height').drop('ebb_flow').drop('ITEM')\\\n",
    "#                 .rio.reproject(dst_crs='EPSG:3857', shape=ds_lowtide.nbart_red.shape[1:], \n",
    "#                 resampling=rasterio.warp.Resampling.bilinear), \n",
    "#             bands=rgb_bands, #select snr_bands for false colour, rgb_bands for true colour\n",
    "#             clamp=clamp,\n",
    "#             fmt='jpeg')\n",
    "        \n",
    "#         m.add_layer(img_layer_rgb)\n",
    "        \n",
    "#         return img_layer_rgb\n",
    "\n",
    "#     # Add images to the map for the nominated imagery date\n",
    "#     idx = index\n",
    "#     img_layer_snr = add_image_layer_snr(times[idx])\n",
    "#     img_layer_rgb = add_image_layer_rgb(times[idx])\n",
    "     \n",
    "#     # Add the drawing controls for the classes\n",
    "#     fill_colours = {'green': '#BEBEFF', 'beige': '#BEFFBE', 'brown': '#FFBEBE'}\n",
    "#     feature_collection = {\n",
    "#         'type': 'FeatureCollection',\n",
    "#         'features': [],\n",
    "#     }\n",
    "#     for type_ in [cover_type]:  \n",
    "#         draw_control = ipyleaflet.DrawControl()\n",
    "#         draw_control.polygon = {\n",
    "#             \"shapeOptions\": {\n",
    "#                 \"fillColor\": fill_colours[type_],\n",
    "#                 \"color\": fill_colours[type_],\n",
    "#             },\n",
    "#             \"allowIntersection\": False,\n",
    "#             'title': type_,\n",
    "#         }\n",
    "#         # Disable polyline and circlemarker controls so that only polygon remains.\n",
    "#         draw_control.polyline = {}\n",
    "#         draw_control.circlemarker = {}\n",
    "#         def handle_draw(self, action, geo_json):\n",
    "#             geo_json['properties']['type'] = type_\n",
    "#             geo_json['properties']['index'] = times[idx]\n",
    "#             feature_collection['features'].append(geo_json),\n",
    "#         draw_control.on_draw(handle_draw)\n",
    "#         draw_control.edit = False\n",
    "#         draw_control.remove = False\n",
    "#         m.add_control(draw_control)\n",
    "        \n",
    "#     draw_control.edit = False  # Until syncing works.\n",
    "#     draw_control.remove = False  # Until syncing works.\n",
    "    \n",
    "    \n",
    "#     # Add button to show true colour image\n",
    "#     button_next = widgets.Button(\n",
    "#         description='True colour',\n",
    "#         button_style='info',\n",
    "#         icon='next')\n",
    "#     def on_click(self):\n",
    "#         nonlocal idx\n",
    "#         nonlocal img_layer_rgb\n",
    "#         nonlocal img_layer_snr\n",
    "#         m.remove_layer(img_layer_snr)\n",
    "#         img_layer_rgb = add_image_layer_rgb(times[idx])\n",
    "#     button_next.on_click(on_click)\n",
    "\n",
    "#         # Add button to show false colour image\n",
    "#     button_back = widgets.Button(\n",
    "#         description='False colour',\n",
    "#         button_style='info',\n",
    "#         icon='back')\n",
    "#     def on_click(self):\n",
    "#         nonlocal idx\n",
    "#         nonlocal img_layer_rgb\n",
    "#         nonlocal img_layer_snr\n",
    "#         m.remove_layer(img_layer_rgb)\n",
    "#         img_layer_snr = add_image_layer_snr(times[idx])\n",
    "#     button_back.on_click(on_click)\n",
    "    \n",
    "#     buttons=[button_next, button_back]\n",
    "    \n",
    "#     return widgets.VBox([widgets.HBox(buttons), m]), feature_collection\n",
    "\n",
    "# # Convert datetimes into strings following polygon identification\n",
    "# def save_fc(fc, fn):\n",
    "#     features = []\n",
    "#     for feature in fc['features']:\n",
    "#         feature = feature.copy()\n",
    "#         feature['properties']['index'] = str(feature['properties']['index'])\n",
    "#         features.append(feature)\n",
    "        \n",
    "#     if not os.path.exists(fn):\n",
    "#         gdf = gpd.GeoDataFrame.from_features(features)\n",
    "#         gdf.to_file(fn)\n",
    "#     else:\n",
    "#         raise RuntimeError('Label file already exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can then run the widget to get the interactive map, as well as a list that will be populated with polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label first class type\n",
    "\n",
    "# map_, fc_green = get_interactive_map(selected_dates, index, cover_type='green')\n",
    "\n",
    "# print('Draw a polygon around green cover classes of interest (e.g. seagrass) for further interrogation')\n",
    "# print ('Imagery date: ' + str(selected_dates[index]))\n",
    "# map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_fc(fc_green, f'green_{dataset_name}.shp')\n",
    "# gdf_green = gpd.read_file(f'green_{dataset_name}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label next class type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Please label areas of beige (e.g. sand).')\n",
    "# map_, fc_beige = get_interactive_map(selected_dates, index, cover_type='beige')\n",
    "# map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_fc(fc_beige, f'beige_{dataset_name}.shp')\n",
    "# gdf_beige = gpd.read_file(f'beige_{dataset_name}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label next class type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Please label areas of brown (e.g. mud)')\n",
    "# map_, fc_brown = get_interactive_map(selected_dates, index, cover_type='brown')\n",
    "# map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_fc(fc_brown, f'brown_{dataset_name}.shp')\n",
    "# gdf_brown = gpd.read_file(f'brown_{dataset_name}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now have 3 GeoDataFrames which we can easily combine into one for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine geodataframes for each class\n",
    "\n",
    "# gdf_brown = gpd.read_file(f'brown_{dataset_name}.shp')\n",
    "# gdf_beige = gpd.read_file(f'beige_{dataset_name}.shp')\n",
    "# gdf_green = gpd.read_file(f'green_{dataset_name}.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cover_labels_gdf = gpd.pd.concat([gdf_green, gdf_beige, gdf_brown], ignore_index=True)  # here, index means the numeric row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.concat fails to produce a GDF in some versions of GPD; https://gis.stackexchange.com/questions/162659/joining-concat-list-of-similar-dataframes-in-geopandas\n",
    "# assert isinstance(cover_labels_gdf, gpd.GeoDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# for i in range(2):\n",
    "#     ax = plt.subplot(1, 1, 1)\n",
    "#     timestamp = sorted(set(cover_labels_gdf['index']))[i]\n",
    "#     subgdf = cover_labels_gdf[cover_labels_gdf['index'] == timestamp]\n",
    "#     subgdf[subgdf['type'] == 'green'].plot(alpha=0.5, ax=ax, color='blue')\n",
    "#     subgdf[subgdf['type'] == 'beige'].plot(alpha=0.2, ax=ax, color='red')\n",
    "#     subgdf[subgdf['type'] == 'brown'].plot(alpha=0.5, ax=ax, color='grey')\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = subgdf\n",
    "\n",
    "# attribute_col = 'id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to attribute an integer per class (type)\n",
    "\n",
    "# # Rename 'type' column to 'class' to match the style of this workflow\n",
    "# gdf.rename(columns = {'type':'class'}, inplace = True)\n",
    "\n",
    "# # Attribute each class with an integer value\n",
    "# val = (gdf['class'].unique()).tolist()\n",
    "\n",
    "# num_list = []\n",
    "# attr_key = []\n",
    "\n",
    "# d = 0\n",
    "# for x in range (len(gdf)):\n",
    "#     for d in range (len(val)):        \n",
    "#         if gdf['class'].values[x] == str(val[d]):\n",
    "#             num_list.append(d)\n",
    "#         # Create a key to interpret the integer attribute for each class\n",
    "#         for y in num_list:\n",
    "#             if y not in attr_key:\n",
    "#                     attr_key.append(y)\n",
    "\n",
    "# val = [[el] for el in val]\n",
    "# for x in attr_key:\n",
    "#     val[x].append(attr_key[x])\n",
    "        \n",
    "# print ('The attribute values for each class are as follows: ' + str(val))\n",
    "\n",
    "# # Update the geodataframe of vector polygons with the integer attribution for each class\n",
    "# gdf['id'] = num_list\n",
    "\n",
    "# # Set the crs of the newly classified geodataframe\n",
    "# gdf = gdf.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the structure of the vector file\n",
    "Import the file and take a look at how the file is structured so we understand what we are iterating through. \n",
    "There are two polygons in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(vector_file)\n",
    "# # gdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the `geopandas.GeoDataFrame` using the function `map_shapefile` to make sure it covers the area of interest we are concerned with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shapefile(gdf, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datacube query object\n",
    "We then create a dictionary that will contain the parameters that will be used to load data from the DEA data cube:\n",
    "\n",
    "> **Note:** We do not include the usual `x` and `y` spatial query parameters here, as these will be taken directly from each of our vector polygon objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query = {'time': (time_buffer(time_of_interest, buffer=time_buff)),\n",
    "# query = {'time': time,\n",
    "#          'measurements': bands,\n",
    "#          'resolution': resolution,\n",
    "#          'output_crs': output_crs,\n",
    "#          'align': align,\n",
    "#          }\n",
    "\n",
    "# query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading satellite data\n",
    "\n",
    "Here we will iterate through each row of the `geopandas.GeoDataFrame` and load satellite data.  The results will be appended to a dictionary object which we can later index to analyse each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell likely superceeded by new notebook structure\n",
    "\n",
    "# # Dictionary to save results \n",
    "# results = {}\n",
    "# # results2 = {}\n",
    "\n",
    "# # Loop through polygons in geodataframe and extract satellite data\n",
    "# for index, row in gdf.iterrows():\n",
    "    \n",
    "#     print(f'Feature: {index + 1}/{len(gdf)}')\n",
    "#     print (gdf['class'].values[index])\n",
    "#     print (str(index))\n",
    "#     print (str(row))\n",
    "    \n",
    "#     if not (str(row[attribute_col]) in results.keys()):\n",
    "#         results[str(row[attribute_col])] = {}\n",
    "    \n",
    "#     # Extract the feature's geometry as a datacube geometry object\n",
    "#     geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    \n",
    "#     # Update the query to include our geopolygon\n",
    "#     query.update({'geopolygon': geom}) \n",
    "    \n",
    "#     # Load landsat\n",
    "#     ds = load_ard(dc=dc, \n",
    "#                   products=products,\n",
    "#                   # min_gooddata=0.99,  # only take uncloudy scenes\n",
    "#                   ls7_slc_off = False,                  \n",
    "#                   group_by='solar_day',\n",
    "#                   **query)\n",
    "    \n",
    "#             ## Tidally tag datasets\n",
    "#     ds = tidal_tag(ds, ebb_flow=True)\n",
    "    \n",
    "#     # Generate a polygon mask to keep only data within the polygon\n",
    "#     mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "    \n",
    "#     # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "#     ds = ds.where(mask)\n",
    "    \n",
    "#     # Append results to a dictionary using the attribute\n",
    "#     # column as an key\n",
    "# #     results.update({str(row[attribute_col]): ds}) ## Original. I think it only saves one polygon per class\n",
    "# #     results.update({str(row[attribute_col]):{str(index): ds}}) # New dict of dicts with each polygon recorded under each class.\n",
    "#     results[str(row[attribute_col])][str(index)] = ds\n",
    "    \n",
    "# #         ## Tidally tag datasets\n",
    "# #     ds1 = tidal_tag(ds, ebb_flow=True)\n",
    "    \n",
    "# #         # Generate a polygon mask to keep only data within the polygon\n",
    "# #     mask1 = xr_rasterize(gdf.iloc[[index]], ds1)\n",
    "    \n",
    "# #         # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "# #     ds1 = ds1.where(mask1)\n",
    "    \n",
    "# #     # Append results to a dictionary using the attribute column as key\n",
    "# #     results2[str(row[attribute_col])][str(index)] = ds1 \n",
    "\n",
    "#     print (row[attribute_col], index)\n",
    "#     print ('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `results` dictionary will contain `xarray` objects labelled by the unique `attribute column` values we specified in the `Analysis parameters` section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second dictionary that contains only datasets filtered by nominated tide range\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for k in results:\n",
    "    \n",
    "    if not (str(k) in results2.keys()):\n",
    "        results2[str(k)] = {}\n",
    "    \n",
    "    for kk in results[k]:\n",
    "        ds = results[k][kk] #.where(ds.ebb_flow == \"Ebb\", drop=True)\n",
    "        lowest_20 = ds.tide_height.quantile([0.20]).values ### Need a workaround for this as lowest 20% is calculated per ds and wont be consistent across all observations. However, perhaps this isn't a problem??\n",
    "#         print (lowest_20)\n",
    "        results2[k][kk] = ds.where(ds.tide_height <= lowest_20, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.keys()\n",
    "\n",
    "for k in results2.keys():\n",
    "    print (val[int(k)], len(results2[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter one of those values below to index our dictionary and conduct further analsyis on the satellite timeseries for that polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_key = '0'\n",
    "# print (results[class_key].keys())\n",
    "polygon_index_key = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot an RGB image     -     needs debugging\n",
    "We can now use the `dea_plotting.rgb` function to plot our loaded data as a three-band RGB plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To show the results of a single polygon:\n",
    "\n",
    "# # Would be useful to see the outline of the whole polygon to compare pixel retrievals\n",
    "\n",
    "# rgb(results2[class_key][polygon_index_key], col='time', size=4)\n",
    "# rgb(results2['1'][polygon_index_key], col='time', size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the results for every polygon in a given class:\n",
    "\n",
    "# Would be useful to see the outline of the whole polygon to compare pixel retrievals\n",
    "\n",
    "# for k in results2[class_key]:\n",
    "#     rgb(results2[class_key][k], col = 'time')#, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate NDVI and plot\n",
    "We can also apply analyses to data loaded for each of our polygons.\n",
    "For example, we can calculate the Normalised Difference Vegetation Index (NDVI) to identify areas of growing vegetation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zonal_stats = 'mean'\n",
    "# indice = 'NDVI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'results' ds, calculate NDVI for all pixels inside each polygon then calculate a zonal stat for the polygon\n",
    "# This code works for a single time step for each polygon\n",
    "\n",
    "polydrill = {}\n",
    "\n",
    "for k in results:\n",
    "    \n",
    "    if not (str(k) in polydrill.keys()):\n",
    "        polydrill[str(k)] = {}\n",
    "    \n",
    "    for kk in results[k]:\n",
    "        \n",
    "        ds = results[k][kk]\n",
    "#         print(ds)\n",
    "        \n",
    "        # calculate ndvi for pixels inside the polygon\n",
    "        ds = calculate_indices(ds, index='NDVI', collection='ga_ls_3')\n",
    "        \n",
    "        # calculate a zonal stat for the polygon\n",
    "        ds = ds.NDVI.mean('y').mean('x')  ## CONFIRM that this is taking the polygon NDVI mean per timeslice\n",
    "        \n",
    "        polydrill[str(k)][str(kk)] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'results2' ds (low tide imagery only), calculate NDVI for all pixels inside each polygon then \n",
    "# calculate a zonal stat for the polygon\n",
    "# This code works for a single time step for each polygon\n",
    "\n",
    "polydrill2 = {}\n",
    "\n",
    "for k in results2:\n",
    "    \n",
    "    if not (str(k) in polydrill2.keys()):\n",
    "        polydrill2[str(k)] = {}\n",
    "    \n",
    "    for kk in results2[k]:\n",
    "        \n",
    "        ds = results2[k][kk]\n",
    "#         print(ds)\n",
    "       \n",
    "        # To work around the calculate_indices function which was stalling on the additional \n",
    "        # coastal variables:\n",
    "        # drop tide_height and ebb_flow variables\n",
    "#         tide_height = ds['tide_height']\n",
    "#         ebb_flow = ds['ebb_flow']\n",
    "#         ds.drop_vars(names = ('tide_height', 'ebb_flow'))\n",
    "#         print (ds)\n",
    "        \n",
    "        # calculate ndvi for pixels inside the polygon\n",
    "        ds = calculate_indices(ds, index='NDVI', collection='ga_ls_3')\n",
    "        \n",
    "        # calculate a zonal stat for the polygon\n",
    "        ds = ds.NDVI.mean('y').mean('x')  ## CONFIRM that this is taking the polygon NDVI mean per timeslice\n",
    "        \n",
    "        # reattach tide_height and ebb_flow variables\n",
    "#         ds['tide_height']= tide_height\n",
    "#         ds['ebb_flow'] = ebb_flow\n",
    "        \n",
    "        polydrill2[str(k)][str(kk)] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show zonal mean NDVI per polygon, per timestep for every polygon in the class (all tide heights)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=(len(val)),sharex='all', figsize=(10,10))\n",
    "\n",
    "for x in val:\n",
    "    for kk in polydrill[str(x[1])]:\n",
    "        polydrill[str(x[1])][kk].plot.line(marker='o', linewidth = 0, ax=axes[x[1]])\n",
    "        axes[x[1]].set_title(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show zonal mean NDVI per polygon, per timestep for every low-tide polygon in the class\n",
    "\n",
    "fig, axes = plt.subplots(nrows=(len(val)),sharex='all', figsize=(10,10))\n",
    "\n",
    "for x in val:\n",
    "    for kk in polydrill2[str(x[1])]:\n",
    "        polydrill2[str(x[1])][kk].plot.line(marker='o', linewidth = 0, ax=axes[x[1]])\n",
    "        axes[x[1]].set_title(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: to attach polygon (not pixel) NDVI value (mean, median - whatever is designated above)\n",
    "### to the polygon set for plotting (heat map style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the shapefiles from imported vector set\n",
    "# map_shapefile(gdf, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2.keys()\n",
    "\n",
    "for k in results2.keys():\n",
    "#     print (type(k))\n",
    "    print (val[int(k)], len(results2[k]))\n",
    "    \n",
    "print (len(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every polygon, extract list pairs on the polygon id and temporal mean and std of the nominated indice\n",
    "\n",
    "lsmean = []\n",
    "lsstd = []\n",
    "\n",
    "for k in polydrill2:\n",
    "    for kk in polydrill2[k]:\n",
    "        lsmean.append([int(kk), polydrill2[k][kk].mean()])\n",
    "        lsstd.append([int(kk), polydrill2[k][kk].std()])\n",
    "\n",
    "# Sort the list by polygon id to match up to the original polygon gdf\n",
    "# Separate the sorted polygon ids from the indice statistic to build into a pd.DataFrame\n",
    "lsmean=sorted(lsmean)\n",
    "lsstd = sorted(lsstd)\n",
    "\n",
    "indicemean = []\n",
    "indicestd = []\n",
    "polyid = []\n",
    "\n",
    "for x in lsmean:\n",
    "#     print (x[1])\n",
    "    polyid.append(x[0])\n",
    "    indicemean.append(x[1])\n",
    "    \n",
    "for x in lsstd:\n",
    "    indicestd.append(x[1])\n",
    "\n",
    "# Build a pd.DataFrame from the sorted polygon id and indice statistics. Nominate a name for the new column.\n",
    "\n",
    "indexstats = pd.DataFrame(indicemean, index = polyid, columns = ['Indice mean'])\n",
    "indexstats['Indice std'] = None\n",
    "indexstats.loc[polyid, 'Indice std'] = indicestd\n",
    "\n",
    "# Merge the indice statistic for each polygon into the original polygon gdf\n",
    "\n",
    "gdf = gdf.merge(indexstats, on=indexstats.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_shapefile(gdf_tests, attribute='Indice std')\n",
    "# map_shapefile(gdf, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the shapefiles from imported vector set. Nominate the new indice statistic column\n",
    "# map_shapefile(gdf_test, attribute=attribute_col)\n",
    "# map_shapefile(gdf, attribute='Indice std')\n",
    "\n",
    "#Next steps:\n",
    "# drop consolidated and unconsolidated polygons\n",
    "# add variance metric on indice (e.g. NDVI sd)\n",
    "# plot variane in nominated indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "- take polygon ndvi values from polydrill2 and calculate mean and sd across all combined timesteps\n",
    "- attach ndvi mean and sd to the correct polygon id's in gdf\n",
    "- plot:`map_shapefile(gdf, attribute=attribute_col)` using attribute=new_stdev(ndvi) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop consolidated and unconsolidated substrate class polygons\n",
    "# Update: this doesn't seem to be working. Continue to generate class dict from gdf then pop these two classes\n",
    "# # x=[]\n",
    "# # for index, row in gdf_test.iterrows():\n",
    "# for x in range (len(gdf)):      \n",
    "#     if gdf['BRD_HAB'].values[x] == \"Intertidal consolidated substrate\":\n",
    "# #         print (gdf_test['BRD_HAB'].values[index])\n",
    "#         gdf.drop(x, inplace=True)\n",
    "#     else:\n",
    "#         if gdf['BRD_HAB'].values[x] == \"Intertidal unconsolidated substrate\":\n",
    "#             gdf.drop(x, inplace=True)\n",
    "# #         print (row)\n",
    "# #         x.append(index)\n",
    "# #     print (row)\n",
    "# # print(len(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_shapefile(gdf, attribute='Indice mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Separate classes from the master gdf to view/plot individually\n",
    "\n",
    "# Define function to append multiple values to single keys in dict\n",
    "# Reference: https://thispointer.com/python-how-to-add-append-key-value-pairs-in-dictionary-using-dict-update/#6\n",
    "def append_value(dict, key, value):\n",
    "    # Check if key exists in dict or not\n",
    "    if key in dict:\n",
    "        # Key exists in dict\n",
    "        # Check if type of value of key is list or not\n",
    "        if not isinstance(dict[key], list):\n",
    "            # If type is not a list then make it a list\n",
    "            dict[key] = [dict[key]]\n",
    "        # Append the value in list\n",
    "        dict[key].append(value)\n",
    "    else:\n",
    "        # As key is not in dict, add key-value pair\n",
    "        dict[key] = value\n",
    "        \n",
    "# create dict to store classes separately (to create class specific gdf's later on)\n",
    "classdict = {}\n",
    "classdict['columns'] = gdf.columns\n",
    "\n",
    "# group gdf classes into dict keys (for later conversion to class specific gdfs)\n",
    "for x in gdf['key_0']:\n",
    "    for cover in gdf['BRD_HAB'].unique():\n",
    "        if gdf['BRD_HAB'].values[x] == cover:\n",
    "            append_value(classdict, cover, gdf.values[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns, consolidated and unconsolidated substrate classes from this analysis\n",
    "\n",
    "classdictkeys = classdict.pop('columns')\n",
    "classdict.pop('Intertidal unconsolidated substrate')\n",
    "classdict.pop('Intertidal consolidated substrate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find a way to avoid hard-coding these class names for their new gdf's\n",
    "for key in classdict.keys():\n",
    "        print (key)\n",
    "\n",
    "# Hardcoded gdfs:\n",
    "grasses = gpd.GeoDataFrame()\n",
    "mangroves = gpd.GeoDataFrame()\n",
    "seagrass = gpd.GeoDataFrame()\n",
    "\n",
    "for item in classdictkeys:\n",
    "#     print (item)\n",
    "    grasses[item] = []\n",
    "    mangroves[item] = []\n",
    "    seagrass[item] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasses['key_0'] = [x[0] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['OBJECTID']= [x[1] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['CONSOL']= [x[2] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['DOM_TYPE']= [x[3] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['DOM_LABEL']= [x[4] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['CO_TYPES']= [x[5] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['TIDE_ZONE']= [x[6] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['BRD_HAB']= [x[7] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['Shape_Leng']= [x[8] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['Shape_Area']= [x[9] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['geometry']= [x[10] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['id']= [x[11] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['Indice mean']= [x[12] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "grasses['Indice std']= [x[13] for x in classdict['Intertidal grass-herb-sedge-other succulent']]\n",
    "\n",
    "grasses.set_index('key_0', inplace=True)\n",
    "grasses = grasses.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# print(grasses['Indice std'].std())\n",
    "\n",
    "map_shapefile(grasses, attribute='Indice std', continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mangroves['key_0'] = [x[0] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['OBJECTID']= [x[1] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['CONSOL']= [x[2] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['DOM_TYPE']= [x[3] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['DOM_LABEL']= [x[4] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['CO_TYPES']= [x[5] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['TIDE_ZONE']= [x[6] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['BRD_HAB']= [x[7] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['Shape_Leng']= [x[8] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['Shape_Area']= [x[9] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['geometry']= [x[10] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['id']= [x[11] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['Indice mean']= [x[12] for x in classdict['Intertidal mangroves and other trees & shrubs']]\n",
    "mangroves['Indice std']= [x[13] for x in classdict['Intertidal mangroves and other trees & shrubs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mangroves.set_index('key_0', inplace=True)\n",
    "mangroves = mangroves.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shapefile(mangroves, attribute='Indice std', continous=True)  # What is this plotting? max std definitely NOT 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seagrass['key_0'] = [x[0] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['OBJECTID']= [x[1] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['CONSOL']= [x[2] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['DOM_TYPE']= [x[3] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['DOM_LABEL']= [x[4] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['CO_TYPES']= [x[5] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['TIDE_ZONE']= [x[6] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['BRD_HAB']= [x[7] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['Shape_Leng']= [x[8] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['Shape_Area']= [x[9] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['geometry']= [x[10] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['id']= [x[11] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['Indice mean']= [x[12] for x in classdict['Intertidal seagrass']]\n",
    "seagrass['Indice std']= [x[13] for x in classdict['Intertidal seagrass']]\n",
    "\n",
    "seagrass.set_index('key_0', inplace=True)\n",
    "seagrass = seagrass.set_crs(\"EPSG:4326\")\n",
    "\n",
    "map_shapefile(seagrass, attribute='Indice std', continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Monday: explore what is actually being plotted in the maps. Why is the polygon colouring different when continuous is set to True???\n",
    "# It would be awesome to see a legend too\n",
    "# Seagrass will include effects of tide range selection (here, it is set to bottom 20% which is likely why the NDVI values range down to -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cell\n",
    "\n",
    "from ipyleaflet import LegendControl\n",
    "\n",
    "m = Map(center=(-10,-45), zoom=4)\n",
    "\n",
    "legend = LegendControl({\"low\":\"#FAA\", \"medium\":\"#A55\", \"High\":\"#500\"}, name=\"Legend\", position=\"bottomright\")\n",
    "m.add_control(legend)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to enable dea_plotting.py functions\n",
    "\n",
    "# Import required packages\n",
    "import math\n",
    "import folium\n",
    "import calendar\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from datetime import datetime\n",
    "from pyproj import Proj, transform\n",
    "from IPython.display import display\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from ipyleaflet import Map, Marker, Popup, GeoJSON, basemaps, Choropleth, LegendControl\n",
    "from skimage import exposure\n",
    "from branca.colormap import linear\n",
    "from odc.ui import image_aspect\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from shapely.geometry import box\n",
    "from skimage.exposure import rescale_intensity\n",
    "from tqdm.auto import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _degree_to_zoom_level(l1, l2, margin=0.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to set zoom level for `display_map`\n",
    "    \"\"\"\n",
    "    \n",
    "    degree = abs(l1 - l2) * (1 + margin)\n",
    "    zoom_level_int = 0\n",
    "    if degree != 0:\n",
    "        zoom_level_float = math.log(360 / degree) / math.log(2)\n",
    "        zoom_level_int = int(zoom_level_float)\n",
    "    else:\n",
    "        zoom_level_int = 18\n",
    "    return zoom_level_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing cell\n",
    "\n",
    "# from ipyleaflet import LegendControl\n",
    "\n",
    "# # map_shapefile function from dea_plotting.py script\n",
    "\n",
    "# def map_shapefile(gdf,\n",
    "#                   attribute,\n",
    "#                   continuous=False,\n",
    "#                   colormap='YlOrRd_09',\n",
    "#                   basemap=basemaps.Esri.WorldImagery,\n",
    "#                   default_zoom=None,\n",
    "#                   hover_col=True,\n",
    "#                   **style_kwargs):\n",
    "#     \"\"\"\n",
    "#     Plots a geopandas GeoDataFrame over an interactive ipyleaflet \n",
    "#     basemap, with features coloured based on attribute column values. \n",
    "#     Optionally, can be set up to print selected data from features in \n",
    "#     the GeoDataFrame. \n",
    "\n",
    "#     Last modified: February 2020\n",
    "\n",
    "#     Parameters\n",
    "#     ----------  \n",
    "#     gdf : geopandas.GeoDataFrame\n",
    "#         A GeoDataFrame containing the spatial features to be plotted \n",
    "#         over the basemap.\n",
    "#     attribute: string, required\n",
    "#         An required string giving the name of any column in the\n",
    "#         GeoDataFrame you wish to have coloured on the choropleth.\n",
    "#     continuous: boolean, optional\n",
    "#         Whether to plot data as a categorical or continuous variable. \n",
    "#         Defaults to remapping the attribute which is suitable for \n",
    "#         categorical data. For continuous data set `continuous` to True.\n",
    "#     colormap : string, optional\n",
    "#         Either a string giving the name of a `branca.colormap.linear` \n",
    "#         colormap or a `branca.colormap` object (for example, \n",
    "#         `branca.colormap.linear.YlOrRd_09`) that will be used to style \n",
    "#         the features in the GeoDataFrame. Features will be coloured \n",
    "#         based on the selected attribute. Defaults to the 'YlOrRd_09' \n",
    "#         colormap.\n",
    "#     basemap : ipyleaflet.basemaps object, optional\n",
    "#         An optional `ipyleaflet.basemaps` object used as the basemap for \n",
    "#         the interactive plot. Defaults to `basemaps.Esri.WorldImagery`.\n",
    "#     default_zoom : int, optional\n",
    "#         An optional integer giving a default zoom level for the \n",
    "#         interactive ipyleaflet plot. Defaults to None, which infers\n",
    "#         the zoom level from the extent of the data.\n",
    "#     hover_col : boolean or str, optional\n",
    "#         If True (the default), the function will print  values from the \n",
    "#         GeoDataFrame's `attribute` column above the interactive map when \n",
    "#         a user hovers over the features in the map. Alternatively, a \n",
    "#         custom shapefile field can be specified by supplying a string\n",
    "#         giving the name of the field to print. Set to False to prevent \n",
    "#         any attributes from being printed.\n",
    "#     **choropleth_kwargs :\n",
    "#         Optional keyword arguments to pass to the `style` paramemter of\n",
    "#         the `ipyleaflet.Choropleth` function. This can be used to \n",
    "#         control the appearance of the shapefile, for example 'stroke' \n",
    "#         and 'weight' (controlling line width), 'fillOpacity' (polygon \n",
    "#         transparency) and 'dashArray' (whether to plot lines/outlines\n",
    "#         with dashes). For more information:\n",
    "#         https://ipyleaflet.readthedocs.io/en/latest/api_reference/choropleth.html\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def on_hover(event, id, properties):\n",
    "#         with dbg:\n",
    "#             text = properties.get(hover_col, '???')\n",
    "#             lbl.value = f'{hover_col}: {text}'\n",
    "            \n",
    "#     # Verify that attribute exists in shapefile   \n",
    "#     if attribute not in gdf.columns:\n",
    "#         raise ValueError(f\"The `attribute` {attribute} does not exist \"\n",
    "#                          f\"in the geopandas.GeoDataFrame. \"\n",
    "#                          f\"Valid attributes include {gdf.columns.values}.\")\n",
    "        \n",
    "#     # If hover_col is True, use 'attribute' as the default hover attribute.\n",
    "#     # Otherwise, hover_col will use the supplied attribute field name\n",
    "#     if hover_col and (hover_col is True):\n",
    "#         hover_col = attribute\n",
    "        \n",
    "#     # If a custom string if supplied to hover_col, check this exists \n",
    "#     elif hover_col and (type(hover_col) == str):\n",
    "#         if hover_col not in gdf.columns:\n",
    "#                 raise ValueError(f\"The `hover_col` field {hover_col} does \"\n",
    "#                                  f\"not exist in the geopandas.GeoDataFrame. \"\n",
    "#                                  f\"Valid attributes include \"\n",
    "#                                  f\"{gdf.columns.values}.\")\n",
    "\n",
    "#     # Convert to WGS 84 and GeoJSON format\n",
    "#     gdf_wgs84 = gdf.to_crs(epsg=4326)\n",
    "#     data_geojson = gdf_wgs84.__geo_interface__\n",
    "    \n",
    "#     # If continuous is False, remap categorical classes for visualisation\n",
    "#     if not continuous:\n",
    "        \n",
    "#         # Zip classes data together to make a dictionary\n",
    "#         classes_uni = list(gdf[attribute].unique())\n",
    "#         classes_clean = list(range(0, len(classes_uni)))\n",
    "#         classes_dict = dict(zip(classes_uni, classes_clean))\n",
    "        \n",
    "#         # Get values to colour by as a list \n",
    "#         classes = gdf[attribute].map(classes_dict).tolist()  \n",
    "    \n",
    "#     # If continuous is True then do not remap\n",
    "#     else: \n",
    "        \n",
    "#         # Get values to colour by as a list\n",
    "#         classes = gdf[attribute].tolist()  \n",
    "\n",
    "#     # Create the dictionary to colour map by\n",
    "#     keys = gdf.index\n",
    "#     id_class_dict = dict(zip(keys.astype(str), classes))  \n",
    "\n",
    "#     # Get centroid to focus map on\n",
    "#     lon1, lat1, lon2, lat2 = gdf_wgs84.total_bounds\n",
    "#     lon = (lon1 + lon2) / 2\n",
    "#     lat = (lat1 + lat2) / 2\n",
    "\n",
    "#     if default_zoom is None:\n",
    "\n",
    "#         # Calculate default zoom from latitude of features\n",
    "#         default_zoom = _degree_to_zoom_level(lat1, lat2, margin=-0.5)\n",
    "\n",
    "#     # Plot map\n",
    "#     m = Map(center=(lat, lon),\n",
    "#             zoom=default_zoom,\n",
    "#             basemap=basemap,\n",
    "#             layout=dict(width='800px', height='600px'))\n",
    "    \n",
    "\n",
    "    \n",
    "#     # Define default plotting parameters for the choropleth map. \n",
    "#     # The nested dict structure sets default values which can be \n",
    "#     # overwritten/customised by `choropleth_kwargs` values\n",
    "#     style_kwargs = dict({'fillOpacity': 0.8}, **style_kwargs)\n",
    "\n",
    "#     # Get colormap from either string or `branca.colormap` object\n",
    "#     if type(colormap) == str:\n",
    "#         colormap = getattr(linear, colormap)\n",
    "    \n",
    "#     # Create the choropleth\n",
    "#     choropleth = Choropleth(geo_data=data_geojson,\n",
    "#                             choro_data=id_class_dict,\n",
    "#                             colormap=colormap,\n",
    "#                             style={**style_kwargs})\n",
    "    \n",
    "#     # If the vector data contains line features, they will not be \n",
    "#     # be coloured by default. To resolve this, we need to manually copy\n",
    "#     # across the 'fillColor' attribute to the 'color' attribute for each\n",
    "#     # feature, then plot the data as a GeoJSON layer rather than the\n",
    "#     # choropleth layer that we use for polygon data.\n",
    "#     linefeatures = any(x in ['LineString', 'MultiLineString'] \n",
    "#                        for x in gdf.geometry.type.values)\n",
    "#     if linefeatures:\n",
    "    \n",
    "#         # Copy colour from fill to line edge colour\n",
    "#         for i in keys:\n",
    "#             choropleth.data['features'][i]['properties']['style']['color'] = \\\n",
    "#             choropleth.data['features'][i]['properties']['style']['fillColor']\n",
    "\n",
    "#         # Add GeoJSON layer to map\n",
    "#         feature_layer = GeoJSON(data=choropleth.data)\n",
    "#         m.add_layer(feature_layer)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         # Add Choropleth layer to map\n",
    "#         m.add_layer(choropleth)\n",
    "\n",
    "#     # If a column is specified by `hover_col`, print data from the\n",
    "#     # hovered feature above the map\n",
    "#     if hover_col and not linefeatures:\n",
    "        \n",
    "#         # Use cholopleth object if data is polygon\n",
    "#         lbl = ipywidgets.Label()\n",
    "#         dbg = ipywidgets.Output()\n",
    "#         choropleth.on_hover(on_hover)\n",
    "#         display(lbl)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         lbl = ipywidgets.Label()\n",
    "#         dbg = ipywidgets.Output()\n",
    "#         feature_layer.on_hover(on_hover)\n",
    "#         display(lbl)\n",
    "\n",
    "#     # Set the legend conditions and round to 2 decimal places\n",
    "#     legend = LegendControl({np.around(np.quantile(classes, 0.00),2): colormap.rgb_hex_str(0.00),\n",
    "#                             np.around(np.quantile(classes, 0.25),2): colormap.rgb_hex_str(0.25),\n",
    "#                             np.around(np.quantile(classes, 0.50),2): colormap.rgb_hex_str(0.50),\n",
    "#                             np.around(np.quantile(classes, 0.75),2): colormap.rgb_hex_str(0.75),\n",
    "#                             np.around(np.quantile(classes, 1.00),2): colormap.rgb_hex_str(1.00) }, \n",
    "#                            name=attribute, position=\"bottomleft\")\n",
    "#     m.add_control(legend)\n",
    "\n",
    "#     # Display the map\n",
    "#     display(m)\n",
    "# #     return colormap\n",
    "#     return len(id_class_dict)\n",
    "# #     print (sorted(classes)[0], sorted(classes). ,sorted(classes)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing cell\n",
    "\n",
    "map_shapefile(seagrass, attribute='Indice std', continuous=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = 'YlOrRd_09'\n",
    "\n",
    "if type(colormap) == str:\n",
    "        colormap = getattr(linear, colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap.rgb_hex_str(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values to colour by as a list\n",
    "classes = seagrass['Indice std'].tolist()\n",
    "type(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "median(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(classes)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around((np.quantile(classes, 1)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mangroves['Indice std'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "045ab485daf24868b7c2382f424cbcb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_b0ea6054510a498e8a7c56ce35cc5c4c",
       "style": "IPY_MODEL_f660eac494c147198a27997ad8b22470"
      }
     },
     "0746427347e2416f920f9ce96228b721": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletGeoJSONModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "data": {
        "bbox": [
         142.4524578701443,
         -32.36320549945,
         142.54736615559136,
         -32.29586369982442
        ],
        "features": [
         {
          "bbox": [
           142.51524944800926,
           -32.31838789697648,
           142.54736615559136,
           -32.29586369982442
          ],
          "geometry": {
           "coordinates": [
            [
             [
              142.51524944800926,
              -32.296091685646715
             ],
             [
              142.546601533663,
              -32.29586369982442
             ],
             [
              142.54736615559136,
              -32.3183698586848
             ],
             [
              142.51526741493365,
              -32.31838789697648
             ],
             [
              142.51524944800926,
              -32.296091685646715
             ]
            ]
           ],
           "type": "Polygon"
          },
          "id": "0",
          "properties": {
           "id": 2,
           "style": {
            "color": "black",
            "fillColor": "#ffffcc",
            "fillOpacity": 0.8,
            "weight": 0.9
           }
          },
          "type": "Feature"
         },
         {
          "bbox": [
           142.4524578701443,
           -32.36320549945,
           142.4845749551165,
           -32.34069269280065
          ],
          "geometry": {
           "coordinates": [
            [
             [
              142.4524578701443,
              -32.340907825281136
             ],
             [
              142.483823262827,
              -32.34069269280065
             ],
             [
              142.4845749551165,
              -32.36320063502121
             ],
             [
              142.45246271443352,
              -32.36320549945
             ],
             [
              142.4524578701443,
              -32.340907825281136
             ]
            ]
           ],
           "type": "Polygon"
          },
          "id": "1",
          "properties": {
           "id": 1,
           "style": {
            "color": "black",
            "fillColor": "#800026",
            "fillOpacity": 0.8,
            "weight": 0.9
           }
          },
          "type": "Feature"
         }
        ],
        "type": "FeatureCollection"
       },
       "style": {
        "fillOpacity": 0.8
       }
      }
     },
     "140008a05899493688edb1cfd07bb91d": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_8563b85ab05a465b900584182485dc98"
      }
     },
     "215e008ea30446418c9489a3fc2a3509": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "grab"
      }
     },
     "2f3cbd62daf645a4a47a3132fde027d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "600px",
       "width": "800px"
      }
     },
     "331f5be31bf049e4a4c2ab49668abcdd": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "472e3957cd724d3fb57312a3fa86dae1": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     },
     "8563b85ab05a465b900584182485dc98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8912f038ad0947a7ae08ed0ccdb15644": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapModel",
      "state": {
       "_dom_classes": [],
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "basemap": {
        "attribution": "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community",
        "max_zoom": 20,
        "name": "Esri.WorldImagery",
        "url": "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
       },
       "center": [
        -32.32953459963721,
        142.4999120128678
       ],
       "controls": [
        "IPY_MODEL_331f5be31bf049e4a4c2ab49668abcdd",
        "IPY_MODEL_472e3957cd724d3fb57312a3fa86dae1"
       ],
       "default_style": "IPY_MODEL_d14c0de5ce3b48bd9eb7729afbe8d701",
       "dragging_style": "IPY_MODEL_a6bbab8038c3446a93e3a549b8f8dd79",
       "east": 142.5685501098633,
       "fullscreen": false,
       "interpolation": "bilinear",
       "layers": [
        "IPY_MODEL_99119a1935514365b929a6d6c4b31a95",
        "IPY_MODEL_0746427347e2416f920f9ce96228b721"
       ],
       "layout": "IPY_MODEL_2f3cbd62daf645a4a47a3132fde027d0",
       "modisdate": "yesterday",
       "north": -32.28597166993233,
       "options": [
        "basemap",
        "bounce_at_zoom_limits",
        "box_zoom",
        "center",
        "close_popup_on_click",
        "double_click_zoom",
        "dragging",
        "fullscreen",
        "inertia",
        "inertia_deceleration",
        "inertia_max_speed",
        "interpolation",
        "keyboard",
        "keyboard_pan_offset",
        "keyboard_zoom_offset",
        "max_zoom",
        "min_zoom",
        "scroll_wheel_zoom",
        "tap",
        "tap_tolerance",
        "touch_zoom",
        "world_copy_jump",
        "zoom",
        "zoom_animation_threshold",
        "zoom_start"
       ],
       "south": -32.373002604986546,
       "style": "IPY_MODEL_215e008ea30446418c9489a3fc2a3509",
       "west": 142.4312210083008,
       "zoom": 13
      }
     },
     "99119a1935514365b929a6d6c4b31a95": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletTileLayerModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "attribution": "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community",
       "base": true,
       "max_native_zoom": 18,
       "max_zoom": 20,
       "min_native_zoom": 0,
       "min_zoom": 1,
       "name": "Esri.WorldImagery",
       "no_wrap": false,
       "options": [
        "attribution",
        "detect_retina",
        "max_native_zoom",
        "max_zoom",
        "min_native_zoom",
        "min_zoom",
        "no_wrap",
        "tile_size"
       ],
       "url": "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
      }
     },
     "a6bbab8038c3446a93e3a549b8f8dd79": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "move"
      }
     },
     "b0ea6054510a498e8a7c56ce35cc5c4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d14c0de5ce3b48bd9eb7729afbe8d701": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "grab"
      }
     },
     "f660eac494c147198a27997ad8b22470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
