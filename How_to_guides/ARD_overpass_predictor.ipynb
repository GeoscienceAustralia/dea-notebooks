{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARD overpass predictor <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* [**Sign up to the DEA Sandbox**](https://docs.dea.ga.gov.au/setup/sandbox.html) to run this notebook interactively from a browser\n",
    "* **Compatibility:** Notebook currently compatible with the `DEA Sandbox` environment\n",
    "* **Special requirements:** This notebook loads data from an external .csv file (`overpass_input.csv`) from the `Supplementary_data` folder of this repository\n",
    "\n",
    "\n",
    "## Background\n",
    "Knowing the time of a satellite overpass (OP) at a field site is necessary to plan field work activities. While predicting the timesteps for a single field site that receives only one overpass from a given satellite is easy, it gets more complicated when you need overpass information across multiple field sites. For most sites, they will lie in a single descending (daytime) satellite acquisition. For Landsat 8 this occurs every 16 days and for Sentinel 2A / 2B this occurs every 10 days.\n",
    "\n",
    "This notebook can be used to output a list of ordered overpasses for given field sites by providing an initial timestamp for field sites of interest. Output for multiple sites are ordered by date such that dual overpasses can be identified.\n",
    "\n",
    "See \"overpass_input.csv\" as an example input file - this contains a number of field sites, with some that receive multiple acquisitions.\n",
    "\n",
    "## Description\n",
    "\n",
    "You must provide a start date + time for the overpass of the site you are interested in - go to https://nationalmap.gov.au/ and https://evdc.esa.int/orbit/ to get an overpass for your location (lat/long) and add this to the input file.\n",
    "\n",
    "Take care with sites that receive multiple acquisitions, ie those that lie in the overlap of 2 acquisitions.\n",
    "\n",
    "- Specify an output file name\n",
    "- Provide an input file - this can be the example included or your own field site information\n",
    "- Specify which field sites receive extra overpasses in an orbital period, ie those which lie in satellite imaging overlap\n",
    "- Specify output field sites - you can select a subset of sites of interest (if adding your site to the original example file)\n",
    "- Combine field site overpasses\n",
    "\n",
    "If you wish to add or alter sites to predict for, edit the input file located at:\n",
    "\n",
    "```\n",
    "../Supplementary_data/ARD_overpass_predictor/overpass_input.csv\n",
    "```\n",
    "\n",
    "And modify the input file path to reflect. You will need to leave the 1st site in the input file (Mullion) as the notebook requires this to order multiple overpasses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run the overpass predictor with the given input file, run all cells in the notebook starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify output file name\n",
    "This is the name of the .csv the notebook will output. This .csv will contain your combined predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can rename the .csv to any desired output filename here. Default is \"output_overpass_pred.csv\"\n",
    "output_path = '../Supplementary_data/ARD_overpass_predictor/output_overpass_pred.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify input file containing initial overpass data for a given site\n",
    "\n",
    "Read in base overpass file ```overpass_input.csv``` \n",
    "\n",
    "- Input date times must be in UTC\n",
    "- Make sure the necessary dates for your site are imported correctly - should be YYYY-MM-DD HH:MM:SS in 24hr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Path</th>\n",
       "      <th>Row</th>\n",
       "      <th>landsat_8</th>\n",
       "      <th>landsat_8_2</th>\n",
       "      <th>sentinel_2a</th>\n",
       "      <th>sentinel_2a_2</th>\n",
       "      <th>sentinel_2b</th>\n",
       "      <th>sentinel_2b_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Site</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mullion</th>\n",
       "      <td>-35.123</td>\n",
       "      <td>148.862</td>\n",
       "      <td>91</td>\n",
       "      <td>84</td>\n",
       "      <td>2019-11-29 23:43:00</td>\n",
       "      <td>2019-11-20 23:49:00</td>\n",
       "      <td>2019-12-20 23:58:00</td>\n",
       "      <td>2019-12-14 00:07:00</td>\n",
       "      <td>2019-12-15 23:58:00</td>\n",
       "      <td>2019-12-19 00:07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lake_George</th>\n",
       "      <td>-35.094</td>\n",
       "      <td>149.463</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>2019-11-29 23:43:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-12-20 23:58:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-12-15 23:58:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narrabundah</th>\n",
       "      <td>-35.334</td>\n",
       "      <td>149.145</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>2019-11-29 23:43:00</td>\n",
       "      <td>2019-11-20 23:49:00</td>\n",
       "      <td>2019-12-20 23:58:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-12-15 23:58:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Latitude  Longitude  Path  Row           landsat_8  \\\n",
       "Site                                                              \n",
       "Mullion       -35.123    148.862    91   84 2019-11-29 23:43:00   \n",
       "Lake_George   -35.094    149.463    90   84 2019-11-29 23:43:00   \n",
       "Narrabundah   -35.334    149.145    90   85 2019-11-29 23:43:00   \n",
       "\n",
       "                    landsat_8_2         sentinel_2a       sentinel_2a_2  \\\n",
       "Site                                                                      \n",
       "Mullion     2019-11-20 23:49:00 2019-12-20 23:58:00 2019-12-14 00:07:00   \n",
       "Lake_George                 NaT 2019-12-20 23:58:00                 NaT   \n",
       "Narrabundah 2019-11-20 23:49:00 2019-12-20 23:58:00                 NaT   \n",
       "\n",
       "                    sentinel_2b       sentinel_2b_2  \n",
       "Site                                                 \n",
       "Mullion     2019-12-15 23:58:00 2019-12-19 00:07:00  \n",
       "Lake_George 2019-12-15 23:58:00                 NaT  \n",
       "Narrabundah 2019-12-15 23:58:00                 NaT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in base file 'overpass_input.csv'\n",
    "# Edit this file with new field sites, or load your own file with field sites as needed.\n",
    "# You don't need to input Path / Row or Lat / Long. The notebook only requires start times for the relevant satellites.\n",
    "base_file_path = '../Supplementary_data/ARD_overpass_predictor/overpass_input.csv'\n",
    "overpass = pd.read_csv(base_file_path, index_col='Site', \n",
    "                       parse_dates=['landsat_8', 'sentinel_2a', 'sentinel_2b', 'landsat_8_2', 'sentinel_2a_2', 'sentinel_2b_2'],\n",
    "                       dayfirst=True)\n",
    "overpass\n",
    "#\"NaT\" values are expected - indicates a date was not entered in input file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mullion'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a list for field sites included in prediction. If adding to the field sites, please leave the 1st entry, \"Mullion\",\n",
    "# as this is used to order secondary overpasses.\n",
    "sites = list(overpass.index)\n",
    "sites[0] # outputs 1st site. Should be 'Mullion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set satellite timesteps for overpasses\n",
    "# Depending on your application, you may want to add more accurate timesteps - but note that times used as inputs are \n",
    "# for the beginning of the acquisition, not the actual overpass time at a specific site.\n",
    "l8_timestep = timedelta(days=16)\n",
    "s2a_timestep = timedelta(days=10)\n",
    "s2b_timestep = timedelta(days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Site\n",
       "Mullion       2019-12-20 23:58:00\n",
       "Lake_George   2019-12-20 23:58:00\n",
       "Narrabundah   2019-12-20 23:58:00\n",
       "Name: sentinel_2a, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify start date\n",
    "l8_startdate = overpass['landsat_8'] \n",
    "l8_startdate_2 = overpass['landsat_8_2'] \n",
    "sentinel2a_startdate = overpass['sentinel_2a']\n",
    "sentinel2a_2_startdate = overpass['sentinel_2a_2']\n",
    "sentinel2b_startdate = overpass['sentinel_2b']\n",
    "sentinel2b_2_startdate = overpass['sentinel_2b_2']\n",
    "sentinel2a_startdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Times\n",
    "The following cell calculates times for Landsat and Sentinel overpasses for field sites. This is done by multiplying a number of desired iterations by the overpass frequency. \n",
    "\n",
    "Overpass frequencies for Landsat 8 = 16 days, Sentinel = 10 days. Increasing the number X in \"for i in range(X):\" will increase the number of iterations to predict over per satellite. An initial value of 20 iterations is set for L8, and 32 for Sentinel to result in the same total time period (320 days) to predict over.\n",
    "\n",
    "Predictions are also calculated for secondary overpasses for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Landsat 8 overpass prediction for 20*(overpass frequency) - ie 20*16 = 320 days. You can change this as desired,\n",
    "# to get overpass predictions for n days. n = x*(OP freq). X is arbitrarily set to 20 to give 320 days as a base example\n",
    "landsat = list()\n",
    "for i in range(20):\n",
    "    landsat.append(l8_startdate + l8_timestep*(i))\n",
    "landsat = pd.DataFrame(landsat)\n",
    "\n",
    "# Sentinel 2a overpass prediction for 32 * the overpass frequency, this is to give a similar total time to the L8 prediction.\n",
    "# OP frequency for Sentinel = 10 days, n days = 32*10 = 320\n",
    "Sentinel_2A = []  \n",
    "for i in range(32):\n",
    "    Sentinel_2A.append(sentinel2a_startdate + s2a_timestep * (i))\n",
    "Sentinel_2A = pd.DataFrame(Sentinel_2A)\n",
    "\n",
    "# Sentinel 2b\n",
    "Sentinel_2B = []\n",
    "for i in range(32):\n",
    "    Sentinel_2B.append(sentinel2b_startdate + s2b_timestep * (i))\n",
    "    \n",
    "Sentinel_2B = pd.DataFrame(Sentinel_2B)\n",
    "\n",
    "# Landasat_2\n",
    "# Prediction for L8 overpasses at sites which are covered by more than 1 overpass in a 16-day period.\n",
    "# - only Mullion in example .csv\n",
    "landsat_2 = []\n",
    "for i in range(20):\n",
    "    landsat_2.append(l8_startdate_2 + l8_timestep*(i))\n",
    "landsat_2 = pd.DataFrame(landsat_2)\n",
    "\n",
    "# Sentinel_2A_2\n",
    "# Sentinel 2a secondary (Mullion)\n",
    "Sentinel_2A_2 = []  \n",
    "for i in range(32):\n",
    "    Sentinel_2A_2.append(sentinel2a_2_startdate + s2a_timestep * (i))\n",
    "Sentinel_2A_2 = pd.DataFrame(Sentinel_2A_2)\n",
    "\n",
    "# Sentinel_2B_2\n",
    "# Sentinel 2b secondary (Lake_George, Narrabundah)\n",
    "Sentinel_2B_2 = []\n",
    "for i in range(32):\n",
    "    Sentinel_2B_2.append(sentinel2b_2_startdate + s2b_timestep * (i))\n",
    "Sentinel_2B_2 = pd.DataFrame(Sentinel_2B_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dataframes for sites that recieve more than 1 overpass in an orbital period.\n",
    "- If adding sites to the input file, please leave in \"Mullion\" - the 1st entry, as this is used to order secondary overpasses by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine Landsat 8 data (base plus extra overpasses)\n",
    "L8_combined2 = pd.concat([landsat, landsat_2])\n",
    "drop_label_L8 = L8_combined2.reset_index(drop=True)\n",
    "L8_combined2 = drop_label_L8.sort_values(by='Mullion') \n",
    "L8_combined2.index.names = ['Landsat_8']\n",
    "\n",
    "# Combine Sentinel 2A data (base plus extra overpasses)\n",
    "S2A_combined = pd.concat([Sentinel_2A, Sentinel_2A_2])\n",
    "drop_label_S2A = S2A_combined.reset_index(drop=True)\n",
    "S2A_combined = drop_label_S2A.sort_values(by='Mullion') \n",
    "S2A_combined.index.names = ['Sentinel_2A']\n",
    "\n",
    "# Combine Sentinel 2B data (base plus extra overpasses)\n",
    "S2B_combined = pd.concat([Sentinel_2B, Sentinel_2B_2])   \n",
    "drop_label_S2B = S2B_combined.reset_index(drop=True)\n",
    "S2B_combined = drop_label_S2B.sort_values(by='Mullion') \n",
    "S2B_combined.index.names = ['Sentinel_2B']\n",
    "\n",
    "# Add satellite label to each entry in \"Sat\" column\n",
    "S2A_combined['Sat'] = 'S2A'\n",
    "S2B_combined['Sat'] = 'S2B'\n",
    "L8_combined2['Sat'] = 'L8'\n",
    "\n",
    "combined = pd.concat([S2B_combined, S2A_combined, L8_combined2])\n",
    "df = pd.DataFrame(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a \"time dummy\" to force a re-order of data by date, to standardise across dataframes.\n",
    "today = datetime.today()\n",
    "timedummy = []\n",
    "t0 = datetime(today.year, today.month, 1)\n",
    "dummystep = timedelta(days=2)\n",
    "\n",
    "for i in range(300):\n",
    "    timedummy.append(t0 + dummystep * (i))\n",
    "    \n",
    "timedummy = pd.DataFrame(timedummy)\n",
    "df['DateStep'] = timedummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output field sites\n",
    "For new field sites, comment \"#\" out the old, add the new in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new df for each field site of interest. To add a new field site, add in same format ie \n",
    "# \"Field_Site = df[['Field_Site', 'Sat', 'DateStep']].copy()\" - if you are appending to the original file simply add in \"site4 = sites[3]\" \n",
    "# and \"Site4 = df[[(site4), 'Sat', 'DateStep']].copy()\" in the appropriate section below.\n",
    "site1 = sites[0]\n",
    "site2 = sites[1]\n",
    "site3 = sites[2]\n",
    "#site4 = sites[3]\n",
    "\n",
    "Site1 = df[[(site1), 'Sat', 'DateStep']].copy()\n",
    "Site2 = df[[(site2), 'Sat', 'DateStep']].copy()\n",
    "Site3 = df[[(site3), 'Sat', 'DateStep']].copy()\n",
    "#Site4 = df[[(site4), 'Sat', 'DateStep']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reorder by date for each site, include satellite tags for each date. \n",
    "# Add new lines for new sites as needed.\n",
    "Site1 = (Site1.sort_values(by=[(site1), 'DateStep'])).reset_index(drop=True)\n",
    "Site2 = (Site2.sort_values(by=[(site2), 'DateStep'])).reset_index(drop=True)\n",
    "Site3 = (Site3.sort_values(by=[(site3), 'DateStep'])).reset_index(drop=True)\n",
    "#Site4 = (Site4.sort_values(by=[(site4), 'DateStep'])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - output individual field site.\n",
    "- if you wish to output multiple field sites, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this cell to ouput an individual field site\n",
    "# Write individual site to excel: take out \"#\" symbol below, or add new line for new site in this format:\n",
    "\n",
    "#Site1.to_excel('Site1.csv')\n",
    "#Site2.to_excel('Site2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dataset\n",
    "The overpass predictions are combined and output to the specified \"output_path\". Times are in the same timezone as the input file. The example file used is in UTC, but you can also run the notebook on local times.\n",
    "\n",
    "To convert UTC (or whatever your input time was) to local time, ie AEST (or any other time zone), add 10h to the times or 11 for AEDT. This part is in the below cell, commented out. Remove the #'s to run the command on your desired column, and it will add the timestep.\n",
    "\n",
    "Yay, you have a list of overpasses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Overpass</th>\n",
       "      <th>Mullion</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Lake_George</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Narrabundah</th>\n",
       "      <th>Sat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-20 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-11-29 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-11-20 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-29 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-12-15 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-11-29 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-06 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-12-15 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2019-12-06 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-14 00:07:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2019-12-20 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2019-12-15 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-15 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-12-25 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2019-12-15 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-12-15 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2019-12-30 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2019-12-20 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-12-19 00:07:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2019-12-31 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2019-12-22 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-12-20 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-01-04 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2019-12-25 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-12-22 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2020-01-09 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2019-12-30 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-12-24 00:07:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-01-14 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2019-12-31 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-12-25 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-01-16 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2020-01-04 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-12-29 00:07:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-01-19 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-01-07 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-12-30 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-01-24 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-01-09 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-12-31 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2020-01-29 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-01-14 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-01-03 00:07:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-02-01 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2020-01-16 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-01-04 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-02-03 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-01-19 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-01-07 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2020-02-08 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-01-23 23:49:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-01-08 00:07:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-02-13 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "      <td>2020-01-24 23:58:00</td>\n",
       "      <td>S2B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-01-09 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-02-17 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "      <td>2020-01-29 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-01-13 00:07:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-02-18 23:58:00</td>\n",
       "      <td>S2A</td>\n",
       "      <td>2020-02-01 23:43:00</td>\n",
       "      <td>L8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Overpass             Mullion  Sat         Lake_George  Sat  \\\n",
       "0        2019-11-20 23:49:00   L8 2019-11-29 23:43:00   L8   \n",
       "1        2019-11-29 23:43:00   L8 2019-12-15 23:43:00   L8   \n",
       "2        2019-12-06 23:49:00   L8 2019-12-15 23:58:00  S2B   \n",
       "3        2019-12-14 00:07:00  S2A 2019-12-20 23:58:00  S2A   \n",
       "4        2019-12-15 23:43:00   L8 2019-12-25 23:58:00  S2B   \n",
       "5        2019-12-15 23:58:00  S2B 2019-12-30 23:58:00  S2A   \n",
       "6        2019-12-19 00:07:00  S2B 2019-12-31 23:43:00   L8   \n",
       "7        2019-12-20 23:58:00  S2A 2020-01-04 23:58:00  S2B   \n",
       "8        2019-12-22 23:49:00   L8 2020-01-09 23:58:00  S2A   \n",
       "9        2019-12-24 00:07:00  S2A 2020-01-14 23:58:00  S2B   \n",
       "10       2019-12-25 23:58:00  S2B 2020-01-16 23:43:00   L8   \n",
       "11       2019-12-29 00:07:00  S2B 2020-01-19 23:58:00  S2A   \n",
       "12       2019-12-30 23:58:00  S2A 2020-01-24 23:58:00  S2B   \n",
       "13       2019-12-31 23:43:00   L8 2020-01-29 23:58:00  S2A   \n",
       "14       2020-01-03 00:07:00  S2A 2020-02-01 23:43:00   L8   \n",
       "15       2020-01-04 23:58:00  S2B 2020-02-03 23:58:00  S2B   \n",
       "16       2020-01-07 23:49:00   L8 2020-02-08 23:58:00  S2A   \n",
       "17       2020-01-08 00:07:00  S2B 2020-02-13 23:58:00  S2B   \n",
       "18       2020-01-09 23:58:00  S2A 2020-02-17 23:43:00   L8   \n",
       "19       2020-01-13 00:07:00  S2A 2020-02-18 23:58:00  S2A   \n",
       "\n",
       "Overpass         Narrabundah  Sat  \n",
       "0        2019-11-20 23:49:00   L8  \n",
       "1        2019-11-29 23:43:00   L8  \n",
       "2        2019-12-06 23:49:00   L8  \n",
       "3        2019-12-15 23:43:00   L8  \n",
       "4        2019-12-15 23:58:00  S2B  \n",
       "5        2019-12-20 23:58:00  S2A  \n",
       "6        2019-12-22 23:49:00   L8  \n",
       "7        2019-12-25 23:58:00  S2B  \n",
       "8        2019-12-30 23:58:00  S2A  \n",
       "9        2019-12-31 23:43:00   L8  \n",
       "10       2020-01-04 23:58:00  S2B  \n",
       "11       2020-01-07 23:49:00   L8  \n",
       "12       2020-01-09 23:58:00  S2A  \n",
       "13       2020-01-14 23:58:00  S2B  \n",
       "14       2020-01-16 23:43:00   L8  \n",
       "15       2020-01-19 23:58:00  S2A  \n",
       "16       2020-01-23 23:49:00   L8  \n",
       "17       2020-01-24 23:58:00  S2B  \n",
       "18       2020-01-29 23:58:00  S2A  \n",
       "19       2020-02-01 23:43:00   L8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sites = ([Site1, Site2, Site3]) ## If you have more sites, ie you have added 1 or more, add here! An example is provided below if you have a total of 5 sites.\n",
    "#combined_sites = ([Site1, Site2, Site3, Site4, Site5])\n",
    "\n",
    "merged = pd.concat(combined_sites, axis=1)\n",
    "merged = merged.rename_axis(\"Overpass\", axis=\"columns\")\n",
    "output = merged.drop(['DateStep'], axis=1)\n",
    "\n",
    "#add timestep: (comment out / delete hash to use as applicable - must include column/site name) - ignore the error\n",
    "#output['Mullion']=output['Mullion']+datetime.timedelta(hours=10)\n",
    "#output['Lake_George']=output['Lake_George']+datetime.timedelta(hours=10)\n",
    "#output['Narrabundah']=output['Narrabundah']+datetime.timedelta(hours=10)\n",
    "\n",
    "output.to_csv(output_path)\n",
    "output.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** August 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`sandbox compatible`, :index:`sentinel 2`, :index:`landsat 8`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
