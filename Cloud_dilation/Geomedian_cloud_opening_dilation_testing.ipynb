{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99e46c0-1ccc-465f-9039-2b4a4399b3c9",
   "metadata": {},
   "source": [
    "# Compare Geomedian summaries with opening, dilation operations <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* [**Sign up to the DEA Sandbox**](https://docs.dea.ga.gov.au/setup/sandbox.html) to run this notebook interactively from a browser\n",
    "* **Compatibility:** Notebook currently compatible with the `DEA Sandbox` environment\n",
    "* **Products used:** \n",
    "TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a220b-395d-4967-b90b-1dcb612d54d3",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b346dcf-62be-4e2c-9969-3dc2e41c3cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "import matplotlib.pyplot as plt\n",
    "from odc.algo import mask_cleanup, erase_bad, enum_to_bool, to_f32, to_f32, xr_geomedian, int_geomedian\n",
    "from datacube.utils import masking\n",
    "from datacube.utils.cog import write_cog\n",
    "import fiona\n",
    "import rioxarray\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../Tools/')\n",
    "from dea_tools.datahandling import wofs_fuser\n",
    "from dea_tools.dask import create_local_dask_cluster\n",
    "\n",
    "from datacube.utils.masking import make_mask\n",
    "from datacube.utils.geometry import CRS, Geometry, GeoBox\n",
    "from dea_tools.plotting import rgb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import fiona\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from dea_tools.datahandling import load_ard\n",
    "\n",
    "sys.path.insert(1, \"../Tools/\")\n",
    "from dea_tools.dask import create_local_dask_cluster\n",
    "\n",
    "\n",
    "start_buffering = 0\n",
    "end_buffering = 10\n",
    "\n",
    "maturity = \"final\"\n",
    "#time_period = None\n",
    "#time_period = ('2023-01-01', '2023-03-01')\n",
    "time_period = \"2022\"\n",
    "\n",
    "bands = [\n",
    "    \"nbart_blue\",\n",
    "    \"nbart_red\",\n",
    "    \"nbart_green\",\n",
    "    \"nbart_nir\",\n",
    "    \"nbart_swir_1\",\n",
    "    \"nbart_swir_2\",\n",
    "    \"oa_fmask\",\n",
    "]\n",
    "\n",
    "products = [\n",
    "#    \"ga_ls5t_ard_3\",\n",
    "#    \"ga_ls7e_ard_3\",\n",
    "    \"ga_ls8c_ard_3\",\n",
    "    \"ga_ls9c_ard_3\",\n",
    "]\n",
    "\n",
    "path_row_list = [\n",
    "#    \"089084\", R\n",
    "    \"090084\",\n",
    "    \"090086\",\n",
    "    \"091086\"\n",
    "#    \"093086\", R\n",
    "#    \"096072\",\n",
    "#    \"098071\", R\n",
    "#    \"099079\",\n",
    "#    \"105069\"\n",
    "]\n",
    "\n",
    "geomedian_threads = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68635ed-ba43-4519-ad9c-27c1b877a232",
   "metadata": {},
   "source": [
    "### Data Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c75fe-cda7-4fd9-bbfd-6a540e163fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def landsat_scene_poly(path_row, radius=None):\n",
    "    \"\"\"\n",
    "    Get geometry for a given landsat path row\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_row : string\n",
    "        Path row to search for\n",
    "    radius : int, optional\n",
    "        If provided, the centroid of the path row geom will be buffered\n",
    "        by `radius` metres to provide a smaller geometry located within\n",
    "        the path row, reducing memory and processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geometry : geometry\n",
    "        geopolygon for given path row\n",
    "    \"\"\"\n",
    "\n",
    "    # Path to Landsat file on S3\n",
    "    landsat_shape = \"https://data.dea.ga.gov.au/derivative/ga_ls_path_row_grid.geojson\"\n",
    "\n",
    "    # Select feature\n",
    "    with fiona.open(landsat_shape) as all_shapes:\n",
    "        for s in all_shapes:\n",
    "            # landsat pathrows dont include 0 in front hence convert path-row to int to drop 0\n",
    "            if s[\"properties\"].get(\"PR\") == int(path_row):\n",
    "                # Extract geom\n",
    "                geom = Geometry(s[\"geometry\"], crs=CRS(\"EPSG:4326\"))\n",
    "\n",
    "                # Buffer centroid by X and return geom\n",
    "                if radius is not None:\n",
    "                    geom = geom.to_crs(\"EPSG:3577\").centroid.buffer(radius)\n",
    "\n",
    "                return geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faa097-3775-4960-a762-eb3f01cd666d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for full path-row geopolygon\n",
    "def define_query_params(path_row, time_period, maturity, radius=None):\n",
    "    \"\"\"\n",
    "    Create query params for odc load\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_row : string\n",
    "        Path row to search for\n",
    "    time_period : list\n",
    "        Time range\n",
    "    maturity : string\n",
    "        The dataset maturity level to include in the analysis\n",
    "    radius : int, optional\n",
    "        If provided, the centroid of the path row geom will be buffered\n",
    "        by `radius` metres to provide a smaller geometry located within\n",
    "        the path row, reducing memory and processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query_params : dictionary\n",
    "        qury params to use for odc load\n",
    "    \"\"\"\n",
    "    query_poly = landsat_scene_poly(path_row, radius)\n",
    "    query_params = dict(\n",
    "        geopolygon=query_poly,\n",
    "        time=time_period,\n",
    "        region_code=path_row,\n",
    "        dataset_maturity=maturity,\n",
    "    )\n",
    "    return query_params\n",
    "\n",
    "\n",
    "# for small scale fast tests\n",
    "\n",
    "# Sandy Beaches: query_params = dict(x=(115.16, 115.23), y=(-30.75, -30.83), dataset_maturity=maturity, time=time_period)\n",
    "# Alpine Snow: query_params = dict(x=(148.22, 148.39), y=(-36.37, -36.50), dataset_maturity=maturity, time=time_period)\n",
    "# Urban Area: query_params = dict(x=(144.77, 145.02), y=(-37.68, -37.85), dataset_maturity=maturity, time=time_period)\n",
    "# Salt Lakes: query_params = dict(x=(135.72, 135.94), y=(-31.26, -31.38), dataset_maturity=maturity, time=time_period)\n",
    "\n",
    "def define_query_params_lat_lon_test(time_period, maturity):\n",
    "    query_params = dict(x=(115.16, 115.23), y=(-30.75, -30.83), dataset_maturity=maturity, time=time_period)\n",
    "    return query_params\n",
    "\n",
    "def define_load_params(bands, load_product, query_params):\n",
    "    \"\"\"\n",
    "    Define load params\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bands : list\n",
    "        measurement bands\n",
    "    load_product : string\n",
    "        odc product\n",
    "    query_params: dictionary\n",
    "        odc query parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    load_params : dict\n",
    "        dictionary of load params\n",
    "    \"\"\"\n",
    "    # Find matching datasets\n",
    "    dss = dc.find_datasets(product=load_product, **query_params)\n",
    "\n",
    "    # Identify native CRS from datasets; fall back on \"EPSG:3577\"\n",
    "    # if no data is found to prevent an error\n",
    "    native_crs = dss[0].crs if len(dss) > 0 else \"EPSG:3577\"\n",
    "\n",
    "    # Set load params (measurements to load, Dask chunking, resampling etc)\n",
    "    load_params = dict(\n",
    "        measurements=bands,\n",
    "        output_crs=native_crs,  # Native CRS\n",
    "        resolution=(-30, 30),  # Native resolution\n",
    "        align=(15, 15),  # Required for native resolution load\n",
    "        group_by=\"solar_day\",\n",
    "        dask_chunks={},\n",
    "        skip_broken_datasets=True, # having around one-three failed timesteps due to s3 read errors for ls5\n",
    "    )\n",
    "        \n",
    "    return load_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0121c39-96fe-4e22-ab39-1b5a7ab4eb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(load_params, load_product, query_params):\n",
    "    \"\"\"\n",
    "    Load odc data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    load_params : dictionary\n",
    "        load parameters dictionary\n",
    "    load_product : string\n",
    "        odc product\n",
    "    query_params: dictionary\n",
    "        odc query parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : dataset\n",
    "        geospatial satellite data dataset\n",
    "    \"\"\"\n",
    "    # Lazily load data\n",
    "    ds = dc.load(product=load_product,\n",
    "                 **query_params,\n",
    "                 **load_params)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_data_geomedian(load_params, load_product, query_params):\n",
    "    \"\"\"\n",
    "    Load odc data for geomedian applying cloud filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    load_params : dictionary\n",
    "        load parameters dictionary\n",
    "    load_product : string\n",
    "        odc products\n",
    "    query_params: dictionary\n",
    "        odc query parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : dataset\n",
    "        geospatial satellite data dataset\n",
    "    \"\"\" \n",
    "    print(load_product)\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=[load_product],\n",
    "                  # remove filtering for geomedian comparisons\n",
    "                  #min_gooddata=0.90,\n",
    "                  **query_params,\n",
    "                  **load_params\n",
    "    )\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6976d54-7b1d-459d-8552-192d8148cbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_geomedian(ds, geomedian_threads):\n",
    "    \"\"\"\n",
    "    Apply geomedian\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : dataset\n",
    "        odc data\n",
    "    geomedian_threads: integer\n",
    "        number of threads for processing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geomedian : dataset\n",
    "     computed geomedian for input dataset\n",
    "    \"\"\"\n",
    "    geomedian = int_geomedian(ds, \n",
    "                              num_threads=geomedian_threads)\n",
    "    geomedian = geomedian.compute()\n",
    "    #rgb(geomedian, size=10)\n",
    "    return geomedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8098b-727b-449f-ad3f-fb13a2a6fcff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_cloud_shadow_mask(ds):\n",
    "    \"\"\"\n",
    "    Calculate cloud shadow mask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : dataset\n",
    "        data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cloud_shadow_mask : xr.DataArray\n",
    "        cloud shadow mask\n",
    "    nodata_mask: xr.DataArray\n",
    "        nodata mask\n",
    "    \"\"\"\n",
    "    # Identify pixels that are either \"nodata\", \"cloud\" or \"cloud_shadow\"\n",
    "    nodata_mask = enum_to_bool(ds.oa_fmask, categories=[\"nodata\"])\n",
    "    cloud_shadow_mask = enum_to_bool(ds.oa_fmask, categories=[\"cloud\", \"shadow\"])\n",
    "\n",
    "    return cloud_shadow_mask, nodata_mask\n",
    "\n",
    "\n",
    "# Plot\n",
    "# cloud_shadow_mask.isel(time=slice(4, 12)).plot(col=\"time\", col_wrap=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4dca1-3991-4834-aa06-9d876f7a7a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_std_gradient_buffer(\n",
    "    context, std_buffer_df, path_row, product, start_buffering, end_buffering, time_period, operation, export_figure=True\n",
    "):\n",
    "    # Set up three panel fig\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    # Apply numpy gradient to each column in dataset\n",
    "    std_gradient_df = std_buffer_df.apply(np.gradient, axis=0)\n",
    "\n",
    "    # Plot standard deviation\n",
    "    std_buffer_df.plot(\n",
    "        ax=axes[0],\n",
    "        xlabel=\"Buffer distance\",\n",
    "        ylabel=f\"{context}\",\n",
    "        title=f\"{context} per buffer pixel\",\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Plot gradient\n",
    "    std_gradient_df.plot(\n",
    "        ax=axes[1],\n",
    "        xlabel=\"Buffer distance\",\n",
    "        ylabel=f\"{context} gradient\",\n",
    "        title=f\"{context} gradient per buffer pixel\",\n",
    "    )\n",
    "\n",
    "    # Add labels to every second item\n",
    "    for index in std_gradient_df.index[::2]:\n",
    "        axes[1].text(\n",
    "            index,\n",
    "            std_gradient_df.nbart_blue.loc[index],\n",
    "            round(std_gradient_df.nbart_blue.loc[index], 2),\n",
    "            size=8,\n",
    "        )\n",
    "\n",
    "    # Plot mean of all gradients\n",
    "    std_gradient_df_mean = std_gradient_df.mean(axis=1).to_frame(\"All bands\")\n",
    "    std_gradient_df_mean.plot(\n",
    "        ax=axes[2],\n",
    "        xlabel=\"Buffer distance\",\n",
    "        ylabel=f\"{context} gradient\",\n",
    "        title=f\"{context} gradient per buffer pixel\",\n",
    "    )\n",
    "\n",
    "    # Add labels to every second item\n",
    "    for index in std_gradient_df_mean.index[::2]:\n",
    "        axes[2].text(\n",
    "            index,\n",
    "            std_gradient_df_mean[\"All bands\"].loc[index],\n",
    "            round(std_gradient_df_mean[\"All bands\"].loc[index], 2),\n",
    "            size=8,\n",
    "        )\n",
    "\n",
    "    # Set grid and x-ticks\n",
    "    for ax in axes:\n",
    "        ax.grid(alpha=0.1)\n",
    "        ax.set_xticks([i for i in range(start_buffering, end_buffering)])\n",
    "\n",
    "    # Add title above subplots\n",
    "    fig.suptitle(f\"Path/row {path_row}, {product}\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally export figure\n",
    "    if export_figure:\n",
    "        fig.savefig(\n",
    "            f\"output_data/{path_row}_{product}_{operation}_{context}_{time_period}_plots.jpg\", bbox_inches=\"tight\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bfb83-0447-4bb7-90c8-c56cd1853c52",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fef4a0-753b-44b1-9ae4-9a05421939c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='Geomedian_cloud_buffering')\n",
    "\n",
    "# Create local dask cluster\n",
    "client = create_local_dask_cluster(return_client=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b966a3-15cf-4dfc-a9a8-4b4f10750b03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Geomedian Analysis for Dilation of cloud and shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3edd8-aeb3-420a-b6fd-5d940e26ae98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datacube.utils.masking import mask_invalid_data\n",
    "\n",
    "# Loop through each validation path row\n",
    "for path_row in path_row_list:\n",
    "    for product in products:\n",
    "        \n",
    "        output_dir = f\"/gdata1/projects/cloud_masking/geomedian_dilation_results/{path_row}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "        # Create query parameters\n",
    "        query_params = define_query_params(\n",
    "            path_row, time_period, maturity, radius=50000\n",
    "        )\n",
    "        \n",
    "        # Small Test Area\n",
    "        #query_params = define_query_params_lat_lon_test(time_period, maturity)\n",
    "        \n",
    "        # Create Load Parameters\n",
    "        load_params = define_load_params(bands, product, query_params)\n",
    "\n",
    "        # Load ARD Data\n",
    "        ds = load_data(load_params, product, query_params).persist()\n",
    "\n",
    "        # Create cloud and shadow mask\n",
    "        cloud_shadow_mask, nodata_mask = calc_cloud_shadow_mask(ds)\n",
    "\n",
    "        output_dict = {}\n",
    "        \n",
    "        # Loop through each buffer radius\n",
    "        for buffer in range(start_buffering, end_buffering + 1):\n",
    "\n",
    "            dilation = buffer\n",
    "            \n",
    "            # create cloud and shadow masked datasets with specified buffer\n",
    "            clouds_shadows_dilation = mask_cleanup(cloud_shadow_mask, mask_filters=[('dilation', buffer)])\n",
    "            ds_buffer = ds.where(~clouds_shadows_dilation)\n",
    "            \n",
    "            # Set invalid nodata pixels to NaN\n",
    "            ds_buffer_valid = mask_invalid_data(ds_buffer)\n",
    "            \n",
    "            print(f\"Calculating geomedian with {buffer} pixel buffer applied\")\n",
    "            ds_geomedian = apply_geomedian(ds_buffer_valid, geomedian_threads)\n",
    "            \n",
    "            # Save the geomedian data\n",
    "            rgb(ds_geomedian, size=10, savefig_path=f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_{str(time_period)}.tif\")\n",
    "            ds_geomedian.nbart_blue.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels__{time_period}_nbart_blue.tif\")\n",
    "            ds_geomedian.nbart_red.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels_{time_period}_nbart_red.tif\")\n",
    "            ds_geomedian.nbart_green.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels_{time_period}_nbart_green.tif\")\n",
    "            ds_geomedian.nbart_nir.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels_{time_period}_nbart_nir.tif\")\n",
    "            ds_geomedian.nbart_swir_1.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels_{time_period}_nbart_swir_1.tif\")\n",
    "            ds_geomedian.nbart_swir_2.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels_{time_period}_nbart_swir_2.tif\")\n",
    "            ds_geomedian.oa_fmask.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_dilation_{dilation}_pixels_{time_period}_oa_fmask.tif\")\n",
    "            \n",
    "            # drop fmask from std results\n",
    "            ds_geomedian = ds_geomedian.drop(\"oa_fmask\")\n",
    "            \n",
    "            print(\"Calc standard deviation of the difference between 0 and X buffer\")\n",
    "            std_ds = ds_geomedian.std().mean().compute()\n",
    "            std_df = std_ds.to_array().to_dataframe(name=\"std\")\n",
    "            output_dict[buffer] = std_df\n",
    "            # Print results\n",
    "            print(\n",
    "                f\"Buffer in pixels: {buffer}, {': '.join(std_df.round(1).to_string(index_names=False).split())}\"\n",
    "            )\n",
    "            \n",
    "        # Concatenate outputs into a single dataframe, then unstack to wide \n",
    "        # format with each variable as a column\n",
    "        std_geomedian_df = pd.concat(output_dict, names=[\"pixel_buffer\", \"variable\"])[\n",
    "            \"std\"\n",
    "        ].unstack(\"variable\")\n",
    "        \n",
    "        # Export results as csv with a \"product\" and \"path_row\" column\n",
    "        std_geomedian_df.assign(product=product, path_row=path_row).to_csv(\n",
    "            f\"{output_dir}/geomedian_std_buffer{path_row}_{product}_dilation_{dilation}_{str(time_period)}.csv\", index=True\n",
    "        )\n",
    "\n",
    "        # Plot the standard deviation and gradient results\n",
    "        plot_std_gradient_buffer(\n",
    "            \"Standard Deviation\",\n",
    "            std_geomedian_df,\n",
    "            path_row,\n",
    "            product,\n",
    "            start_buffering,\n",
    "            end_buffering,\n",
    "            str(time_period),\n",
    "            f\"dilation_{dilation}\",\n",
    "            export_figure=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ed8cc-f66a-4607-a11a-f664043324fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Geomedian Analysis for Opening on cloud and Dilation on cloud and shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a28cc9-b85e-491b-8157-9b7eb2428603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datacube.utils.masking import mask_invalid_data\n",
    "\n",
    "# Loop through each validation path row\n",
    "for path_row in path_row_list:\n",
    "    for product in products:\n",
    "        \n",
    "        output_dir = f\"/gdata1/projects/cloud_masking/geomedian_opening_dilation_results/{path_row}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "        # Create query parameters\n",
    "        # small test example\n",
    "        #query_params = define_query_params_lat_lon_test(time_period, maturity)\n",
    "        \n",
    "        # full specified area\n",
    "        query_params = define_query_params(\n",
    "            path_row, time_period, maturity, radius=50000\n",
    "        )\n",
    "        \n",
    "        load_params = define_load_params(bands, product, query_params)\n",
    "\n",
    "        # Load ARD Data\n",
    "        ds = load_data(load_params, product, query_params).persist()\n",
    "\n",
    "        # Create three seperate masks for cloud, shadow and no data\n",
    "        cloud_mask = enum_to_bool(ds.oa_fmask, categories=[\"cloud\"])\n",
    "        shadow_mask = enum_to_bool(ds.oa_fmask, categories=[\"shadow\"])\n",
    "        nodata_mask = enum_to_bool(ds.oa_fmask, categories=[\"nodata\"])\n",
    "\n",
    "        output_dict = {}\n",
    "        \n",
    "        dilation = 6\n",
    "        \n",
    "        # Loop through each buffer radius\n",
    "        for buffer in range(start_buffering, end_buffering + 1):\n",
    "            \n",
    "            print(f\"Creating masks with {buffer} pixel buffer applied\")\n",
    "            \n",
    "            # Opening on cloud mask with varying level of pixels applied\n",
    "            opening_cloud_mask = mask_cleanup(cloud_mask, mask_filters=[('opening', buffer)])\n",
    "            \n",
    "            # Combine mask with cloud and shadow\n",
    "            combined_opening_cloud_mask = shadow_mask | opening_cloud_mask\n",
    "            \n",
    "            # Run combined mask against fixed dilation on cloud + shadow\n",
    "            combined_opening_dilation_mask = mask_cleanup(combined_opening_cloud_mask, mask_filters=[('dilation', dilation)])\n",
    "            \n",
    "            # Apply no data mask\n",
    "            combined_mask = combined_opening_dilation_mask | nodata_mask\n",
    "\n",
    "            # Run mask on dataset\n",
    "            ds_buffer = ds.where(~combined_mask)\n",
    "            \n",
    "            # Set invalid nodata pixels to NaN\n",
    "            ds_buffer_valid = mask_invalid_data(ds_buffer)\n",
    "\n",
    "            # Plot the inividual time series\n",
    "            \"\"\"\n",
    "            rgb(\n",
    "                ds_buffer, \n",
    "                size=10,\n",
    "                col=\"time\",\n",
    "                #savefig_path=f\"{output_dir}/geomedian_{path_row}_{product}_{buffer}_{time_period}.tif\"\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            # Apply geomedian\n",
    "            print(f\"Calculating geomedian with {buffer} pixel buffer applied\")\n",
    "            ds_geomedian = apply_geomedian(ds_buffer_valid, geomedian_threads)\n",
    "            \n",
    "            # Convert to float, setting all nodata pixels to `np.nan` (required\n",
    "            # for the standard deviation calculation)\n",
    "            # ds_geomedian = to_f32(ds_geomedian)\n",
    "            \n",
    "            # Save the geomedian data\n",
    "            rgb(ds_geomedian, size=10, savefig_path=f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_{str(time_period)}.tif\")\n",
    "            ds_geomedian.nbart_blue.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels__{time_period}_nbart_blue.tif\")\n",
    "            ds_geomedian.nbart_red.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels_{time_period}_nbart_red.tif\")\n",
    "            ds_geomedian.nbart_green.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels_{time_period}_nbart_green.tif\")\n",
    "            ds_geomedian.nbart_nir.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels_{time_period}_nbart_nir.tif\")\n",
    "            ds_geomedian.nbart_swir_1.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels_{time_period}_nbart_swir_1.tif\")\n",
    "            ds_geomedian.nbart_swir_2.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels_{time_period}_nbart_swir_2.tif\")\n",
    "            ds_geomedian.oa_fmask.rio.to_raster(f\"{output_dir}/geomedian_{path_row}_{product}_opening_{buffer}_dilation_{dilation}_pixels_{time_period}_oa_fmask.tif\")\n",
    "            \n",
    "            # drop fmask from std results\n",
    "            ds_geomedian = ds_geomedian.drop(\"oa_fmask\")\n",
    "            \n",
    "            print(\"Calc standard deviation of the difference between 0 and X buffer\")\n",
    "            std_ds = ds_geomedian.std().mean().compute()\n",
    "            std_df = std_ds.to_array().to_dataframe(name=\"std\")\n",
    "            output_dict[buffer] = std_df\n",
    "            # Print results\n",
    "            print(\n",
    "                f\"Buffer in pixels: {buffer}, {': '.join(std_df.round(1).to_string(index_names=False).split())}\"\n",
    "            )\n",
    "            \n",
    "        # Concatenate outputs into a single dataframe, then unstack to wide \n",
    "        # format with each variable as a column\n",
    "        std_geomedian_df = pd.concat(output_dict, names=[\"pixel_buffer\", \"variable\"])[\n",
    "            \"std\"\n",
    "        ].unstack(\"variable\")\n",
    "        \n",
    "        # Export results as csv with a \"product\" and \"path_row\" column\n",
    "        std_geomedian_df.assign(product=product, path_row=path_row).to_csv(\n",
    "            f\"{output_dir}/geomedian_std_buffer{path_row}_{product}_opening_{buffer}_dilation_{dilation}_{str(time_period)}.csv\", index=True\n",
    "        )\n",
    "\n",
    "        # Plot the standard deviation and gradient results\n",
    "        plot_std_gradient_buffer(\n",
    "            \"Standard Deviation\",\n",
    "            std_geomedian_df,\n",
    "            path_row,\n",
    "            product,\n",
    "            start_buffering,\n",
    "            end_buffering,\n",
    "            str(time_period),\n",
    "            f\"opening_dilation_{dilation}\",\n",
    "            export_figure=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4d0ac-a040-4cba-82f4-5e94816cefea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
