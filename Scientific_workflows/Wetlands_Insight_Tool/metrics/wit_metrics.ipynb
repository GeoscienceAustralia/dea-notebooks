{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate metrics with WIT data\n",
    "* **Compatibility:** Notebook currently compatible with the NCI VDI or DEA sandbox\n",
    "* **Special requirements:**\n",
    "    * If running on the NCI, ensure that `module load dea` is run prior to launching this notebook\n",
    "    * If on NCI and NCI only, in need of loading data from the database, check you have the latest version of the `wit_tooling` package by copying and pasting the following code into a cell below and running the cell. You will need to restart the notebook kernel after running this cell.\n",
    "\n",
    "        `!pip install --user git+git://github.com/GeoscienceAustralia/wit_tooling`\n",
    "\n",
    "* **Products used:** \n",
    "     * Collection 2 Landsat Surface Reflectance:\n",
    "        [ls5_nbart_albers](https://explorer.dea.ga.gov.au/ls5_nbart_albers)\n",
    "        [ls7_nbart_albers](https://explorer.dea.ga.gov.au/ls7_nbart_albers)\n",
    "        [ls8_nbart_albers](https://explorer.dea.ga.gov.au/ls8_nbart_albers)\n",
    "     * Collection 2 Landsat Fractional Cover,generated using the Joint Remote Sensing Research Program algorithm:\n",
    "        [ls5_fc_albers](https://explorer.dea.ga.gov.au/ls5_fc_albers)\n",
    "        [ls7_fc_albers](https://explorer.dea.ga.gov.au/ls7_fc_albers)\n",
    "        [ls8_fc_albers](https://explorer.dea.ga.gov.au/ls8_fc_albers)\n",
    "     * Water Observations from Space, generated using the Geoscience Australia Algorithm:\n",
    "        [wofs_albers](https://explorer.sandbox.dea.ga.gov.au/wofs_albers)\n",
    "\n",
    "* **Dependencies**: This code requires two things to run (see the analysis parameters section for more information):\n",
    "     * A pre-calculated WIT csv\n",
    "     * A shapefile (or equivalent) that contains the area that the WIT result was run over.\n",
    "     * A csv containing IDs of polygons from the shapefile\n",
    "     \n",
    "## Background\n",
    "The WIT data are generated by DEA with given wetland polygons and stored in a database on NCI. The data can be dumped into a csv when required. Any statistics can be generated with WIT data. This notebook provides a way in computing temporal statistics (metrics).\n",
    "## Description\n",
    "This notebook uses existing WIT data to compute metrics.\n",
    "* First we load the existing WIT csv data from a saved csv location\n",
    "* Then we compute the metrics for all polygons and output the results to csvs\n",
    "\n",
    "## Data definition\n",
    "* WIT data\n",
    "\n",
    "```\n",
    "   TIME: time of obersavation\n",
    "   BS: percentage of bare soil\n",
    "   NPV: percentage of non photosynthetic vegetation\n",
    "   PV: percentage of green/photosynthetic vegetation\n",
    "   WET: percentage of wetness\n",
    "   WATER: percentage of water\n",
    "```\n",
    "* WIT metrics: refer [WIT metrics](https://docs.google.com/document/d/1JBZzVRW6K0fJT4jws3lRranPLPBYBkTDvpu94knv5dY/edit?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "* Follow the instructions under **Special Requirements** above to `load dea`and install `wit_tooling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import io\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed if loading data from database\n",
    "# and only works on NCI\n",
    "load_from_db = False\n",
    "if load_from_db:\n",
    "    from wit_tooling import query_wit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_list(key, values, shapefile):\n",
    "    \"\"\"\n",
    "        Get a generator of shapes from the given shapefile\n",
    "            key: the key to match in 'properties' in the shape file\n",
    "            values: a list of property values\n",
    "            shapefile: the name of your shape file\n",
    "            e.g. key='ORIGID', values=[1, 2, 3, 4, 5], \n",
    "            shapefile='/g/data/r78/DEA_Wetlands/shapefiles/MDB_ANAE_Aug2017_modified_2019_SB_3577.shp'\n",
    "    \"\"\"\n",
    "    count = len(values)\n",
    "    with fiona.open(shapefile) as allshapes:\n",
    "        for shape in allshapes:\n",
    "            shape_id = shape['properties'].get(key)\n",
    "            if shape_id is None:\n",
    "                continue\n",
    "            if shape_id in values:\n",
    "                yield(shape_id, shape)\n",
    "                count -= 1\n",
    "            if count <= 0:\n",
    "                break\n",
    "    \n",
    "#def get_areas(features, pkey='SYSID'):\n",
    "def get_areas(features, pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Calculate the area of a list/generator of shapes\n",
    "        input:\n",
    "            features: a list of shapes indexed by the key\n",
    "        output:\n",
    "            a dataframe of area index by the key\n",
    "    \"\"\"\n",
    "    re = pd.DataFrame()\n",
    "    for f in features:\n",
    "        va = pd.DataFrame([[f[0], geometry.shape(f[1]['geometry']).area/1e4]], columns=[pkey, 'area'])\n",
    "        re = re.append(va, sort=False)\n",
    "    return re.set_index(pkey)\n",
    "\n",
    "def dump_wit_data(key, feature_list, output):\n",
    "    \"\"\"\n",
    "        dump wit data from the database into a file\n",
    "        input:\n",
    "            key: Name to id the polygon\n",
    "            feature_list: a list or generator of features\n",
    "        output:\n",
    "            a csv file to save all the wit data\n",
    "    \"\"\"\n",
    "    for f_id, f in feature_list:\n",
    "        _, wit_data = query_wit_data(f)\n",
    "        csv_buf = io.StringIO()\n",
    "        wit_df = pd.DataFrame(data=wit_data, columns=['TIME', 'BS', 'NPV', 'PV', 'WET', 'WATER'])\n",
    "        wit_df.insert(0, key, f_id)\n",
    "        wit_df.to_csv(csv_buf, index=False, header=False)\n",
    "        csv_buf.seek(0)\n",
    "        with open(output, 'a') as f:\n",
    "            f.write(csv_buf.read())\n",
    "    with open(output, 'a') as f:\n",
    "        f.write(','.join(list(wit_df.columns))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_metrics(wit_data, members=['PV', 'WET', 'WATER', 'BS', 'NPV', ['NPV', 'PV', 'WET'],\n",
    "                                          ['PV', 'WET'], ['WATER', 'WET']], threshold=[25, 75], pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Compute the annual max, min, mean, count with given wit data, members and threshold\n",
    "        input:\n",
    "            wit_data: dataframe of WIT\n",
    "            members: the elements which the metrics are computed against, can be a column from wit_data, e.g. 'PV'\n",
    "                         or the sum of wit columns, e.g. ['WATER', 'WET']\n",
    "            threshold: a list of thresholds such that (elements >= threshold[i]) is True, \n",
    "                        where i = 0, 1...len(threshold)-1\n",
    "        output:\n",
    "            dataframe of metrics\n",
    "    \"\"\"\n",
    "    years = wit_data['TIME']\n",
    "    i = 0\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    for m in members:\n",
    "        if isinstance(m, list):\n",
    "            wit_df.insert(wit_df.columns.size+i, '+'.join(m), wit_df[m].sum(axis=1))\n",
    "    years = pd.DatetimeIndex(wit_df['TIME']).year.unique()\n",
    "    shape_id_list = wit_df[pkey].unique()\n",
    "    #shane changed 4 to 5 to accomodate median added below \n",
    "    wit_metrics = [pd.DataFrame()] * 5\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['TIME']).year==y].drop(columns=['TIME']).groupby(pkey).max()\n",
    "        wit_yearly.insert(0, 'YEAR', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_max' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[0] = wit_metrics[0].append(wit_yearly, sort=False)\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['TIME']).year==y].drop(columns=['TIME']).groupby(pkey).min()\n",
    "        wit_yearly.insert(0, 'YEAR', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_min' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[1] = wit_metrics[1].append(wit_yearly, sort=False)\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['TIME']).year==y].drop(columns=['TIME']).groupby(pkey).mean()\n",
    "        wit_yearly.insert(0, 'YEAR', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_mean' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[2] = wit_metrics[2].append(wit_yearly, sort=False)\n",
    "        \n",
    "    #*********************** START ADDED BY SHANE ***********************\n",
    "    #adding median\n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['TIME']).year==y].drop(columns=['TIME']).groupby(pkey).median()\n",
    "        wit_yearly.insert(0, 'YEAR', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: n+'_median' for n in wit_yearly.columns[1:]})\n",
    "        wit_metrics[3] = wit_metrics[3].append(wit_yearly, sort=False)\n",
    "    #*********************** END ADDED BY SHANE ***********************      \n",
    "    for y in years:\n",
    "        wit_yearly = wit_df[pd.DatetimeIndex(wit_df['TIME']).year==y][[pkey, 'BS']].groupby(pkey).count()\n",
    "        wit_yearly.insert(0, 'YEAR', y)\n",
    "        wit_yearly = wit_yearly.rename(columns={n: 'count' for n in wit_yearly.columns[1:]})\n",
    "        #shane changed index from 3 to 4 to accomodate median added above \n",
    "        wit_metrics[4] = wit_metrics[4].append(wit_yearly, sort=False)\n",
    "    for t in threshold:\n",
    "        wit_df_ts = wit_df.copy(deep=True)\n",
    "        wit_metrics += [pd.DataFrame()]\n",
    "        wit_df_ts.loc[:, wit_df_ts.columns[2:]] = wit_df_ts.loc[:, wit_df_ts.columns[2:]].mask((wit_df_ts[wit_df_ts.columns[2:]] < t/100), np.nan)\n",
    "        for y in years:\n",
    "            wit_yearly = wit_df_ts[pd.DatetimeIndex(wit_df_ts['TIME']).year==y].drop(columns=['TIME']).groupby(pkey).count()\n",
    "            wit_yearly.insert(0, 'YEAR', y)\n",
    "            wit_yearly = wit_yearly.rename(columns={n: n+'_count'+str(t) for n in wit_yearly.columns[1:]})\n",
    "            wit_metrics[-1] = wit_metrics[-1].append(wit_yearly, sort=False)\n",
    "    wit_yearly_metrics = wit_metrics[0]\n",
    "    for i in range(len(wit_metrics)-1):\n",
    "        wit_yearly_metrics = pd.merge(wit_yearly_metrics, wit_metrics[i+1], on=[pkey, 'YEAR'], how='inner')\n",
    "    return wit_yearly_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_time(wit_ww, threshold, pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Compute inundation event time by given threshold\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            threshold: a value such that (WATER+WET > threshold) = inundation\n",
    "        output:\n",
    "            dateframe of inundation event time\n",
    "    \"\"\"\n",
    "    if isinstance(threshold, pd.DataFrame):\n",
    "        gid = wit_ww.index.unique()[0]\n",
    "        poly_threshold = threshold.loc[gid].to_numpy()[0]\n",
    "    else:\n",
    "        poly_threshold = threshold\n",
    "    i_start = wit_ww[wit_ww['WW'] >= poly_threshold]['TIME'].min()\n",
    "    if pd.isnull(i_start):\n",
    "        re = pd.DataFrame([[np.nan] * 4], columns=['start_time', 'end_time', 'duration', 'gap'], index=wit_ww.index.unique())\n",
    "        re.index.name = pkey\n",
    "        return re\n",
    "    re_idx = np.searchsorted(wit_ww[(wit_ww['WW'] < poly_threshold)]['TIME'].values, \n",
    "                             wit_ww[(wit_ww['WW'] >= poly_threshold)]['TIME'].values)\n",
    "    re_idx, count = np.unique(re_idx, return_counts=True)\n",
    "    start_idx = np.zeros(len(count)+1, dtype='int')\n",
    "    start_idx[1:] = np.cumsum(count)\n",
    "    re_start = wit_ww[(wit_ww['WW'] >= poly_threshold)].iloc[start_idx[:-1]][['TIME']].rename(columns={'TIME': 'start_time'})\n",
    "    re_end = wit_ww[(wit_ww['WW'] >= poly_threshold)].iloc[start_idx[1:] - 1][['TIME']].rename(columns={'TIME': 'end_time'})\n",
    "    re = pd.concat([re_start, re_end], axis=1)\n",
    "    re.insert(2, 'duration', \n",
    "              (re['end_time'] - re['start_time'] + np.timedelta64(1, 'D')).astype('timedelta64[D]').astype('timedelta64[D]'))\n",
    "    re.insert(3, 'gap', np.concatenate([[np.timedelta64(0, 'D')],\n",
    "                                        (re['start_time'][1:].values - re['end_time'][:-1].values - np.timedelta64(1, 'D')).astype('timedelta64[D]')]))\n",
    "    re.insert(0, pkey, wit_ww.index.unique()[0])\n",
    "    return re.set_index(pkey)\n",
    "    \n",
    "def get_im_stats(grouped_wit, im_time, wit_area):\n",
    "    \"\"\"\n",
    "        Get inundation stats given wit data and events\n",
    "        input:\n",
    "            grouped_wit: wit data\n",
    "            im_time: inundation events in time\n",
    "        output:\n",
    "            the stats of inundation events\n",
    "    \"\"\"\n",
    "    gid = grouped_wit.index.unique()[0]\n",
    "    if gid not in im_time.indices.keys():\n",
    "        return pd.DataFrame([[np.nan]*5], columns=['start_time', 'max_wet', 'mean_wet', 'max_wet_area', 'mean_wet_area'],\n",
    "                           index=[gid])\n",
    "    re_left = np.searchsorted(grouped_wit['TIME'].values.astype('datetime64'),\n",
    "                         im_time.get_group(gid)['start_time'].values, side='left')\n",
    "    re_right = np.searchsorted(grouped_wit['TIME'].values.astype('datetime64'),\n",
    "                         im_time.get_group(gid)['end_time'].values, side='right')\n",
    "    re = pd.DataFrame()\n",
    "    for a, b in zip(re_left, re_right):\n",
    "        tmp = pd.concat([grouped_wit.iloc[a:a+1]['TIME'].rename('start_time').astype('datetime64'),\n",
    "                         pd.Series(grouped_wit.iloc[a:b]['WW'].max(),index=[gid], name='max_wet'),\n",
    "                         pd.Series(grouped_wit.iloc[a:b]['WW'].mean(),index=[gid], name='mean_wet')],\n",
    "                        axis=1)\n",
    "        tmp.insert(3, 'max_wet_area', tmp['max_wet'].values * wit_area[wit_area.index==gid].values)\n",
    "        tmp.insert(4, 'mean_wet_area', tmp['mean_wet'].values * wit_area[wit_area.index==gid].values)\n",
    "        re = re.append(tmp, sort=False)\n",
    "    re.index.name = grouped_wit.index.name\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_time(wit_df, threshold=0.01, pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Compute the inundation events with given wit data and threshold\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            threshold: a value such that (WATER+WET > threshold) = inundation,\n",
    "        output:\n",
    "            dataframe of events\n",
    "    \"\"\"\n",
    "    return wit_df.groupby(pkey).apply(get_event_time, threshold=threshold, pkey=pkey).dropna().droplevel(0)\n",
    "\n",
    "def event_stats(wit_df, wit_im, wit_area, pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Compute inundation event stats with given wit wetness, events defined by (start_time, end_time) \n",
    "        and polygon areas\n",
    "        input:\n",
    "            wit_df: wetness computed from wit data\n",
    "            wit_im: inundation event\n",
    "            wit_area: polygon areas indexed by the key\n",
    "        output:\n",
    "            dataframe of event stats\n",
    "    \"\"\"\n",
    "    grouped_im = wit_im[['start_time', 'end_time']].groupby(pkey)\n",
    "    return wit_df.groupby(pkey).apply(get_im_stats, im_time=grouped_im, wit_area=wit_area).droplevel(0)\n",
    "\n",
    "def inundation_metrics(wit_data, wit_area, threshold=0.01, pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Compute inundation metrics with given wit data, polygon areas and threshold\n",
    "        input:\n",
    "            wit_data: a dataframe of wit_data\n",
    "            wit_area: polygon areas indexed by the key\n",
    "            threshold: a value such that (WATER+WET > threshold) = inundation\n",
    "        output:\n",
    "            dataframe of inundation metrics\n",
    "    \"\"\"\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    wit_df.insert(2, 'WW', wit_df[['WATER', 'WET']].sum(axis=1))\n",
    "    wit_df = wit_df.drop(columns=wit_df.columns[3:])\n",
    "    wit_df['TIME'] = wit_df['TIME'].astype('datetime64')\n",
    "    wit_df = wit_df.set_index(pkey)\n",
    "    wit_im_time = event_time(wit_df, threshold, pkey)\n",
    "    wit_im_stats = event_stats(wit_df, wit_im_time, wit_area, pkey)\n",
    "    return pd.merge(wit_im_time, wit_im_stats.dropna(), on=[pkey, 'start_time'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_wit(grouped_wit, pkey='SYSID'):\n",
    "    daily_wit = pd.DataFrame({pkey: grouped_wit[pkey].unique()[0], 'TIME': pd.date_range(grouped_wit['TIME'].astype('datetime64[D]').min(), grouped_wit['TIME'].astype('datetime64[D]').max(), freq='D'),\n",
    "                          'BS': np.nan, 'NPV': np.nan, 'PV': np.nan, 'WET': np.nan, 'WATER': np.nan})\n",
    "    _, nidx, oidx = np.intersect1d(daily_wit['TIME'].to_numpy().astype('datetime64[D]'), grouped_wit['TIME'].to_numpy().astype('datetime64[D]'),\n",
    "                  return_indices=True)\n",
    "    daily_wit.loc[nidx, [\"BS\",\"NPV\",\"PV\",\"WET\",\"WATER\"]]  = grouped_wit[[\"BS\",\"NPV\",\"PV\",\"WET\",\"WATER\"]].iloc[oidx].to_numpy()\n",
    "    daily_wit = daily_wit.interpolate(axis=0)\n",
    "    return daily_wit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_time_median(wit_data, members=[['WATER', 'WET']], pkey='SYSID'):\n",
    "    \"\"\"\n",
    "        Compute the all time median\n",
    "        input:\n",
    "            wit_data: dataframe of WIT\n",
    "            members: the elements which the metrics are computed against, can be a column from wit_data, e.g. 'PV'\n",
    "                         or the sum of wit columns, e.g. ['WATER', 'WET']\n",
    "        output:\n",
    "            dataframe of median indexed by pkey\n",
    "    \"\"\"\n",
    "    wit_df = wit_data.copy(deep=True)\n",
    "    i = 0\n",
    "    for m in members:\n",
    "        if isinstance(m, list):\n",
    "            wit_df.insert(wit_df.columns.size+i, '+'.join(m), wit_df[m].sum(axis=1))\n",
    "        i += 1\n",
    "    return wit_df.groupby(pkey).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section is to compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the files needed, upload the files below to the sandbox if run there\n",
    "# set the path accordingly\n",
    "# fid_file: a file with all the ids needed to locate polygons in the shape file\n",
    "# shapefile: the shape file mentioned above to find the polygons\n",
    "# wit_data_file: a csv with wit data dumped from the database\n",
    "fid_file = './example_data/ramsar_REFCODE.csv'\n",
    "shapefile = './example_data/ramsar_wetlands_3577_20190403.shp'\n",
    "wit_data_file = './example_data/ramsar_wit.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change pkey to match the key of iding polygon\n",
    "# e.g. pkey = 'REFCODE' if using the files in example_data\n",
    "#pkey = 'SYSID'\n",
    "pkey = 'REFCODE'\n",
    "features = shape_list(pkey, pd.read_csv(fid_file, header=None).values, shapefile)\n",
    "####################################################################################\n",
    "# This section is to dump the WIT data from database to a csv, DO NOT do this unless:\n",
    "# * a. you know what it's doing or\n",
    "# * b. you were told so and\n",
    "# * c. you have access to the database and\n",
    "# * d. the notebook runs on NCI\n",
    "# load all the wit data required from the database and save them to a csv file\n",
    "if load_from_db:\n",
    "    dump_wit_data(pkey, features, wit_data_file)\n",
    "####################################################################################\n",
    "# get the area of polygons\n",
    "wit_area = get_areas(features, pkey=pkey)\n",
    "# load wit data into a pandas dataframe\n",
    "wit_data = pd.read_csv(wit_data_file, header=None, skipfooter=1, \n",
    "                       names=[pkey,\"TIME\",\"BS\",\"NPV\",\"PV\",\"WET\",\"WATER\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute yearly metrics and save the results to a csv\n",
    "# set the output file to your own path\n",
    "wit_yearly_metrics = annual_metrics(wit_data, pkey=pkey)\n",
    "ofn = \"ANAE_yearly_metrics.csv\"\n",
    "wit_yearly_metrics.to_csv(ofn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all time median, serve as the threshold\n",
    "# save the threshold in a csv, set the output file to your own path\n",
    "wit_median = all_time_median(wit_data, pkey=pkey)\n",
    "threshold_list = wit_median[['WATER+WET']]\n",
    "ofn = \"ANAE_event_threshold.csv\"\n",
    "threshold_list.to_csv(ofn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxdate = pd.pivot_table(wit_data, index=pkey, values=['TIME'], aggfunc=np.max)\n",
    "maxdate['TIME'] = maxdate['TIME'].astype('datetime64')\n",
    "\n",
    "# optional read in threshold_list had it been changed outside of the notebook\n",
    "# threshold_list = pd.read_csv('ANAE_event_threshold.csv', index_col=0)\n",
    "# here we'll use the threshold_list computed in the cell above\n",
    "\n",
    "\"\"\" \n",
    "    interpolate wit data in daily frequency\n",
    "    comment this out for now as data volume crashes sandbox\n",
    "    note should be able to adjust event metrics calculations\n",
    "    to use linear estimation of time pre- and post- events.\n",
    "\"\"\"\n",
    "#wit_data = wit_data.groupby(pkey).apply(interpolate_wit, pkey=pkey).droplevel(0)\n",
    "\n",
    "# compute event metrics with threshold_list then save the results to a csv\n",
    "# set the output file to your own path\n",
    "\n",
    "wit_im =inundation_metrics(wit_data, wit_area, threshold_list, pkey=pkey)\n",
    "ofn = 'ANAE_inudation_metrics.csv'\n",
    "wit_im.to_csv(ofn)\n",
    "    \n",
    "#*********************** START ADDED BY SHANE ***********************\n",
    "# create a pivot table to gather the time since last inundation using the event metrics\n",
    "# and export as csv\n",
    "# timesincelast = number of days from last event end-date to final date in WIT record\n",
    "lastevent = pd.pivot_table(wit_im, index=pkey, values=['end_time'], aggfunc=np.max)\n",
    "#lastevent['end_time']=lastevent['end_time'].astype('datetime64')\n",
    "time_since_last = pd.merge(maxdate, lastevent, on=[pkey], how='inner')\n",
    "time_since_last.insert(2, 'timesincelast', \n",
    "          (time_since_last['TIME'] - time_since_last['end_time']).astype('timedelta64[D]'))\n",
    "ofn = 'ANAE_time_since_last_inundation.csv'\n",
    "time_since_last.to_csv(ofn)\n",
    "#*********************** END ADDED BY SHANE ***********************\n",
    "    \n",
    "    \n",
    "print(\"all done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** June 2021\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`landsat 8`, :index:`time series`, :index:`WIT`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
