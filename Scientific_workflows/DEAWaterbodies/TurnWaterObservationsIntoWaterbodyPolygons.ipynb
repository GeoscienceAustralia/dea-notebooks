{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn water observations into waterbody polygons <img align=\"right\" src=\"../../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatibility:** Notebook currently compatible with both the NCI and DEA Sandbox environments if you set your filepaths to the required datasets. \n",
    "* **Products used:** \n",
    "[wofs_summary](https://explorer.sandbox.dea.ga.gov.au/wofs_summary)\n",
    "* **Special requirements:** \n",
    "This notebook requires the [python_geohash](https://pypi.org/project/python-geohash/) library. If you are using the default dea environment, this package may not be available. You can install it locally by using `pip install --user python-geohash`.\n",
    "* **Prerequisites:** \n",
    "    * NetCDF files with WOfS outputs that will be used to define the persistent water body polygons\n",
    "        * Variable name: `TileFolder`\n",
    "        * This folder can be either a custom extraction of datacube-stats, or you can choose to use the WOfS summary tiles for all of Australia (see [here for further information](#Tiles)).\n",
    "    * A coastline polygon to filter out polygons generated from ocean pixels.\n",
    "        * Variable name: `LandSeaMaskFile`\n",
    "        * Here we have generated a hightide coastline using the [Intertidal Extents Model (ITEM) v2](http://pid.geoscience.gov.au/dataset/ga/113842) dataset. See [here for more details](#coastline)\n",
    "* **Optional prerequisites:**\n",
    "    * River line dataset for filtering out polygons comprised of river segments.\n",
    "        * Variable name: `MajorRiversDataset`\n",
    "        * The option to filter out major rivers is provided, and so this dataset is optional if `FilterOutRivers = False`.\n",
    "        * Here we use the [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/), filtered to only keep features tagged as `major rivers`. \n",
    "        * There are some identified issues with this data layer that make the filtering using this data inconsistent (see [the discussion here](#rivers))\n",
    "        * We therefore turn this off during the production of the water bodies shapefile. \n",
    "    * Urban high rise polygon dataset\n",
    "        * Variable name: `UrbanMaskFile`, but this is optional and can be skipped by setting `UrbanMask = False`.\n",
    "        * WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This results in 'waterbodies' around these misclassified shadows in capital cities. If you are not using WOfS for your analysis, you may choose to set `UrbanMask = False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "On average, the Australian Government invests around half a billion dollars a year in monitoring, protecting and enhancing Australia's land, coasts and oceans. DEA provides near real-time satellite information which can be used by government to better target these investments.\n",
    "\n",
    "Water is among one the most precious natural resources and is essential for the survival of life on Earth. Within Australia, the scarcity of water is both an economic and social issue. Water is required not only for consumption but for industries and environmental ecosystems to function and flourish. \n",
    "\n",
    "With the demand for water increasing, there is a need to better understand our water availability to ensure we are managing our water resources effectively and efficiently.  \n",
    "\n",
    "Digital Earth Australia (DEA)'s [Water Observations from Space (WOfS) dataset](https://www.sciencedirect.com/science/article/pii/S0034425715301929), provides a water classified image of Australia approximately every 16 days. These individual water observations have been combined into a [WOfS summary product](https://explorer.sandbox.dea.ga.gov.au/wofs_summary), which calculates the frequency of wet observations (compared against all clear observations of that pixel), over the full 30 year satellite archive. \n",
    "\n",
    "The WOfS summary product provides valuable insights into the persistence of water across the Australian landscape on a pixel by pixel basis. While knowing the wet history of a single pixel within a waterbody is useful, it is more useful to be able to map the whole waterbody as a single object. \n",
    "\n",
    "This notebook demonstrates a workflow for mapping waterbodies across Australia as polygon objects. This workflow has been used to produce **DEA Waterbodies**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This code follows the following workflow:\n",
    "\n",
    "* Load the required python packages\n",
    "* Load the required functions\n",
    "* Set your chosen analysis parameters\n",
    "    * minimum number of valid observations\n",
    "    * wetness threshold/s\n",
    "    * min/max waterbody size\n",
    "    * optional flag to filter out waterbodies that intersect with major rivers\n",
    "        * if you set this flag you will need to provide a dataset to do the filtering\n",
    "    * set the analysis region\n",
    "    * set the input files for the analysis\n",
    "    * read in a land/sea mask\n",
    "    * read in an urban mask\n",
    "* Generate a list of netCDF files within a specified folder location\n",
    "* Opens each netCDF file and:\n",
    "    * Keep only pixels observed at least x times\n",
    "    * Keep only pixels identified as wet at least x% of the time\n",
    "        * Here the code can take in two wetness thresholds, to produce two initial temporary polygon files.\n",
    "    * Convert the raster data into polygons\n",
    "* Append the polygon set to a temporary shapefile\n",
    "* Remove artificial polygon borders created at tile boundaries by merging polygons that intersect across Albers Tile boundaries\n",
    "* Filter the combined polygon dataset (note that this step happens after the merging of Albers tile boundary polygons to ensure that artifacts are not created by part of a polygon being filtered out, while the remainder of the polygon that sits on a separate tile is treated differently).\n",
    "    * Filter the polygons based on size\n",
    "    * Remove polygons that intersect with Australia's coastline\n",
    "    * Remove erroneous 'water' polygons within high-rise CBD areas\n",
    "    * Combine the two generated wetness thresholds (optional)\n",
    "    * Optional filtering for proximity to major rivers (as identified by the Geofabric dataset)\n",
    "* Save out the final polygon set to a shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, work through this notebook starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geohash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73cbae34b7c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeohash\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geohash'"
     ]
    }
   ],
   "source": [
    "import rasterio.features\n",
    "from shapely.geometry import Polygon, shape, mapping\n",
    "from shapely.ops import unary_union\n",
    "import geopandas as gp\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import math\n",
    "import geohash as gh\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the functions for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def Generate_list_of_albers_tiles(TileFolder=\"TileFolder\", CustomData=True):\n",
    "    \"\"\"\n",
    "    Generate a list of Albers tiles to loop through for the water body analysis. This \n",
    "    function assumes that the list of tiles will be generated from a custom \n",
    "    datacube-stats run, and the file names will have the format\n",
    "    \n",
    "    */wofs_summary_8_-37_{date}.nc\n",
    "    \n",
    "    The tile number is expected in the 2nd and 3rd last positions when the string has been\n",
    "    broken using `_`. If this is not the case, then this code will not work, and will throw an error. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    TileFolder : str\n",
    "        This is the path to the folder of netCDF files for analysis. If this is not provided, or an\n",
    "        incorrect path name is provided, the code will exit with an error.\n",
    "    CustomData : boolean\n",
    "        This is passed in from elsewhere in the notebook. If this is not entered, the default parameter is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    CustomRegionAlbersTiles: list\n",
    "        List of Albers tiles across the analysis region. \n",
    "        E.g. ['8_-32', '9_-32', '10_-32', '8_-33', '9_-33']\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(TileFolder) == False:\n",
    "        print(\n",
    "            \"** ERROR ** \\n\"\n",
    "            \"You need to specify a folder of files for running a custom region\")\n",
    "        return\n",
    "\n",
    "    # Grab a list of all of the netCDF files in the tile folder\n",
    "    TileFiles = glob.glob(f\"{TileFolder}*.nc\")\n",
    "\n",
    "    CustomRegionAlbersTiles = set()\n",
    "    for filePath in TileFiles:\n",
    "        AlbersTiles = re.split(\"[_\\.]\", filePath)\n",
    "        if CustomData:\n",
    "            # Test that the albers tile numbers are actually where we expect them to be in the file name\n",
    "            try:\n",
    "                int(AlbersTiles[-4])\n",
    "                int(AlbersTiles[-3])\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    \"** ERROR ** \\n\"\n",
    "                    'The netCDF files are expected to have the file format \"*/wofs_summary_8_-37_{date}.nc\",\\n'\n",
    "                    \"with the Albers tile numbers in the 2nd and 3rd last positions when separated on `_`. \\n\"\n",
    "                    \"Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.\"\n",
    "                )\n",
    "                return\n",
    "\n",
    "            # Now that we're happy that the file is reading the correct Albers tiles\n",
    "            ThisTile = f\"{AlbersTiles[-3]}_{AlbersTiles[-2]}\"\n",
    "        else:\n",
    "            # Test that the albers tile numbers are actually where we expect them to be in the file name\n",
    "            try:\n",
    "                int(AlbersTiles[-3])\n",
    "                int(AlbersTiles[-2])\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    \"** ERROR ** \\n\"\n",
    "                    'The netCDF files are expected to have the file format \"*/wofs_filtered_summary_8_-37.nc\",\\n'\n",
    "                    \"with the Albers tile numbers in the 2nd and 3rd last positions when separated on `_` and `.`. \\n\"\n",
    "                    \"Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.\"\n",
    "                )\n",
    "                return\n",
    "\n",
    "            # Now that we're happy that the file is reading the correct Albers tiles\n",
    "            ThisTile = f\"{AlbersTiles[-3]}_{AlbersTiles[-2]}\"\n",
    "        CustomRegionAlbersTiles.add(ThisTile)\n",
    "    CustomRegionAlbersTiles = list(CustomRegionAlbersTiles)\n",
    "    return CustomRegionAlbersTiles\n",
    "\n",
    "\n",
    "def Generate_list_of_tile_datasets(ListofAlbersTiles,\n",
    "                                   Year,\n",
    "                                   TileFolder=\"TileFolder\",\n",
    "                                   CustomData=True):\n",
    "    \"\"\"\n",
    "    Generate a list of Albers tiles datasets to loop through for the water body analysis. Here, the \n",
    "    ListofAlbersTiles is used to generate a list of NetCDF files where the Albers coordinates have \n",
    "    been substituted into the naming file format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    CustomRegionAlbersTiles: list\n",
    "        List of albers tiles to loop through\n",
    "        E.g. ['8_-32', '9_-32', '10_-32', '8_-33', '9_-33']\n",
    "    Year: int\n",
    "        Year for the analysis. This will correspond to the netCDF files for analysis.\n",
    "    TileFolder : str\n",
    "        This is the path to the folder of netCDF files for analysis. If this is not provided, or an\n",
    "        incorrect path name is provided, the code will exit with an error.\n",
    "    CustomData : boolean\n",
    "        This is passed from elsewhere in the notebook. If this parameter is not entered, the default value\n",
    "        is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Alltilespaths: list\n",
    "        List of file paths to files to be analysed.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(TileFolder) == False:\n",
    "        print(\n",
    "            \"** ERROR ** \\n\"\n",
    "            \"You need to specify a folder of files for running a custom region\")\n",
    "        raise\n",
    "\n",
    "    Alltilespaths = []\n",
    "\n",
    "    if CustomData:\n",
    "        for tile in ListofAlbersTiles:\n",
    "            Tiles = glob.glob(f\"{TileFolder}*_{tile}_{Year}0101.nc\")\n",
    "            Alltilespaths.append(\n",
    "                Tiles[0])  # Assumes only one file will be returned\n",
    "    else:\n",
    "        for tile in ListofAlbersTiles:\n",
    "            # Use glob to check that the file actually exists in the format we expect\n",
    "            Tiles = glob.glob(f\"{TileFolder}wofs_filtered_summary_{tile}.nc\")\n",
    "            # Check that assumption by seeing if the returned list is empty\n",
    "            if not Tiles:\n",
    "                Tiles = glob.glob(f\"{TileFolder}WOFS_3577_{tile}_summary.nc\")\n",
    "            # Check that we actually have something now\n",
    "            if not Tiles:\n",
    "                print(\n",
    "                    \"** ERROR ** \\n\"\n",
    "                    \"An assumption in the file naming conventions has gone wrong somewhere.\\n\"\n",
    "                    \"We assume two file naming formats here: {TileFolder}wofs_filtered_summary_{tile}.nc, \\n\"\n",
    "                    \"and {TileFolder}WOFS_3577_{tile}_summary.nc. The files you have directed to don't meet \\n\"\n",
    "                    \"either assumption. Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.\"\n",
    "                )\n",
    "            Alltilespaths.append(\n",
    "                Tiles[0])  # Assumes only one file will be returned\n",
    "\n",
    "    return Alltilespaths\n",
    "\n",
    "\n",
    "def Filter_shapefile_by_intersection(gpdData,\n",
    "                                     gpdFilter,\n",
    "                                     filtertype=\"intersects\",\n",
    "                                     invertMask=True,\n",
    "                                     returnInverse=False):\n",
    "    \"\"\"\n",
    "    Filter out polygons that intersect with another polygon shapefile. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gpdData: geopandas dataframe\n",
    "        Polygon data that you wish to filter\n",
    "    gpdFilter: geopandas dataframe\n",
    "        Dataset you are using as a filter\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    filtertype: default = 'intersects'\n",
    "        Options = ['intersects', 'contains', 'within']\n",
    "    invertMask: boolean\n",
    "        Default = 'True'. This determines whether you want areas that DO ( = 'False') or DON'T ( = 'True')\n",
    "        intersect with the filter shapefile.\n",
    "    returnInnverse: boolean\n",
    "        Default = 'False'. If true, then return both parts of the intersection - those that intersect AND \n",
    "        those that don't as two dataframes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gpdDataFiltered: geopandas dataframe\n",
    "        Filtered polygon set, with polygons that intersect with gpdFilter removed.\n",
    "    IntersectIndex: list of indices of gpdData that intersect with gpdFilter\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    if 'returnInverse = True'\n",
    "    gpdDataFiltered, gpdDataInverse: two geopandas dataframes\n",
    "        Filtered polygon set, with polygons that DON'T intersect with gpdFilter removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the coordinate reference systems of both dataframes are the same\n",
    "\n",
    "    # assert gpdData.crs == gpdFilter.crs, 'Make sure the the coordinate reference systems of the two provided dataframes are the same'\n",
    "\n",
    "    Intersections = gp.sjoin(gpdFilter, gpdData, how=\"inner\", op=filtertype)\n",
    "\n",
    "    # Find the index of all the polygons that intersect with the filter\n",
    "    IntersectIndex = sorted(set(Intersections[\"index_right\"]))\n",
    "\n",
    "    # Grab only the polygons NOT in the IntersectIndex\n",
    "    # i.e. that don't intersect with a river\n",
    "    if invertMask:\n",
    "        gpdDataFiltered = gpdData.loc[~gpdData.index.isin(IntersectIndex)]\n",
    "    else:\n",
    "        gpdDataFiltered = gpdData.loc[gpdData.index.isin(IntersectIndex)]\n",
    "\n",
    "    if returnInverse:\n",
    "        # We need to use the indices from IntersectIndex to find the inverse dataset, so we\n",
    "        # will just swap the '~'.\n",
    "\n",
    "        if invertMask:\n",
    "            gpdDataInverse = gpdData.loc[gpdData.index.isin(IntersectIndex)]\n",
    "        else:\n",
    "            gpdDataInverse = gpdData.loc[~gpdData.index.isin(IntersectIndex)]\n",
    "\n",
    "        return gpdDataFiltered, IntersectIndex, gpdDataInverse\n",
    "    else:\n",
    "\n",
    "        return gpdDataFiltered, IntersectIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "The following section walks you through the analysis parameters you will need to set for this workflow. Each section describes the parameter, how it is used, and what value was used for the DEA Waterbodies product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wetnessThreshold'></a>\n",
    "### How frequently wet does a pixel need to be to be included?\n",
    "The value/s set here will be the minimum frequency (as a decimal between 0 and 1) that you want water to be detected across all analysis years before it is included. \n",
    "\n",
    "E.g. If this was set to 0.10, any pixels that are wet *at least* 10% of the time across all valid observations will be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "Following the exploration of an appropriate wetness threshold for DEA Waterbodies ([see here](DEAWaterbodiesSupplement/DEAWaterbodiesThresholdSensitivityAnalysis.ipynb)), we choose to set two thresholds here. The code is set up to loop through both wetness thresholds, and to write out two temporary shapefiles. These two shapefiles with two separate thresholds are then used together to combine polygons from both thresholds later on in the workflow.\n",
    "\n",
    "Polygons identified by the secondary threshold that intersect with the polygons generated by the primary threshold will be extracted, and included in the final polygon dataset. This means that the **location** of polygons is set by the primary threshold, but the **shape** of these polygons is set by the secondary threshold.\n",
    "\n",
    "Threshold values need to be provided as a list of either one or two floating point numbers. If one number is provided, then this will be used to generate the initial polygon dataset. If two thresholds are entered, the **first number becomes the secondary threshold, and the second number becomes the primary threshold**. If more than two numbers are entered, the code will generate an error below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtLeastThisWet = [0.05, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='size'></a>\n",
    "### How big/small should the polygons be?\n",
    "This filtering step can remove very small and/or very large waterbody polygons. The size listed here is in m2. A single pixel in Landsat data is 25 m X 25 m = 625 m2. \n",
    "\n",
    "**MinSize**\n",
    "\n",
    "E.g. A minimum size of 6250 means that polygons need to be at least 10 pixels to be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "**MaxSize**\n",
    "\n",
    "E.g. A maximum size of 1 000 000 means that you only want to consider polygons less than 1 km2. If you don't want to use this filter, set this number to `math.inf`. \n",
    "\n",
    "*NOTE: if you are doing this analysis for all of Australia, very large polygons will be generated offshore, in the steps prior to filtering by the Australian coastline. For this reason, we have used a `MaxSize` = Area of Kati Thanda-Lake Eyre. This will remove the huge ocean polygons, but keep large inland waterbodies that we want to map.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinSize = 3125  # 5 pixels\n",
    "MaxSize = 5000000000  # approx area of Lake Eyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='valid'></a>\n",
    "### Filter results based on number of valid observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of valid WOfS observations for each pixel varies depending on the frequency of clouds and cloud shadow, the proximity to high slope and terrain shadow, and the seasonal change in solar angle. \n",
    "\n",
    "The `count_clear` parameter within the [`wofs_summary`](https://explorer.sandbox.dea.ga.gov.au/wofs_summary) data provides a count of the number of valid observations each pixel recorded over the analysis period. We can use this parameter to mask out pixels that were infrequently observed. \n",
    "If this mask is not applied, pixels that were observed only once could be included if that observation was wet (i.e. a single wet observation means the calculation of the frequency statistic would be (1 wet observation) / (1 total observation) = 100% frequency of wet observations).\n",
    "\n",
    "Here we set the minimum number of observations to be 128 (roughly 4 per year over our 32 year analysis). Note that this parameter does not specify the timing of these observations, but rather just the **total number of valid observations** (observed at any time of the year, in any year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinimumValidObs = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rivers'></a>\n",
    "### Do you want to filter out polygons that intersect with major rivers?\n",
    "\n",
    "The [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/) can be used to filter out polygons that intersect with major rivers. This is done to remove river segments from the polygon dataset. The `SH_Network AHGFNetworkStream any` layer within the `SH_Network_GDB_V2_1_1.zip` geodatabase can be used. You may chose to filter the rivers dataset to only keep rivers tagged as `major`, as the full rivers dataset contains a lot of higher order streams and can remove smaller waterbodies situated on upland streams.\n",
    "\n",
    "If you have an alternative dataset you wish to use inplace of the Bureau of Meteorology Geofabric, you can set the filepath to this file in the `MajorRiversDataset` variable. Any alternative dataset needs to be a vector dataset, and [able to be read in by the fiona python library](https://fiona.readthedocs.io/en/latest/fiona.html#fiona.open).\n",
    "\n",
    "Note that we reproject this dataset to `epsg 3577`, [Australian Albers coordinate reference system](https://spatialreference.org/ref/epsg/3577/) to match the coordinate reference system of the WOfS data we use. If this is not correct for your analysis, you can change this in the cell below. A list of epsg code [can be found here](https://spatialreference.org/ref/epsg/).\n",
    "\n",
    "If you don't want to filter out polygons that intersect with rivers, set this parameter to `False`.\n",
    "\n",
    "**Note that for the Water Body Polygon dataset, we set this filter to False (`FilterOutRivers = False`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note when using the Geofabric to filter out rivers\n",
    "\n",
    "The option to filter out rivers was switched off for the production of DEA Waterbodies. \n",
    "\n",
    "Note that the Geofabric continues the streamline through on-river dams, which means these polygons are filtered out. This may not be the desired result. \n",
    "\n",
    "![Stream and Dam intersection](DocumentationFigures/OnRiverDam.JPG \"An in-river dam that would be removed by the river filter, but may not be the desired result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterOutRivers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    "# Where is this file located?\n",
    "MajorRiversDataset = '/g/data/r78/cek156/ShapeFiles/Geofabric_v2_1_1/SH_Network_GDB_V2_1_1_Major_Filtered.shp'\n",
    "\n",
    "# Read in the major rivers dataset (if you are using it)\n",
    "if FilterOutRivers:\n",
    "    MajorRivers = gp.GeoDataFrame.from_file(MajorRiversDataset)\n",
    "    MajorRivers = MajorRivers.to_crs({'init': 'epsg:3577'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Tiles'></a>\n",
    "### Set up the input datasets for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code allows you to choose whether to use the WOfS summary data, or your own custom analysis. \n",
    "There are a number of options available to you here:\n",
    "* All of Australia WOfS analysis\n",
    "    * Set `AllofAustraliaAllTime = True`\n",
    "    * Set `CustomData = False`\n",
    "    * Set `AutoGenerateTileList = False`\n",
    "* Some of Australia WOfS analysis\n",
    "    * Set `AllofAustraliaAllTime = False`\n",
    "    * Set `CustomData = False`\n",
    "    * Set `AutoGenerateTileList = False`\n",
    "        * You will then need to input a list of Albers Equal Area tiles over which you would like to perform your analysis in the `ListofAlbersTiles` variable.\n",
    "* Custom analysis for any spatial extent\n",
    "    * Set `AllofAustraliaAllTime = False`\n",
    "    * Set `CustomData = True`\n",
    "        * Provide a path to where the files are sitting. *Note that this code assumes the files are netCDF.*\n",
    "    * Set `AutoGenerateTileList = True/False`\n",
    "        * If you want to analyse all of the tiles in the custom folder, set this to `True`.\n",
    "        * If you want to analyse a subset of the tiles in the custom folder, set this to `False`, and provide a list of tiles to the `ListofAlbersTiles` variable.\n",
    "\n",
    "**All of Australia analysis**\n",
    "\n",
    "If you would like to perform the analysis for all of Australia, using the published WOfS all time summaries, set `AllofAustraliaAllTime = True`. \n",
    "\n",
    "The WOfS all time summaries NetCDF files used are located in `/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/`. These files contain the following three variables: `count_wet`, `count_clear` and `frequency`.\n",
    "\n",
    "**Custom Data option**\n",
    "\n",
    "This code is set up to allow you to input your own set of custom data statistics. You can generate your own custom statistics using the [datacube-stats](https://github.com/opendatacube/datacube-stats) code repository. \n",
    "For example, you may wish to calculate WOfS summary statistics over a specified period, rather than over the full 1987 to 2018 period provided in the WOfS summary product. Datacube-stats allows you to specify the parameters for generating statistical summaries. You will need to ensure the output format is set to netCDF to make it compatible with the code here.\n",
    "\n",
    "If `CustomData = True`, you will need to specify the location of the data you would like to use for this analysis, setting `TileFolder` below, under the `if CustomData` code section below.  \n",
    "\n",
    "If `CustomData = False`, the code will automatically look at the published WOfS all time summaries.\n",
    "\n",
    "**Autogeneration of tile list**\n",
    "\n",
    "`AutoGenerateTileList` will only be used if `AllOfAustraliaAllTime = False`. We only want to generate a list of tiles to iterate through if it will be a subset of all of the available data.\n",
    "\n",
    "If you would like to automatically generate a list of tiles using the outputs of an analysis (e.g. if you have run a custom `datacube-stats` analysis over just NSW), set `AutoGenerateTileList = True` and update the location of the output file directory. This will generate a tile list consisting of every available tile within that folder. \n",
    "Note that this option currently assumes a filename format. If you experience an error when running this step, you may need to modify the `Generate_list_of_albers_tiles` function loaded above.\n",
    "\n",
    "If you would like to manually feed in a list of albers tiles (i.e. run a subset of the tiles available within a chosen folder), set `AutoGenerateTileList = False`, and feed in a list of tiles in the format:\n",
    "\n",
    "```\n",
    "ListofAlbersTiles = ['7_-34', '10_-40', '16_-34']\n",
    "```\n",
    "\n",
    "\n",
    "For testing and debugging, set `CustomData = True` and `AutoGenerateTileList = False`, then enter a list of tiles to run using the `ListofAlbersTiles` described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllOfAustraliaAllTime = False\n",
    "\n",
    "CustomData = False\n",
    "AutoGenerateTileList = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CustomData:\n",
    "    # Path to the files you would like to use for the analysis\n",
    "    TileFolder = '/g/data/r78/cek156/datacube_stats/WOFSDamsAllTimeNSWMDB/'\n",
    "else:\n",
    "    # Default path to the WOfS summary product\n",
    "    TileFolder = '/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to generate the tile list if we are not doing all of Australia.\n",
    "if not AllOfAustraliaAllTime:\n",
    "    if AutoGenerateTileList:\n",
    "        ListofAlbersTiles = Generate_list_of_albers_tiles(\n",
    "            TileFolder, CustomData)\n",
    "    else:\n",
    "        # Provide you own list of tiles to be run\n",
    "        ListofAlbersTiles = [\n",
    "            '8_-32', '19_-40', '10_-30', '14_-31', '13_-31', '18_-38', '19_-33',\n",
    "            '9_-31', '20_-34', '6_-39', '17_-31', '12_-40', '13_-39', '10_-38',\n",
    "            '6_-41', '16_-32', '9_-40', '10_-31', '16_-37', '10_-39', '16_-35',\n",
    "            '7_-38', '10_-37', '17_-39', '9_-34', '19_-35', '15_-33', '15_-30',\n",
    "            '11_-32', '20_-39', '17_-30', '13_-40', '7_-41', '17_-29', '12_-33',\n",
    "            '15_-28', '16_-40', '6_-35', '17_-40', '13_-38', '17_-42', '14_-39',\n",
    "            '13_-29', '17_-37', '16_-38', '9_-32', '16_-34', '9_-41', '11_-31',\n",
    "            '7_-36', '16_-39', '18_-30', '9_-37', '20_-37', '13_-32', '7_-40',\n",
    "            '18_-41', '20_-32', '8_-41', '14_-28', '18_-39', '14_-43', '12_-39',\n",
    "            '20_-36', '8_-34', '17_-41', '12_-41', '18_-31', '11_-38', '18_-34',\n",
    "            '14_-35', '12_-42', '19_-39', '12_-34', '10_-42', '11_-35',\n",
    "            '17_-35', '15_-41', '18_-33', '6_-37', '13_-41', '10_-40', '14_-33',\n",
    "            '13_-37', '8_-36', '6_-36', '16_-43', '18_-36', '14_-40', '15_-43',\n",
    "            '12_-30', '5_-39', '8_-39', '18_-35', '15_-39', '15_-29', '7_-34',\n",
    "            '11_-34', '14_-41', '15_-42', '16_-29', '16_-28', '14_-37', '8_-33',\n",
    "            '6_-38', '19_-38', '13_-33', '16_-36', '15_-37', '12_-38', '7_-35',\n",
    "            '18_-40', '12_-31', '16_-41', '14_-38', '19_-37', '10_-34',\n",
    "            '14_-32', '12_-32', '14_-42', '15_-35', '16_-31', '19_-36', '7_-37',\n",
    "            '11_-41', '14_-36', '13_-35', '16_-42', '13_-36', '6_-40', '17_-36',\n",
    "            '10_-41', '18_-37', '14_-29', '14_-30', '20_-38', '17_-38',\n",
    "            '12_-36', '10_-35', '9_-42', '21_-33', '12_-37', '17_-32', '15_-31',\n",
    "            '10_-36', '15_-36', '19_-34', '17_-34', '12_-35', '20_-40',\n",
    "            '20_-33', '19_-31', '20_-35', '16_-30', '18_-32', '12_-29',\n",
    "            '11_-30', '15_-32', '16_-33', '8_-37', '10_-33', '13_-34', '11_-33',\n",
    "            '13_-30', '11_-36', '8_-40', '11_-40', '10_-32', '9_-38', '11_-42',\n",
    "            '11_-39', '15_-34', '21_-34', '13_-42', '9_-35', '8_-42', '17_-33',\n",
    "            '8_-38', '13_-28', '15_-38', '9_-36', '8_-35', '11_-37', '14_-34',\n",
    "            '15_-40', '9_-33', '19_-32', '9_-39', '7_-39'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coastline'></a>\n",
    "### Read in a land/sea mask\n",
    "\n",
    "A high tide coastline for Australia is used to mask out ocean polygons. You can choose which land/sea mask you would like to use, depending on how much coastal water you would like in the final product. \n",
    "\n",
    "We use a coastline generated using the [Intertidal Extents Model (ITEM) v2](http://pid.geoscience.gov.au/dataset/ga/113842) dataset. This particular coastline creates a mask by identifying any water pixels that are continuously connected to the ocean, or an estuary. Any polygons that intersect with this mask are filtered out. I.e. if a polygon identified within our workflow overlaps with this coastal mask by even a single pixel, it will be discarded. We chose this very severe ocean mask as the aim of DEA Waterbodies is not to map coastal waterbodies, but just inland ones. For a detailed description of how this coastline was created, see [this notebook](DEAWaterbodiesSupplement/CreateAustralianCoastlineUsingITEM.ipynb).\n",
    "\n",
    "Note that the mask we use here sets `ocean = 1, land = NaN`. If your mask has `land = 1` instead, you can either invert it, or change the code in the [`Filter merged polygons by:`](#Filtering) code section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the coastal mask you would like to use.\n",
    "LandSeaMaskFile = '/g/data/r78/cek156/ShapeFiles/ITEMv2Coastline/ITEM_Ocean_Polygon.shp'\n",
    "\n",
    "Coastline = gp.read_file(LandSeaMaskFile)\n",
    "Coastline = Coastline.to_crs({'init': 'epsg:3577'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Urban'></a>\n",
    "\n",
    "### Read in a mask for high-rise CBDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This results in 'waterbodies' around these misclassified shadows in capital cities. \n",
    "\n",
    "To address this problem, we use the [Australian Bureau of Statistics Statistical Area 3 shapefile (2016)](http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202016?OpenDocument#Data) to define a spatial footprint for Australia's CBD areas. \n",
    "\n",
    "We use the following polygons as our CBD filter:\n",
    "\n",
    "|SA3_CODE1|SA3_NAME16|\n",
    "|---------|----------|\n",
    "|11703    |Sydney Inner City|\n",
    "|20604    |Melbourne City|\n",
    "|30501    |Brisbane Inner|\n",
    "|30901    |Broadbeach - Burleigh|\n",
    "|30910    |Surfers Paradise|\n",
    "|40101    |Adelaide City|\n",
    "|50302    |Perth City|\n",
    "\n",
    "If you are not using WOfS for your analysis, you may choose to set `UrbanMask = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrbanMask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UrbanMask:\n",
    "    UrbanMaskFile = '/g/data/r78/cek156/ShapeFiles/ABS_1270055001_sa3_2016_aust_shape/HighRiseCBD_ABS_sa3.shp'\n",
    "\n",
    "    CBDs = gp.read_file(UrbanMaskFile)\n",
    "    CBDs = CBDs.to_crs({'init': 'epsg:3577'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the first temporary polygon dataset\n",
    "\n",
    "This code section:\n",
    "\n",
    "1. Checks that the `AtLeastThisWet` threshold has been correctly entered above\n",
    "2. Sets up a `for` loop that allows the user to input multiple temporal datasets (see below)\n",
    "3. Generates a list of netCDF files to loop through\n",
    "4. Sets up a `for` loop for that list of files. Here we have separate data for each Landsat tile, so this loop loops through the list of tile files\n",
    "5. Opens the netCDF `frequency` data and removes the `time` dimension (which in this case is only of size 1)\n",
    "6. Opens the netCDF `count_clear` data and removes the `time` dimension (which in this case is only of size 1)\n",
    "7. Removes any pixels not observed at least [`MinimumValidObs` times](#valid)\n",
    "8. Sets up a `for` loop for the entered [`AtLeastThisWet` thresholds](#wetnessThreshold)\n",
    "9. Masks out any data that does not meet the wetness threshold\n",
    "10. Converts the data to a Boolean array, with included pixels == 1\n",
    "11. Converts the raster array to a polygon dataset\n",
    "12. Cleans up the polygon dataset\n",
    "13. Resets the `geometry` to a shapely geometry\n",
    "14. Merges any overlapping polygons\n",
    "15. Convert the output of the merging back into a geopandas dataframe\n",
    "16. Calculates the area of each polygon\n",
    "17. Saves the results to a shapefile\n",
    "\n",
    "Within this section you need to set up:\n",
    "- **WaterBodiesShp:** The name and filepath of the intermediate output polygon set\n",
    "- **WOFSshpMerged:** The filepath for the location of temp files during the code run\n",
    "- **WOFSshpFiltered:** The name and filepath of the outputs following the [filtering steps](#Filtering)\n",
    "- **FinalName:** The name and file path of the final, completed waterbodies shapefile\n",
    "- **years to analyse:** `for year in range(x,y)` - note that the last year is NOT included in the analysis. This for loop is set up to allow you to loop through multiple datasets to create multiple polygon outputs. If you only have one input dataset, set this to `range(<year of the analysis>, <year of the analysis + 1>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up some file names for the inputs and outputs\n",
    "# The name and filepath of the intermediate output polygon set\n",
    "WaterBodiesShp = f'/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/Temp'\n",
    "\n",
    "# The name and filepath of the temp, filtered output polygon set\n",
    "WOFSshpMerged = f'/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/'\n",
    "WOFSshpFiltered = '/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodiesFiltered.shp'\n",
    "\n",
    "# Final shapefile name\n",
    "FinalName = '/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodies.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not set up the hybrid threshold option. If you meant to use this option, please \n",
      "set this option by including two wetness thresholds in the `AtLeastThisWet` variable above. \n",
      "The wetness threshold we will use is [0.95].\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_8_-32_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_19_-40_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_14_-31_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_10_-37_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_9_-34_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_20_-39_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_6_-35_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_9_-32_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_7_-36_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_18_-30_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_8_-41_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_14_-28_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_8_-34_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_6_-37_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_14_-33_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n",
      "/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/WOFS_3577_6_-36_summary.nc did not run. \n",
      "This is probably because there are no waterbodies present in this tile.\n"
     ]
    }
   ],
   "source": [
    "# First, test whether the wetness threshold has been correctly set\n",
    "if len(AtLeastThisWet) == 2:\n",
    "    print(\n",
    "        f'We will be running a hybrid wetness threshold. Please ensure that the major threshold is \\n'\n",
    "        f'listed second, with the supplementary threshold entered first.'\n",
    "        f'**You have set {AtLeastThisWet[-1]} as the primary threshold,** \\n'\n",
    "        f'**with {AtLeastThisWet[0]} set as the supplementary threshold.**')\n",
    "elif len(AtLeastThisWet) == 1:\n",
    "    print(\n",
    "        f'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "        f'set this option by including two wetness thresholds in the `AtLeastThisWet` variable above. \\n'\n",
    "        f'The wetness threshold we will use is {AtLeastThisWet}.')\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f'There is something wrong with your entered wetness threshold. Please enter a list \\n'\n",
    "        f'of either one or two numbers. You have entered {AtLeastThisWet}. \\n'\n",
    "        f'See above for more information')\n",
    "\n",
    "# Now perform the analysis to generate the first iteration of polygons\n",
    "for year in range(1980, 1981):\n",
    "\n",
    "    ### Get the list of netcdf file names to loop through\n",
    "    if AllOfAustraliaAllTime:\n",
    "        # Grab everything from the published WOfS all time summaries\n",
    "        Alltiles = glob.glob(f'{TileFolder}*.nc')\n",
    "    else:\n",
    "        Alltiles = Generate_list_of_tile_datasets(ListofAlbersTiles, year,\n",
    "                                                  TileFolder, CustomData)\n",
    "\n",
    "    for WOFSfile in Alltiles:\n",
    "        try:\n",
    "            # Read in the data\n",
    "            # Note that the netCDF files we are using here contain a variable called 'frequency',\n",
    "            # which is what we are using to define our water polygons.\n",
    "            # If you use a different netCDF input source, you may need to change this variable name here\n",
    "            WOFSnetCDFData = xr.open_rasterio(f'NETCDF:{WOFSfile}:frequency')\n",
    "            # Remove the superfluous time dimension\n",
    "            WOFSnetCDFData = WOFSnetCDFData.squeeze()\n",
    "\n",
    "            # Open the clear count variable to generate the minimum observation mask\n",
    "            # If you use a different netCDF input source, you may need to change this variable name here\n",
    "            WOFSvalidcount = xr.open_rasterio(f'NETCDF:{WOFSfile}:count_clear')\n",
    "            WOFSvalidcount = WOFSvalidcount.squeeze()\n",
    "\n",
    "            # Filter our WOfS classified data layer to remove noise\n",
    "            # Remove any pixels not abserved at least MinimumValidObs times\n",
    "            WOFSValidFiltered = WOFSvalidcount >= MinimumValidObs\n",
    "\n",
    "            for Thresholds in AtLeastThisWet:\n",
    "                # Remove any pixels that are wet < AtLeastThisWet% of the time\n",
    "                WOFSfiltered = WOFSnetCDFData > Thresholds\n",
    "\n",
    "                # Now find pixels that meet both the MinimumValidObs and AtLeastThisWet criteria\n",
    "                # Change all zeros to NaN to create a nan/1 mask layer\n",
    "                # Pixels == 1 now represent our water bodies\n",
    "                WOFSfiltered = WOFSfiltered.where((WOFSfiltered != 0) &\n",
    "                                                  (WOFSValidFiltered != 0))\n",
    "\n",
    "                # Convert the raster to polygons\n",
    "                # We use a mask of '1' to only generate polygons around values of '1' (not NaNs)\n",
    "                WOFSpolygons = rasterio.features.shapes(\n",
    "                    WOFSfiltered.data.astype('float32'),\n",
    "                    mask=WOFSfiltered.data.astype('float32') == 1,\n",
    "                    transform=WOFSnetCDFData.transform)\n",
    "                # The rasterio.features.shapes returns a tuple. We only want to keep the geometry portion,\n",
    "                # not the value of each polygon (which here is just 1 for everything)\n",
    "                WOFSbreaktuple = (a for a, b in WOFSpolygons)\n",
    "\n",
    "                # Put our polygons into a geopandas geodataframe\n",
    "                PolygonGP = gp.GeoDataFrame(list(WOFSbreaktuple))\n",
    "\n",
    "                # Grab the geometries and convert into a shapely geometry\n",
    "                # so we can quickly calcuate the area of each polygon\n",
    "                PolygonGP['geometry'] = None\n",
    "                for ix, poly in PolygonGP.iterrows():\n",
    "                    poly['geometry'] = shape(poly)\n",
    "\n",
    "                # Set the geometry of the dataframe to be the shapely geometry we just created\n",
    "                PolygonGP = PolygonGP.set_geometry('geometry')\n",
    "                # We need to add the crs back onto the dataframe\n",
    "                PolygonGP.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "                # Combine any overlapping polygons\n",
    "                MergedPolygonsGeoms = unary_union(PolygonGP['geometry'])\n",
    "\n",
    "                # Turn the combined multipolygon back into a geodataframe\n",
    "                MergedPolygonsGPD = gp.GeoDataFrame(\n",
    "                    [poly for poly in MergedPolygonsGeoms])\n",
    "                # Rename the geometry column\n",
    "                MergedPolygonsGPD.columns = ['geometry']\n",
    "                # We need to add the crs back onto the dataframe\n",
    "                MergedPolygonsGPD.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "                # Calculate the area of each polygon again now that overlapping polygons\n",
    "                # have been merged\n",
    "                MergedPolygonsGPD['area'] = MergedPolygonsGPD['geometry'].area\n",
    "\n",
    "                # Save the polygons to a shapefile\n",
    "                schema = {\n",
    "                    'geometry': 'Polygon',\n",
    "                    'properties': {\n",
    "                        'area': 'float'\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Generate our dynamic filename\n",
    "                FileName = f'{WaterBodiesShp}_{Thresholds}.shp'\n",
    "                # Append the file name to the list so we can call it later on\n",
    "\n",
    "                if os.path.isfile(FileName):\n",
    "                    with fiona.open(FileName,\n",
    "                                    \"a\",\n",
    "                                    crs=from_epsg(3577),\n",
    "                                    driver='ESRI Shapefile',\n",
    "                                    schema=schema) as output:\n",
    "                        for ix, poly in MergedPolygonsGPD.iterrows():\n",
    "                            output.write(({\n",
    "                                'properties': {\n",
    "                                    'area': poly['area']\n",
    "                                },\n",
    "                                'geometry': mapping(shape(poly['geometry']))\n",
    "                            }))\n",
    "                else:\n",
    "                    with fiona.open(FileName,\n",
    "                                    \"w\",\n",
    "                                    crs=from_epsg(3577),\n",
    "                                    driver='ESRI Shapefile',\n",
    "                                    schema=schema) as output:\n",
    "                        for ix, poly in MergedPolygonsGPD.iterrows():\n",
    "                            output.write(({\n",
    "                                'properties': {\n",
    "                                    'area': poly['area']\n",
    "                                },\n",
    "                                'geometry': mapping(shape(poly['geometry']))\n",
    "                            }))\n",
    "\n",
    "        except:\n",
    "            print(\n",
    "                f'{WOFSfile} did not run. \\n'\n",
    "                f'This is probably because there are no waterbodies present in this tile.'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MergeTiles'></a>\n",
    "\n",
    "## Merge polygons that have an edge at a tile boundary\n",
    "\n",
    "Now that we have all of the polygons across our whole region of interest, we need to check for artifacts in the data caused by tile boundaries. \n",
    "\n",
    "We have created a shapefile that consists of the albers tile boundaries, plus a 1 pixel (25 m) buffer. This shapefile will help us to find any polygons that have a boundary at the edge of an albers tile. We can then find where polygons touch across this boundary, and join them up.\n",
    "\n",
    "Within this section you need to set up:\n",
    "- **AlbersBuffer:** The file location of a shapefile that is a 1 pixel buffer around the Albers tile boundaries\n",
    "\n",
    "*NOTE: for the Australia-wide analysis, the number and size of polygons means that this cell cannot be run in this notebook. Instead, we ran this cell on raijin*\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#PBS -P r78\n",
    "#PBS -q hugemem\n",
    "#PBS -l walltime=96:00:00\n",
    "#PBS -l mem=500GB\n",
    "#PBS -l jobfs=200GB\n",
    "#PBS -l ncpus=7\n",
    "#PBS -l wd\n",
    "#PBS -lother=gdata1a\n",
    " \n",
    "module use /g/data/v10/public/modules/modulefiles/\n",
    "module load dea\n",
    "\n",
    "PYTHONPATH=$PYTHONPATH:/g/data/r78/cek156/dea-notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AlbersBuffer = gp.read_file('/g/data/r78/cek156/ShapeFiles/AlbersBuffer25m.shp')\n",
    "\n",
    "for Threshold in AtLeastThisWet:\n",
    "    print(f'Working on {Threshold} shapefile')\n",
    "    # We are using the more severe wetness threshold as the main polygon dataset.\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'AtLeastThisWet'\n",
    "    # variable, with the higher threshold listed second.\n",
    "    WaterPolygons = gp.read_file(f'{WaterBodiesShp}_{Threshold}.shp')\n",
    "\n",
    "    # Find where the albers polygon overlaps with our dam polygons\n",
    "    BoundaryMergedDams, IntersectIndexes, NotBoundaryDams = Filter_shapefile_by_intersection(\n",
    "        WaterPolygons, AlbersBuffer, invertMask=False, returnInverse=True)\n",
    "\n",
    "    # Now combine overlapping polygons in `BoundaryDams`\n",
    "    UnionBoundaryDams = BoundaryMergedDams.unary_union\n",
    "\n",
    "    # `Explode` the multipolygon back out into individual polygons\n",
    "    UnionGDF = gp.GeoDataFrame(crs=WaterPolygons.crs,\n",
    "                               geometry=[UnionBoundaryDams])\n",
    "    MergedDams = UnionGDF.explode()\n",
    "\n",
    "    # Then combine our new merged polygons with the `NotBoundaryDams`\n",
    "    # Combine New merged polygons with the remaining polygons that are not near the tile boundary\n",
    "    AllTogether = gp.GeoDataFrame(\n",
    "        pd.concat([NotBoundaryDams, MergedDams], ignore_index=True,\n",
    "                  sort=True)).set_geometry('geometry')\n",
    "\n",
    "    # Calculate the area of each polygon\n",
    "    AllTogether['area'] = AllTogether.area\n",
    "\n",
    "    # Check for nans\n",
    "    AllTogether.dropna(inplace=True)\n",
    "\n",
    "    schema = {'geometry': 'Polygon', 'properties': {'area': 'float'}}\n",
    "\n",
    "    print(f'Writing out {Threshold} shapefile')\n",
    "\n",
    "    with fiona.open(f'{WOFSshpMerged}Union_{Threshold}.shp',\n",
    "                    \"w\",\n",
    "                    crs=from_epsg(3577),\n",
    "                    driver='ESRI Shapefile',\n",
    "                    schema=schema) as output:\n",
    "        for ix, poly in AllTogether.iterrows():\n",
    "            output.write(({\n",
    "                'properties': {\n",
    "                    'area': poly['area']\n",
    "                },\n",
    "                'geometry': mapping(shape(poly['geometry']))\n",
    "            }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Filtering'></a>\n",
    "\n",
    "## Filter the merged polygons by:\n",
    "- **Area:**\n",
    "Based on the `MinSize` and `MaxSize` parameters set [here](#size).\n",
    "- **Coastline:**\n",
    "Using the `Coastline` dataset loaded [here](#coastline).\n",
    "- **CBD location (optional):**\n",
    "Using the `CBDs` dataset loaded [here](#Urban).\n",
    "- **Wetness thresholds:**\n",
    "Here we apply the hybrid threshold described [here](#wetness)\n",
    "- **Intersection with rivers (optional):**\n",
    "Using the `MajorRivers` dataset loaded [here](#rivers)\n",
    "\n",
    "*NOTE: for the Australia-wide analysis, the number and size of polygons means that this cell cannot be run in this notebook. Instead, we ran this cell on raijin*\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#PBS -P r78\n",
    "#PBS -q hugemem\n",
    "#PBS -l walltime=96:00:00\n",
    "#PBS -l mem=500GB\n",
    "#PBS -l jobfs=200GB\n",
    "#PBS -l ncpus=7\n",
    "#PBS -l wd\n",
    "#PBS -lother=gdata1a\n",
    " \n",
    "module use /g/data/v10/public/modules/modulefiles/\n",
    "module load dea\n",
    "\n",
    "PYTHONPATH=$PYTHONPATH:/g/data/r78/cek156/dea-notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not set up the hybrid threshold option. If you meant to use this option, please \n",
      "set this option by including two wetness thresholds in the `AtLeastThisWet` variable above\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    AllTogether = gp.read_file(f'{WOFSshpMerged}Temp_{AtLeastThisWet[1]}.shp')\n",
    "except IndexError:\n",
    "    AllTogether = gp.read_file(f'{WOFSshpMerged}Temp_{AtLeastThisWet[0]}.shp')\n",
    "AllTogether['area'] = pd.to_numeric(AllTogether.area)\n",
    "\n",
    "# Filter out any polygons smaller than MinSize, and greater than MaxSize\n",
    "WaterBodiesBig = AllTogether.loc[((AllTogether['area'] > MinSize) &\n",
    "                                  (AllTogether['area'] <= MaxSize))]\n",
    "\n",
    "# Filter out any ocean in the pixel\n",
    "WaterBodiesLand, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "    WaterBodiesBig, Coastline, invertMask=True)\n",
    "\n",
    "# WOfS has a known bug where deep shadows from high-rise CBD buildings are misclassified\n",
    "# as water. We will use the ABS sa3 dataset to filter out Brisbane, Gold Coast, Sydney,\n",
    "# Melbourne, Adelaide and Perth CBDs.\n",
    "# If you have chosen to set UrbanMask = False, this step will be skipped.\n",
    "if UrbanMask:\n",
    "    NotCities, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "        WaterBodiesLand, CBDs)\n",
    "else:\n",
    "    print(\n",
    "        'You have chosen not to filter out waterbodies within CBDs. If you meant to use this option, please \\n'\n",
    "        'set `UrbanMask = True` variable above, and set the path to your urban filter shapefile'\n",
    "    )\n",
    "    NotCities = WaterBodiesLand\n",
    "\n",
    "# Check for hybrid wetness thresholds\n",
    "if len(AtLeastThisWet) == 2:\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'AtLeastThisWet'\n",
    "    # variable, with the supplementary threshold listed first.\n",
    "    LowerThreshold = gp.read_file(\n",
    "        f'{WOFSshpMerged}Union_{AtLeastThisWet[0]}.shp')\n",
    "    LowerThreshold['area'] = pd.to_numeric(LowerThreshold.area)\n",
    "    # Filter out those pesky huge polygons\n",
    "    LowerThreshold = LowerThreshold.loc[(LowerThreshold['area'] <= MaxSize)]\n",
    "    # Find where the albers polygon overlaps with our dam polygons\n",
    "    BoundaryMergedDams, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "        LowerThreshold, NotCities)\n",
    "    # Pull out the polygons from the supplementary shapefile that intersect with the primary shapefile\n",
    "    LowerThresholdToUse = LowerThreshold.loc[LowerThreshold.index.isin(\n",
    "        IntersectIndexes)]\n",
    "    # Concat the two polygon sets together\n",
    "    CombinedPolygons = gp.GeoDataFrame(\n",
    "        pd.concat([LowerThresholdToUse, NotCities], ignore_index=True))\n",
    "    # Merge overlapping polygons\n",
    "    CombinedPolygonsUnion = CombinedPolygons.unary_union\n",
    "    # `Explode` the multipolygon back out into individual polygons\n",
    "    UnionGDF = gp.GeoDataFrame(crs=LowerThreshold.crs,\n",
    "                               geometry=[CombinedPolygonsUnion])\n",
    "    HybridDams = UnionGDF.explode()\n",
    "else:\n",
    "    print(\n",
    "        'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "        'set this option by including two wetness thresholds in the `AtLeastThisWet` variable above'\n",
    "    )\n",
    "    HybridDams = NotCities\n",
    "\n",
    "# Here is where we do the river filtering (if FilterOutRivers == True)\n",
    "if FilterOutRivers:\n",
    "    WaterBodiesBigRiverFiltered, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "        HybridDams, MajorRivers)\n",
    "else:\n",
    "    # If river filtering is turned off, then we just keep all the same polygons\n",
    "    WaterBodiesBigRiverFiltered = HybridDams\n",
    "\n",
    "# We need to add the crs back onto the dataframe\n",
    "WaterBodiesBigRiverFiltered.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "# Calculate the area and perimeter of each polygon again now that overlapping polygons\n",
    "# have been merged\n",
    "WaterBodiesBigRiverFiltered['area'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].area\n",
    "WaterBodiesBigRiverFiltered['perimeter'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].length\n",
    "\n",
    "# Calculate the Polsby-Popper value (see below), and write out too\n",
    "WaterBodiesBigRiverFiltered['PPtest'] = (\n",
    "    (WaterBodiesBigRiverFiltered['area'] * 4 * math.pi) /\n",
    "    (WaterBodiesBigRiverFiltered['perimeter']**2))\n",
    "\n",
    "# Save the polygons to a shapefile\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {\n",
    "        'area': 'float',\n",
    "        'perimeter': 'float',\n",
    "        'PPtest': 'float'\n",
    "    }\n",
    "}\n",
    "\n",
    "with fiona.open(WOFSshpFiltered,\n",
    "                \"w\",\n",
    "                crs=from_epsg(3577),\n",
    "                driver='ESRI Shapefile',\n",
    "                schema=schema) as output:\n",
    "    for ix, poly in WaterBodiesBigRiverFiltered.iterrows():\n",
    "        output.write(({\n",
    "            'properties': {\n",
    "                'area': poly['area'],\n",
    "                'perimeter': poly['perimeter'],\n",
    "                'PPtest': poly['PPtest']\n",
    "            },\n",
    "            'geometry': mapping(shape(poly['geometry']))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dividing up very large polygons\n",
    "\n",
    "The size of polygons is determined by the contiguity of waterbody pixels through the landscape. This can result in very large polygons, e.g. where rivers are wide and unobscured by trees, or where waterbodies are connected to rivers or neighbouring waterbodies. The image below shows this for the Menindee Lakes, NSW. The relatively flat terrain in this part of Australia means that the 0.05 wetness threshold results in the connection of a large stretch of river and the individual lakes into a single large polygon that spans 154 km. This polygon is too large to provide useful insights into the changing water surface area of the Menindee Lakes, and needs to be broken into smaller, more useful polygons.\n",
    "\n",
    "![Menindee Lakes original polygon](DocumentationFigures/menindeeLakes.JPG)\n",
    "\n",
    "We do this by applying the [Polsby-Popper test (1991)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2936284). The Polsby-Popper test is an assessment of the 'compactness' of a polygon. This method was originally developed to test the shape of congressional and state legislative districts, to prevent gerrymandering. \n",
    "\n",
    "The Polsby-Popper test examines the ratio between the area of a polygon, and the area of a circle equal to the perimeter of that polygon. The result falls between 0 and 1, with values closer to 1 being assessed as more compact.\n",
    "\n",
    "\\begin{align*}\n",
    "PPtest = \\frac{polygon\\ area * 4\\pi}{polygon\\ perimeter^2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The Menindee Lakes polygon above has a PPtest value $\\approx$ 0.00. \n",
    "\n",
    "We selected all polygons with a `PPtest` value <=0.005. This resulted in a subset of 186 polygons. \n",
    "\n",
    "![Polygons with a Polsby-Popper test score of less than 0.005](DocumentationFigures/PPtestlessthan005.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 186 polygons were buffered with a -50 meter (2 pixel) buffer to separate the polygons where they are connected bu two pixels or less. This allows us to split up these very large polygons by using natural thinning points. The resulting negatively buffered polygons was run through the `multipart to singlepart` tool in QGIS, to give the now separated polygons unique IDs. \n",
    "\n",
    "These polygons were then buffered with a +50 meter buffer to return the polygons to approximately their original size. These final polygons were used to separate the 186 original polygons identified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for dividing up the identified very large polygons varied depending on the polygon in question. Where large waterbodies (like the Menindee Lakes) were connected, the buffered polygons were used to determine the cut points in the original polygons. Where additional breaks were required, the [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/) `waterbodies` dataset was used as an additional source of information for breaking up connected segments.\n",
    "\n",
    "The buffering method didn't work on large segments of river, which became a series of disconnected pieces when negatively and positively buffered. Instead, we used a combination of tributaries and man-made features such as bridges and weirs to segment these river sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks and recalculation of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaterBodiesBigRiverFiltered = gp.read_file(WOFSshpFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the area and perimeter of each polygon again following the manual checking\n",
    "# step performed above\n",
    "WaterBodiesBigRiverFiltered['area'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].area\n",
    "WaterBodiesBigRiverFiltered['perimeter'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the PPtest column, since we don't really want this as an attribute of the final shapefile\n",
    "WaterBodiesBigRiverFiltered.drop(labels='PPtest', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply the size filtering, just to check that all of the split and filtered waterbodies are\n",
    "# still in the size range we want\n",
    "DoubleCheckArea = WaterBodiesBigRiverFiltered.loc[(\n",
    "    (WaterBodiesBigRiverFiltered['area'] > MinSize) &\n",
    "    (WaterBodiesBigRiverFiltered['area'] <= MaxSize))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a unique ID for each polygon\n",
    "\n",
    "A unique identifier is required for every polygon to allow it to be referenced. The naming convention for generating unique IDs here is the [geohash](geohash.org).\n",
    "\n",
    "A Geohash is a geocoding system used to generate short unique identifiers based on latitude/longitude coordinates. It is a short combination of letters and numbers, with the length of the string a function of the precision of the location. The methods for generating a geohash are outlined [here - yes, the official documentation is a wikipedia article](https://en.wikipedia.org/wiki/Geohash).\n",
    "\n",
    "Here we use the python package `python-geohash` to generate a geohash unique identifier for each polygon. We use `precision = 9` geohash characters, which represents an on the ground accuracy of <20 metres. This ensures that the precision is high enough to differentiate between waterbodies located next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert from Albers coordinates to lat/lon, in order to generate the geohash\n",
    "GetUniqueID = DoubleCheckArea.to_crs(epsg=4326)\n",
    "\n",
    "# Generate a geohash for the centroid of each polygon\n",
    "GetUniqueID['UID'] = GetUniqueID.apply(lambda x: gh.encode(\n",
    "    x.geometry.centroid.y, x.geometry.centroid.x, precision=9),\n",
    "                                       axis=1)\n",
    "\n",
    "# Check that our unique ID is in fact unique\n",
    "assert GetUniqueID['UID'].is_unique\n",
    "\n",
    "# Make an arbitrary numerical ID for each polygon. We will first sort the dataframe by geohash\n",
    "# so that polygons close to each other are numbered similarly\n",
    "SortedData = GetUniqueID.sort_values(by=['UID']).reset_index()\n",
    "SortedData['WB_ID'] = SortedData.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The step above creates an 'index' column, which we don't actually want, so drop it.\n",
    "SortedData.drop(labels='index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the final results to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackToAlbers = SortedData.to_crs(epsg=3577)\n",
    "BackToAlbers.to_file(FinalName, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some extra curation\n",
    "\n",
    "Following the development of timeseries for each individual polygon, it was determined that a number of polygons do not produce complete timeseries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting polygons that cross swath boundaries\n",
    "\n",
    "Three large polygons were identified that straddle Landsat swath boundaries. This is problematic, as the whole polygon will never be observed on a single day, which trips the requirement for at least 90% of a polygon to be observed in order for an observation to be valid. \n",
    "\n",
    "There are two options for dealing with this issue:\n",
    "- Splitting the polygons using the swath boundaries, so that each half of the polygon will be observed in a single day. This will retain information as to the exact timing of observations. \n",
    "- Creating time averaged timeseries, which would group observations into monthly blocks and provide a value for each month. This would provide information for the whole polygon, but would lose the specific timing information. \n",
    "\n",
    "We chose to go with the first option to keep the high fidelity timing information for each polygon. Three polygons were split using the swath boundaries as a guide. The split polygons were given a new `WB_ID`, and a new geohash was calculated for each new polygon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaterBodiesSplit = gp.read_file(\n",
    "    '/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodiesSplitEliminate.shp'\n",
    ")\n",
    "\n",
    "# We need to convert from Albers coordinates to lat/lon, in order to generate the geohash\n",
    "GetUniqueID = WaterBodiesSplit.to_crs(epsg=4326)\n",
    "\n",
    "# Only recalculate the geohash for the polygons that have changed:\n",
    "ChangedWB_ID = [145126, 66034, 146567, 295902, 295903, 295904, 295905]\n",
    "\n",
    "for ix, rowz in GetUniqueID.iterrows():\n",
    "    if rowz['WB_ID'] in ChangedWB_ID:\n",
    "        # Generate a geohash for the centroid of each polygon\n",
    "        GetUniqueID.loc[ix, 'WB_ID'] = gh.encode(\n",
    "            GetUniqueID.iloc[ix].geometry.centroid.y,\n",
    "            GetUniqueID.iloc[ix].geometry.centroid.x,\n",
    "            precision=9)\n",
    "        print('Changing geohash')\n",
    "\n",
    "# Check that our unique ID is in fact unique\n",
    "assert GetUniqueID['UID'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final version of the polygons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackToAlbers = GetUniqueID.to_crs(epsg=3577)\n",
    "BackToAlbers.to_file(\n",
    "    '/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodiesFINAL.shp',\n",
    "    driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** December 2019. Peer Code Quality Check Performed, March 2019\n",
    "\n",
    "**Compatible datacube version:** A full list of python packages used to produce DEA Waterbodies is available [here](TurnWaterObservationsIntoWaterbodyPolygons.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Tags: :index:`fiona`, :index:`geopandas`, :index:`Geotiff`, :index:`masking`, :index:`rasterio`, :index:`shapefile`, :index:`WOfS`, :index:`WOFL`, :index:`shapely`, :index:`raster to polygons`, :index:`polygons`, :index:`vectorise`, :index:`no_testing`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
