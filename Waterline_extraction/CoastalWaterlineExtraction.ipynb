{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract coastal waterlines across time\n",
    "**What does this notebook do?** \n",
    "\n",
    "This notebooks demonstrates how to tidally tag remotely sensed imagery using the [OSU Tidal Prediction Software or OTPS](http://volkov.oce.orst.edu/tides/otps.html) model, create geomedian composites for a given set of epochs and tidal range, and extract waterline contours from the composite layers for each epoch. \n",
    "\n",
    "**Requirements:** \n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea/20180515`  *(currently using an older version of `dea` due to a bug in `xr.concat`; will be reverted to `module load dea` in future)*\n",
    "\n",
    "`module load otps`\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "**Date:** September 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`tidal_model`, :index:`OTPS`, :index:`tidal_tagging`, :index:`predict_tide`, :index:`composites`, :index:`dask`, :index:`write_geotiff`,  :index:`filmstrip_plot`, :index:`geopandas`, :index:`point_in_polygon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datetime import datetime, timedelta\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.storage import masking\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools, DEAPlotting\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result\n",
    "\n",
    "def ds_to_rgb(ds, bands=['red', 'green', 'blue'], reflect_stand=4000):\n",
    "    \n",
    "    \"\"\"Converts an xarray dataset to a three band array\"\"\"\n",
    "    \n",
    "    rawimg = np.transpose(ds[bands].to_array().data, [1, 2, 0])\n",
    "    img_toshow = (rawimg / reflect_stand).clip(0, 1)\n",
    "    return img_toshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite. This is the only cell that needs to be edited to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f2dd4f3e2e8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'pointstuart_wide'  # name used as prefix for output files\n",
    "# lat, lon = -12.238, 131.849  # centre of study area\n",
    "# buffer = 15000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'mitchell'  # name used as prefix for output files\n",
    "# lat, lon = -15.14995, 141.63650  # centre of study area\n",
    "# buffer = 10000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'inskip'  # name used as prefix for output files\n",
    "# lat, lon = -25.7909598625, 153.073231901  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'narrabeen'  # name used as prefix for output files\n",
    "# lat, lon = -33.72, 151.3006  # centre of study area\n",
    "# buffer = 2000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'victoriariver_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -15.4440342949, 129.811912341  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'victoriariver2_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -15.4475061785, 130.128534164  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'ard_test_comp'  # name used as prefix for output files\n",
    "# lat, lon = -19.4889944734, 147.507306961  # centre of study area\n",
    "# buffer = 10800  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'kingashbaylarge'  # name used as prefix for output files\n",
    "# lat, lon = -15.913720, 136.494590  # centre of study area\n",
    "# buffer = 10000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'oldbar'  # name used as prefix for output files\n",
    "# lat, lon = -31.965, 152.593  # centre of study area\n",
    "# buffer = 2000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'seals'  # name used as prefix for output files\n",
    "# lat, lon = -32.443603, 152.527712  # centre of study area\n",
    "# buffer = 2000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'goldcoast1'  # name used as prefix for output files\n",
    "# lat, lon = -28.157917, 153.52392  # centre of study area\n",
    "# buffer = 3300  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'apollobay_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -38.759958, 143.672176  # centre of study area\n",
    "# buffer = 3300  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up centre of study area and buffer size in metres for data extraction\n",
    "study_area = 'nellybay'  # name used as prefix for output files\n",
    "lat, lon = -19.162193, 146.849419  # centre of study area\n",
    "buffer = 2500  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up compositing parameters\n",
    "time_period = ('1987-01-01', '2019-01-01')  # Total date range to import data from\n",
    "sensors = ['ls5', 'ls7', 'ls8']  # Landsat sensors from which to import data from\n",
    "ls7_slc_off = False # Whether to include striped > May 31 2003 Landsat 7 SLC-off data\n",
    "lower_tideheight = 0.0 # Minimum proportion of the observed tidal range to include \n",
    "upper_tideheight = 1.0  # Maximum proportion of the observed tidal range to include \n",
    "epoch_years = 1  # Length of each epoch used to compute geomedians (i.e. 5 years = 1988 to 1993)\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'time': time_period,\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "DEAPlotting.display_map(x=query['x'], y=query['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return observations for each sensor for the entire time series\n",
    "Use `dask` to lazily load Landsat data and PQ for each sensor for the entire time series. No data is actually loaded here: this is saved until after the layers have been filtered by tidal stage and date to save processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 32 epochs: 1987-01-01, 1988-01-01, 1989-01-01, 1990-01-01, 1991-01-01, 1992-01-01, 1993-01-01, 1994-01-01, 1995-01-01, 1996-01-01, 1997-01-01, 1998-01-01, 1999-01-01, 2000-01-01, 2001-01-01, 2002-01-01, 2003-01-01, 2004-01-01, 2005-01-01, 2006-01-01, 2007-01-01, 2008-01-01, 2009-01-01, 2010-01-01, 2011-01-01, 2012-01-01, 2013-01-01, 2014-01-01, 2015-01-01, 2016-01-01, 2017-01-01, 2018-01-01\n",
      "Using tidepost coordinates: 146.849419 E, -19.162193 S\n",
      "\n",
      "Computing tidal heights for 656 ls5 observations\n",
      "Computing tidal heights for 125 ls7 observations\n",
      "Computing tidal heights for 249 ls8 observations\n"
     ]
    }
   ],
   "source": [
    "# If output data and figure directories doesn't exist, create them\n",
    "if not os.path.isdir('output_data/{}/'.format(study_area)):\n",
    "    os.makedirs('output_data/{}/'.format(study_area))\n",
    "    \n",
    "if not os.path.isdir('figures/{}/'.format(study_area)):\n",
    "    os.makedirs('figures/{}/'.format(study_area))\n",
    "\n",
    "# Create list of epochs between start and end of time_period in datetime format\n",
    "start = datetime.strptime(time_period[0], '%Y-%m-%d')\n",
    "end = datetime.strptime(time_period[1], '%Y-%m-%d')\n",
    "epochs = date_range(start, end, epoch_years, 'years')\n",
    "\n",
    "# epochs = pd.date_range(start=time_period[0], end=time_period[1],  freq='AS')\n",
    "\n",
    "\n",
    "# Print list of epochs\n",
    "epochs_strings = [epoch.strftime('%Y-%m-%d') for epoch in epochs][:-1]\n",
    "print('Processing {} epochs: {}'.format(len(epochs_strings), ', '.join(epochs_strings)))\n",
    "\n",
    "# # Identify tide point by importing ITEM polygons\n",
    "# polygons_gpd = gpd.read_file('/g/data/r78/intertidal/GA_native_tidal_model.shp')\n",
    "# polygon_gpd = polygons_gpd[polygons_gpd.intersects(Point(lon, lat))]\n",
    "# tidepost_lon, tidepost_lat = polygon_gpd[['lon', 'lat']].values[0]\n",
    "tidepost_lat, tidepost_lon  = lat, lon\n",
    "print('Using tidepost coordinates: {} E, {} S\\n'.format(tidepost_lon, tidepost_lat))\n",
    "\n",
    "# Output dicts to hold entire time-series for each sensor\n",
    "sensor_dict = {}\n",
    "pq_dict = {}\n",
    "\n",
    "# For each sensor, dask load data and compute tide heights for each sensor\n",
    "for sensor in sensors:\n",
    "    \n",
    "    # The following code will fail if there is no data for the sensor in time_period;\n",
    "    # this try statement catches this and skips the affected sensor    \n",
    "    try:\n",
    "    \n",
    "        # Return observations matching query without actually loading them using dask\n",
    "        sensor_all = dc.load(product = '{}_nbart_albers'.format(sensor), \n",
    "                         group_by = 'solar_day', \n",
    "                         measurements = ['blue', 'green', 'nir', 'swir1', 'swir2'],\n",
    "                         dask_chunks={'time': 1},\n",
    "                         **query)\n",
    "\n",
    "        # Load PQ data matching query without actually loading them using dask\n",
    "        pq_all = dc.load(product = '{}_pq_albers'.format(sensor),\n",
    "                        group_by = 'solar_day',\n",
    "                        fuse_func=ga_pq_fuser, \n",
    "                        dask_chunks={'time': 1},\n",
    "                        **query)\n",
    "        \n",
    "        # Remove Landsat 7 SLC-off from PQ layer if ls7_slc_off = False\n",
    "        if not ls7_slc_off and sensor == 'ls7':\n",
    "            sensor_all = sensor_all.where(sensor_all.time < np.datetime64('2003-05-30'), drop=True) \n",
    "\n",
    "        # Use the tidal mode to compute tide heights for each observation:\n",
    "        print('Computing tidal heights for {} {} observations'.format(len(sensor_all.time), sensor))\n",
    "        obs_datetimes = sensor_all.time.data.astype('M8[s]').astype('O').tolist()\n",
    "        obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "        obs_predictedtides = predict_tide(obs_timepoints)\n",
    "        obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "        # Assign tide heights to the dataset as a new variable\n",
    "        sensor_all['tide_heights'] = xr.DataArray(obs_tideheights, [('time', sensor_all.time)]) \n",
    "\n",
    "        # Append results for each sensor to a dictionary with sensor name as the key\n",
    "        sensor_dict[sensor] = sensor_all\n",
    "        pq_dict[sensor] = pq_all \n",
    "    \n",
    "    except AttributeError:\n",
    "        \n",
    "        print('{} selection failed due to likely lack of data in time_period; skipping'.format(sensor))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data by tidal height and epoch time range\n",
    "Use image time-stamps to compute tidal heights at the time of each observation, and take a subset of the entire Landsat and PQ time series based on tide height and epoch date range for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping tidal heights of -0.88 m to 1.83 m out of an observed local tidal range of -0.88 m to 1.83 m\n",
      "    Filtering from 1987-01-01 to 1988-01-01 for ls5\n",
      "    Filtering from 1987-01-01 to 1988-01-01 for ls7\n",
      "    Filtering from 1987-01-01 to 1988-01-01 for ls8\n",
      "    Filtering from 1988-01-01 to 1989-01-01 for ls5\n",
      "    Filtering from 1988-01-01 to 1989-01-01 for ls7\n",
      "    Filtering from 1988-01-01 to 1989-01-01 for ls8\n",
      "    Filtering from 1989-01-01 to 1990-01-01 for ls5\n",
      "    Filtering from 1989-01-01 to 1990-01-01 for ls7\n",
      "    Filtering from 1989-01-01 to 1990-01-01 for ls8\n",
      "    Filtering from 1990-01-01 to 1991-01-01 for ls5\n",
      "    Filtering from 1990-01-01 to 1991-01-01 for ls7\n",
      "    Filtering from 1990-01-01 to 1991-01-01 for ls8\n",
      "    Filtering from 1991-01-01 to 1992-01-01 for ls5\n",
      "    Filtering from 1991-01-01 to 1992-01-01 for ls7\n",
      "    Filtering from 1991-01-01 to 1992-01-01 for ls8\n",
      "    Filtering from 1992-01-01 to 1993-01-01 for ls5\n",
      "    Filtering from 1992-01-01 to 1993-01-01 for ls7\n",
      "    Filtering from 1992-01-01 to 1993-01-01 for ls8\n",
      "    Filtering from 1993-01-01 to 1994-01-01 for ls5\n",
      "    Filtering from 1993-01-01 to 1994-01-01 for ls7\n",
      "    Filtering from 1993-01-01 to 1994-01-01 for ls8\n",
      "    Filtering from 1994-01-01 to 1995-01-01 for ls5\n",
      "    Filtering from 1994-01-01 to 1995-01-01 for ls7\n",
      "    Filtering from 1994-01-01 to 1995-01-01 for ls8\n",
      "    Filtering from 1995-01-01 to 1996-01-01 for ls5\n",
      "    Filtering from 1995-01-01 to 1996-01-01 for ls7\n",
      "    Filtering from 1995-01-01 to 1996-01-01 for ls8\n",
      "    Filtering from 1996-01-01 to 1997-01-01 for ls5\n",
      "    Filtering from 1996-01-01 to 1997-01-01 for ls7\n",
      "    Filtering from 1996-01-01 to 1997-01-01 for ls8\n",
      "    Filtering from 1997-01-01 to 1998-01-01 for ls5\n",
      "    Filtering from 1997-01-01 to 1998-01-01 for ls7\n",
      "    Filtering from 1997-01-01 to 1998-01-01 for ls8\n",
      "    Filtering from 1998-01-01 to 1999-01-01 for ls5\n",
      "    Filtering from 1998-01-01 to 1999-01-01 for ls7\n",
      "    Filtering from 1998-01-01 to 1999-01-01 for ls8\n",
      "    Filtering from 1999-01-01 to 2000-01-01 for ls5\n",
      "    Filtering from 1999-01-01 to 2000-01-01 for ls7\n",
      "    Filtering from 1999-01-01 to 2000-01-01 for ls8\n",
      "    Filtering from 2000-01-01 to 2001-01-01 for ls5\n",
      "    Filtering from 2000-01-01 to 2001-01-01 for ls7\n",
      "    Filtering from 2000-01-01 to 2001-01-01 for ls8\n",
      "    Filtering from 2001-01-01 to 2002-01-01 for ls5\n",
      "    Filtering from 2001-01-01 to 2002-01-01 for ls7\n",
      "    Filtering from 2001-01-01 to 2002-01-01 for ls8\n",
      "    Filtering from 2002-01-01 to 2003-01-01 for ls5\n",
      "    Filtering from 2002-01-01 to 2003-01-01 for ls7\n",
      "    Filtering from 2002-01-01 to 2003-01-01 for ls8\n",
      "    Filtering from 2003-01-01 to 2004-01-01 for ls5\n",
      "    Filtering from 2003-01-01 to 2004-01-01 for ls7\n",
      "    Filtering from 2003-01-01 to 2004-01-01 for ls8\n",
      "    Filtering from 2004-01-01 to 2005-01-01 for ls5\n",
      "    Filtering from 2004-01-01 to 2005-01-01 for ls7\n",
      "    Filtering from 2004-01-01 to 2005-01-01 for ls8\n",
      "    Filtering from 2005-01-01 to 2006-01-01 for ls5\n",
      "    Filtering from 2005-01-01 to 2006-01-01 for ls7\n",
      "    Filtering from 2005-01-01 to 2006-01-01 for ls8\n",
      "    Filtering from 2006-01-01 to 2007-01-01 for ls5\n",
      "    Filtering from 2006-01-01 to 2007-01-01 for ls7\n",
      "    Filtering from 2006-01-01 to 2007-01-01 for ls8\n",
      "    Filtering from 2007-01-01 to 2008-01-01 for ls5\n",
      "    Filtering from 2007-01-01 to 2008-01-01 for ls7\n",
      "    Filtering from 2007-01-01 to 2008-01-01 for ls8\n",
      "    Filtering from 2008-01-01 to 2009-01-01 for ls5\n",
      "    Filtering from 2008-01-01 to 2009-01-01 for ls7\n",
      "    Filtering from 2008-01-01 to 2009-01-01 for ls8\n",
      "    Filtering from 2009-01-01 to 2010-01-01 for ls5\n",
      "    Filtering from 2009-01-01 to 2010-01-01 for ls7\n",
      "    Filtering from 2009-01-01 to 2010-01-01 for ls8\n",
      "    Filtering from 2010-01-01 to 2011-01-01 for ls5\n",
      "    Filtering from 2010-01-01 to 2011-01-01 for ls7\n",
      "    Filtering from 2010-01-01 to 2011-01-01 for ls8\n",
      "    Filtering from 2011-01-01 to 2012-01-01 for ls5\n",
      "    Filtering from 2011-01-01 to 2012-01-01 for ls7\n",
      "    Filtering from 2011-01-01 to 2012-01-01 for ls8\n",
      "    Filtering from 2012-01-01 to 2013-01-01 for ls5\n",
      "    Filtering from 2012-01-01 to 2013-01-01 for ls7\n",
      "    Filtering from 2012-01-01 to 2013-01-01 for ls8\n",
      "    Filtering from 2013-01-01 to 2014-01-01 for ls5\n",
      "    Filtering from 2013-01-01 to 2014-01-01 for ls7\n",
      "    Filtering from 2013-01-01 to 2014-01-01 for ls8\n",
      "    Filtering from 2014-01-01 to 2015-01-01 for ls5\n",
      "    Filtering from 2014-01-01 to 2015-01-01 for ls7\n",
      "    Filtering from 2014-01-01 to 2015-01-01 for ls8\n",
      "    Filtering from 2015-01-01 to 2016-01-01 for ls5\n",
      "    Filtering from 2015-01-01 to 2016-01-01 for ls7\n",
      "    Filtering from 2015-01-01 to 2016-01-01 for ls8\n",
      "    Filtering from 2016-01-01 to 2017-01-01 for ls5\n",
      "    Filtering from 2016-01-01 to 2017-01-01 for ls7\n",
      "    Filtering from 2016-01-01 to 2017-01-01 for ls8\n",
      "    Filtering from 2017-01-01 to 2018-01-01 for ls5\n",
      "    Filtering from 2017-01-01 to 2018-01-01 for ls7\n",
      "    Filtering from 2017-01-01 to 2018-01-01 for ls8\n",
      "    Filtering from 2018-01-01 to 2019-01-01 for ls5\n",
      "    Filtering from 2018-01-01 to 2019-01-01 for ls7\n",
      "    Filtering from 2018-01-01 to 2019-01-01 for ls8\n"
     ]
    }
   ],
   "source": [
    "# If any sensor has 0 observations, remove it from the dictionary before proceeding\n",
    "sensor_dict = {key:value for key, value in sensor_dict.items() if len(value.time) > 0}\n",
    "\n",
    "# Calculate max and min tide heights for the entire time series and all sensors\n",
    "obs_min = np.min([sensor_ds.tide_heights.min() for sensor_ds in sensor_dict.values()])\n",
    "obs_max = np.max([sensor_ds.tide_heights.max() for sensor_ds in sensor_dict.values()])\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for subsequent data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Keeping tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))\n",
    "\n",
    "# Create dictionaries to hold filtered sensor data for each epoch\n",
    "sensor_epoch_dict = defaultdict(list)\n",
    "pq_epoch_dict = defaultdict(list)\n",
    "   \n",
    "for epoch, sensor in itertools.product(epochs[:-1], sensor_dict.keys()):\n",
    "                                           \n",
    "    # Select dataset\n",
    "    sensor_all = sensor_dict[sensor]  \n",
    "    pq_all = pq_dict[sensor] \n",
    "    \n",
    "    # Filter to keep only observations that have matching PQ data \n",
    "    time = (sensor_all.time - pq_all.time).time\n",
    "    sensor_subset = sensor_all.sel(time=time)\n",
    "    pq_subset = pq_all.sel(time=time)\n",
    "                                           \n",
    "    # Filter by tidal stage\n",
    "    sensor_subset = sensor_subset.where((sensor_subset.tide_heights >= sel_min) &\n",
    "                                        (sensor_subset.tide_heights <= sel_max), drop = True)\n",
    "    \n",
    "    # Filter pq to same timesteps (this avoids conversion to float by `.where`)\n",
    "    pq_subset = pq_subset.sel(time = sensor_subset.time)\n",
    "\n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Subset sensor to from and to date for epoch\n",
    "    print('    Filtering from {} to {} for {}'.format(from_date, to_date, sensor))\n",
    "    sensor_subset = sensor_subset.sel(time=slice(from_date, to_date))\n",
    "    pq_subset = pq_subset.sel(time=slice(from_date, to_date))\n",
    "\n",
    "    # Add subsetted data to dicts (one key matching a list of sensor data for each epoch)\n",
    "    sensor_epoch_dict[from_date].append(sensor_subset)\n",
    "    pq_epoch_dict[from_date].append(pq_subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1987-01-01\n",
      "    15 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1988-01-01\n",
      "    32 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1989-01-01\n",
      "    28 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1990-01-01\n",
      "    30 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1991-01-01\n",
      "    34 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1992-01-01\n",
      "    37 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1993-01-01\n",
      "    36 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1994-01-01\n",
      "    38 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1995-01-01\n",
      "    35 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1996-01-01\n",
      "    34 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1997-01-01\n",
      "    28 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1998-01-01\n",
      "    26 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 1999-01-01\n",
      "    42 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2000-01-01\n",
      "    27 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2001-01-01\n",
      "    36 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2002-01-01\n",
      "    36 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2003-01-01\n",
      "    32 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2004-01-01\n",
      "    39 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2005-01-01\n",
      "    33 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2006-01-01\n",
      "    32 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2007-01-01\n",
      "    23 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2008-01-01\n",
      "    27 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2009-01-01\n",
      "    36 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2010-01-01\n",
      "    13 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2011-01-01\n",
      "    25 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2012-01-01\n",
      "    0 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2013-01-01\n",
      "    29 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2014-01-01\n",
      "    40 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2015-01-01\n",
      "    44 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2016-01-01\n",
      "    46 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2017-01-01\n",
      "    46 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n",
      "Loading and combining ls5, ls7, ls8 data for 2018-01-01\n",
      "    44 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    }
   ],
   "source": [
    "# Dict to hold output geomedian composites\n",
    "ndwi_dict = {}\n",
    "mndwi_dict = {}\n",
    "awei_ns_dict = {}\n",
    "awei_s_dict = {}\n",
    "tideheights_dict = {}\n",
    "\n",
    "for from_date in sensor_epoch_dict.keys():\n",
    "    \n",
    "    # For the first time, actually load data for all sensors and then combine into one dataset  \n",
    "    print('Loading and combining {} data for {}'.format(\", \".join(sensor_dict.keys()), from_date))\n",
    "    sensor_combined = xr.concat([i.compute() for i in sensor_epoch_dict[from_date]], dim='time')\n",
    "    pq_combined = xr.concat([i.compute() for i in pq_epoch_dict[from_date]], dim='time')\n",
    "    print('    {} observations combined'.format(len(sensor_combined.time)))\n",
    "    \n",
    "    # Manually add flag definition back in (it is lost during the concatenation)\n",
    "    pq_combined.pixelquality.attrs['flags_definition'] = pq_all.pixelquality.flags_definition\n",
    "    \n",
    "    # Sort output datasets by date\n",
    "    sensor_combined = sensor_combined.sortby('time')\n",
    "    pq_combined = pq_combined.sortby('time')\n",
    "    \n",
    "    # Identify pixels with no clouds/shadows in either ACCA for Fmask\n",
    "    good_quality = masking.make_mask(pq_combined.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True)\n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    print('    Applying PQ mask and setting nodata to NaN')\n",
    "    sensor_combined = sensor_combined.where(good_quality)\n",
    "    sensor_combined = masking.mask_invalid_data(sensor_combined)\n",
    "    \n",
    "    # Compute NDWI\n",
    "    sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.nir) / (sensor_combined.green + sensor_combined.nir)\n",
    "    sensor_combined[\"mndwi\"] = (sensor_combined.green - sensor_combined.swir1) / (sensor_combined.green + sensor_combined.swir1)\n",
    "    sensor_combined[\"awei_ns\"] = (4 * (sensor_combined.green * 0.0001 - sensor_combined.swir1 * 0.0001) -\n",
    "                                 (0.25 * sensor_combined.nir * 0.0001 + 2.75 * sensor_combined.swir2 * 0.0001))\n",
    "    sensor_combined[\"awei_s\"] = (sensor_combined.blue * 0.0001 + 2.5 * sensor_combined.green * 0.0001 - \n",
    "                                 1.5 * (sensor_combined.nir * 0.0001 + sensor_combined.swir1 * 0.0001) - 0.25 * sensor_combined.swir2 * 0.0001)\n",
    "\n",
    "    # Compute NDWI composite using all timesteps\n",
    "    print('    Computing NDWI, MNDWI and AWEI median')\n",
    "    ndwi_median = sensor_combined[[\"ndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    mndwi_median = sensor_combined[[\"mndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    awei_ns_median = sensor_combined[[\"awei_ns\"]].median(dim='time', keep_attrs=True)\n",
    "    awei_s_median = sensor_combined[[\"awei_s\"]].median(dim='time', keep_attrs=True)\n",
    "    \n",
    "    # Export to file\n",
    "    print('    Exporting to file')\n",
    "    filename_ndwi = 'output_data/{0}/{0}_{1}_ndwi.tif'.format(study_area, from_date)\n",
    "    filename_mndwi = 'output_data/{0}/{0}_{1}_mndwi.tif'.format(study_area, from_date)\n",
    "    filename_awei_ns = 'output_data/{0}/{0}_{1}_awei_ns.tif'.format(study_area, from_date)\n",
    "    filename_awei_s = 'output_data/{0}/{0}_{1}_awei_s.tif'.format(study_area, from_date)\n",
    "    write_geotiff(filename=filename_ndwi, dataset=ndwi_median)\n",
    "    write_geotiff(filename=filename_mndwi, dataset=mndwi_median)\n",
    "    write_geotiff(filename=filename_awei_ns, dataset=awei_ns_median)\n",
    "    write_geotiff(filename=filename_awei_s, dataset=awei_s_median)\n",
    "    \n",
    "    # Assign to dict\n",
    "    ndwi_dict[from_date] = ndwi_median\n",
    "    mndwi_dict[from_date] = mndwi_median\n",
    "    awei_ns_dict[from_date] = awei_ns_median\n",
    "    awei_s_dict[from_date] = awei_s_median\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1987-01-01.shp\n",
      "1988-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1988-01-01.shp\n",
      "1989-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1989-01-01.shp\n",
      "1990-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1990-01-01.shp\n",
      "1991-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1991-01-01.shp\n",
      "1992-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1992-01-01.shp\n",
      "1993-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1993-01-01.shp\n",
      "1994-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1994-01-01.shp\n",
      "1995-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1995-01-01.shp\n",
      "1996-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1996-01-01.shp\n",
      "1997-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1997-01-01.shp\n",
      "1998-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1998-01-01.shp\n",
      "1999-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_1999-01-01.shp\n",
      "2000-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2000-01-01.shp\n",
      "2001-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2001-01-01.shp\n",
      "2002-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2002-01-01.shp\n",
      "2003-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2003-01-01.shp\n",
      "2004-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2004-01-01.shp\n",
      "2005-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2005-01-01.shp\n",
      "2006-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2006-01-01.shp\n",
      "2007-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2007-01-01.shp\n",
      "2008-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2008-01-01.shp\n",
      "2009-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2009-01-01.shp\n",
      "2010-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2010-01-01.shp\n",
      "2011-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2011-01-01.shp\n",
      "2012-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "    No data for contour 0; skipping\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2012-01-01.shp\n",
      "2013-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2013-01-01.shp\n",
      "2014-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2014-01-01.shp\n",
      "2015-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2015-01-01.shp\n",
      "2016-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2016-01-01.shp\n",
      "2017-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2017-01-01.shp\n",
      "2018-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0\n",
      "Exporting contour shapefile to output_data/nellybay/nellybay_awei_s_2018-01-01.shp\n"
     ]
    }
   ],
   "source": [
    "threshold = 0\n",
    "metric = 'awei_s'\n",
    "\n",
    "from skimage import filters\n",
    "\n",
    "for date, data in awei_s_dict.items():\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'date': [date[0:4]]}\n",
    "    attribute_dtypes = {'date': 'int'}\n",
    "    \n",
    "    try:\n",
    "\n",
    "#         threshold = filters.threshold_otsu(data[metric].values.flatten()[~np.isnan(data[metric].values.flatten())])\n",
    "\n",
    "        # Extract contours with custom attribute fields:\n",
    "        contour_dict = SpatialTools.contour_extract(z_values=[threshold],\n",
    "                                       ds_array=data[metric],\n",
    "                                       ds_crs='epsg:3577',\n",
    "                                       ds_affine=data.geobox.transform,\n",
    "                                       output_shp=f'output_data/{study_area}/{study_area}_{metric}_{date}.shp',\n",
    "                                       min_vertices=20,\n",
    "                                       attribute_data=attribute_data,\n",
    "                                       attribute_dtypes=attribute_dtypes)\n",
    "    except:\n",
    "        print('none')\n",
    "    \n",
    "# Combine all shapefiles into one file\n",
    "import glob\n",
    "shapefiles = glob.glob(f'output_data/{study_area}/{study_area}_{metric}_*01-01.shp')\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['date', 'geometry']].sort_values('date')\n",
    "gdf['date'] = gdf['date'].astype(np.int16)\n",
    "gdf.crs = 'epsg:3577'\n",
    "gdf.to_file(f'output_data/{study_area}/{study_area}_{metric}_combined.shp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.337275 , -0.386875 , -0.4522   , ...,  0.050975 ,  0.051475 ,\n",
       "         0.0518875],\n",
       "       [-0.33955  , -0.254425 , -0.36345  , ...,  0.0514375,  0.050875 ,\n",
       "         0.0511375],\n",
       "       [-0.397625 , -0.2524   , -0.261    , ...,  0.05215  ,  0.0510375,\n",
       "         0.051125 ],\n",
       "       ...,\n",
       "       [-0.5185625, -0.5258875, -0.3862375, ...,  0.130575 ,  0.1239   ,\n",
       "         0.1191125],\n",
       "       [-0.515825 , -0.50615  , -0.4653125, ...,  0.12385  ,  0.1183875,\n",
       "         0.112975 ],\n",
       "       [-0.502025 , -0.51695  , -0.48725  , ...,  0.1256   ,  0.118375 ,\n",
       "         0.112775 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[metric].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.337275, -0.386875, -0.4522  , ...,  0.1256  ,  0.118375,\n",
       "        0.112775])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation against Narrabeen beach width data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pyproj import Proj, transform\n",
    "# from shapely.geometry import Point, LineString\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def dms2dd(s):\n",
    "#     # example: s = \"051'56.29\"\n",
    "#     degrees, minutes, seconds = re.split('[\\'\"]+', s)\n",
    "#     if float(degrees) > 0:\n",
    "#         dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60)\n",
    "#     else:\n",
    "#         dd = float(degrees) - float(minutes)/60 - float(seconds)/(60*60);\n",
    "#     return dd\n",
    "\n",
    "# def dist_angle(lon, lat, dist, angle):\n",
    "#     lon_2 = lon + dist *  np.sin(angle * np.pi / 180)\n",
    "#     lat_2 = lat + dist *  np.cos(angle * np.pi / 180)\n",
    "#     return pd.Series({'y2': lat_2, 'x2': lon_2})\n",
    "\n",
    "\n",
    "# # Import data and parse DMS to DD\n",
    "# data = \"PF1 -3342'20.65 15118'16.30 118.42\\nPF2 -3342'33.45 15118'10.33 113.36\\nPF4 -3343'01.55 15117'58.84 100.26\\nPF6 -3343'29.81 15117'58.65 83.65\\nPF8 -3343'55.94 15118'06.47 60.48\"\n",
    "# coords = pd.read_csv(pd.compat.StringIO(data), sep=' ', names=['site', 'y', 'x', 'angle'])\n",
    "# coords['x'] = [dms2dd(i) for i in coords.x]\n",
    "# coords['y'] = [dms2dd(i) for i in coords.y]\n",
    "\n",
    "# # Extend survey lines out from start coordinates using supplied angle\n",
    "# coords_end = coords.apply(lambda x: dist_angle(x.x, x.y, 0.005, x.angle), axis = 1)\n",
    "# coords = pd.concat([coords, coords_end], axis=1).drop('angle', axis=1)\n",
    "\n",
    "# # Reproject coords to Albers and create geodataframe\n",
    "# inProj = Proj(init='epsg:4326')\n",
    "# outProj = Proj(init='epsg:3577')\n",
    "# coords['x'], coords['y'] = transform(inProj,outProj,coords.x.values,coords.y.values)\n",
    "# coords['x2'], coords['y2'] = transform(inProj,outProj,coords.x2.values,coords.y2.values)\n",
    "# coords['geometry'] = coords.apply(lambda x: LineString([Point(x.x, x.y), Point(x.x2, x.y2)]), axis = 1)\n",
    "# transects_gdf = gpd.GeoDataFrame(coords, geometry='geometry').set_index('site')\n",
    "# transects_gdf.crs = 'EPSG:3577'\n",
    "\n",
    "# # Export to file\n",
    "# transects_gdf.reset_index().to_file('narrabeen_transects.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_list = []\n",
    "\n",
    "# for transect_id in ['PF1', 'PF2', 'PF4', 'PF6', 'PF8']:\n",
    "\n",
    "#     ###########\n",
    "#     # Landsat #\n",
    "#     ###########\n",
    "\n",
    "#     # Import contours\n",
    "#     contours_gpf = gpd.read_file('output_data/narrabeen/narrabeen_ndwi_combined.shp').set_index('date')\n",
    "#     contours_gdf = contours_gpf.drop([1987.0, 2016.0, 2017.0, 2018.0])\n",
    "\n",
    "#     # Extract intersections for transect \n",
    "#     intersect_gdf = gpd.GeoDataFrame(geometry = contours_gdf.intersection(transects_gdf.loc[transect_id].geometry))\n",
    "#     intersect_gdf.crs = 'EPSG:3577'\n",
    "#     intersect_gdf.reset_index().to_file('intersect_transect1.shp')\n",
    "\n",
    "#     # Compute distances\n",
    "#     landsat_beachwidths = intersect_gdf.distance(Point(transects_gdf.loc[transect_id].geometry.coords[0]))\n",
    "#     landsat_beachwidths = landsat_beachwidths.rename('Landsat beach width (m)')\n",
    "\n",
    "\n",
    "#     ##############\n",
    "#     # Validation #\n",
    "#     ##############\n",
    "\n",
    "#     profiles_df = pd.read_csv('/g/data/r78/rt1527/dea-notebooks/Waterline_extraction/validation/Narrabeen/Narrabeen_Profiles.csv', \n",
    "#                               skiprows=1, names=['site', 'date', 'distance', 'elevation', 'flag'])\n",
    "\n",
    "#     # Set to datetime\n",
    "#     profiles_df['date'] = pd.to_datetime(profiles_df['date'], format='%d/%m/%Y')\n",
    "\n",
    "#     # Restrict to years\n",
    "#     profiles_df = profiles_df[(profiles_df.date.dt.year > 1987) & (profiles_df.date.dt.year < 2016)]\n",
    "\n",
    "#     # Use linear interpolation to find distance along beach of 0 elevation \n",
    "#     profiles_interp = profiles_df.groupby(['site', 'date']).apply(lambda x: interp1d(x.elevation, x.distance)(0) if \n",
    "#                                                                     x.elevation.min() < 0 else np.nan)  #.plot()\n",
    "\n",
    "#     profiles_interp = profiles_interp.loc[transect_id].astype(np.float)\n",
    "#     validation_beachwidths = profiles_interp.groupby(profiles_interp.index.year).mean()\n",
    "#     validation_beachwidths_std = profiles_interp.groupby(profiles_interp.index.year).std()\n",
    "#     validation_beachwidths = validation_beachwidths.rename('Validation beach width (m)')\n",
    "#     validation_beachwidths_std = validation_beachwidths_std.rename('Validation beach width (stdev)')\n",
    "\n",
    "#     # Combine into a single dataframe and append to output list\n",
    "#     landsat_validation_df = pd.DataFrame([validation_beachwidths, validation_beachwidths_std, landsat_beachwidths]).T\n",
    "#     landsat_validation_df['site'] = transect_id\n",
    "#     landsat_validation_df['RMSE'] = ((landsat_validation_df['Validation beach width (m)'] - landsat_validation_df['Landsat beach width (m)']) ** 2).mean() ** .5\n",
    "#     out_list.append(landsat_validation_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot transects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landsat_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df_wide = pd.concat(out_list).reset_index()\n",
    "# combined_df_wide['Validation beach width (min stdev)'] = combined_df_wide['Validation beach width (m)'] - combined_df_wide['Validation beach width (stdev)']\n",
    "# combined_df_wide['Validation beach width (max stdev)'] = combined_df_wide['Validation beach width (m)'] + combined_df_wide['Validation beach width (stdev)']\n",
    "# combined_df_wide =  combined_df_wide.drop(['Validation beach width (stdev)'], axis=1)\n",
    "# combined_df_wide = combined_df_wide.iloc[:, [0, 1, 5, 6, 2, 3, 4]]\n",
    "\n",
    "# combined_df = pd.melt(combined_df_wide, id_vars=['site', 'date', 'RMSE'], value_name='Beach width (m)')\n",
    "# g = sns.catplot(data=combined_df, x='date', y='Beach width (m)', hue='variable', col='site', kind='point', legend=False, col_wrap=3, \n",
    "#                 palette=['#1f77b4', '#a8d2f0', '#a8d2f0', '#ff7f0e'], markers=[\".\", \"\", \"\", \".\"], linestyles=['-', ':', ':', '-'])\n",
    "# g.set_xticklabels(rotation=90)\n",
    "\n",
    "# axes = g.axes.flatten()\n",
    "# for i, ax in enumerate(axes):\n",
    "#     current_title = ax.get_title()\n",
    "#     current_rmse = np.round(combined_df.groupby('site').RMSE.max()[i], 2)\n",
    "#     ax.set_title(f'{current_title}\\nRMSE = {np.round(current_rmse, 2)} m')\n",
    "# plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(data=combined_df_wide, x='Validation beach width (m)', y='Landsat beach width (m)', height=10, aspect=1.0)\n",
    "# plt.plot(np.linspace(0, 150), np.linspace(0, 150), 'black', linestyle='dashed')\n",
    "# plt.ylim([10,150])\n",
    "# plt.xlim([10,150])\n",
    "\n",
    "# print(((combined_df_wide['Validation beach width (m)'] - combined_df_wide['Landsat beach width (m)']) ** 2).mean() ** .5)\n",
    "# import scipy.stats\n",
    "# print(scipy.stats.linregress(combined_df_wide['Validation beach width (m)'], combined_df_wide['Landsat beach width (m)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(data=combined_df_wide, x='Validation beach width (m)', y='Landsat beach width (m)',hue='site', height=7, aspect=1.0)\n",
    "# plt.plot(np.linspace(0, 150), np.linspace(0, 150), 'black', linestyle='dashed')\n",
    "# plt.ylim([10,150]); plt.xlim([10,150]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
