{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Uploading Data to AWS\n",
    "\n",
    "These instructions are indended as a starting point. Please add to them if you find steps that are missing etc.\n",
    "\n",
    "**Date** June 2018\n",
    "\n",
    "**Author** Claire Krause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an AWS account\n",
    "\n",
    "Before you can do anything, you need to get a GA AWS account. This will give you permissions to move data onto the DEA AWS S3 bucket. Speak to DEA's cloud engineer to organise an account. They can also point you to the correct place to load data, and will make sure the data you intend to upload can be accommodated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the required data into cog tif files\n",
    "\n",
    "The preferred format for datasets on S3 is cog format, aka 'cloud optimised geoTIFF'. This file format is smaller than netCDF, enables pyramid loading, and is supported by `rasterio`.\n",
    "\n",
    "### Translating data into cog format\n",
    "\n",
    "[Harshu](https://github.com/harshurampur) has written some helper functions for translating data into cog format. This code can be found in the [Geotiff-conversion repo](https://github.com/harshurampur/Geotiff-conversion). To use this code, clone the repo into a suitable location (I used my folder on `/g/data/...`). For translation, there are two functions to choose from, depending on the current format of the data you want to upload: \n",
    "* `geotiff-cog.py` - for translating GeoTIFF to cog geoTIFFs\n",
    "* `netcdf-cog.py` - for translating netCDF to cog geoTIFFs\n",
    "\n",
    "To run the translation scripts, you will need to specify the path of the input and output files. You can then run the scripts from the command line. (You will need to have the DEA library loaded as this script requires some libraries to run):\n",
    "\n",
    "`python geotiff-cog.py -p /path/to/directory/of/files/for/translation -o /path/to/output/directory`\n",
    "\n",
    "The script will create a directory in the output location that mirrors the directory name containing the input files. This script can handle data with multiple bands and time steps. A new cog `.tif` file is created for each timestep and band, so this code can take a while to run..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the data to aws\n",
    "\n",
    "### Add your aws credentials to raijin:\n",
    "\n",
    "In order to run aws commands from within raijin, you will need to have your credentials and configuration files set up in your raijin home directory.\n",
    "\n",
    "Your credientials can be found by opening your AWS account, and selecting `IAM`. Select `Users` on the left-hand panel, then search for your u-number. Once you click on your u-number, you will see a box to `Add new security keys`. Select this, and then copy the access key and secret access key into the credentials file explained below.\n",
    "\n",
    "`cd ~`\n",
    "\n",
    "`mkdir .aws`\n",
    "\n",
    "`nano .aws/credentials`\n",
    "```\n",
    "[default]\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key=\n",
    "```\n",
    "*(Ctrl-O to save, Ctrl-X to exit)*\n",
    "\n",
    "`nano .aws/config`\n",
    "```\n",
    "[default]\n",
    "region=ap-southeast-2\n",
    "```\n",
    "*(Ctrl-O to save, Ctrl-X to exit)*\n",
    "\n",
    "### Run the PBS script to sync your data with aws\n",
    "\n",
    "The code for doing this can be found in the same [Geotiff-conversion repository](https://github.com/harshurampur/Geotiff-conversion/tree/master/compute-sync). Modify the `compute_sync.sh` code with your own username, and update last line the command to point to the two directories you want to sink;\n",
    "- the directory on NCI with the prepared data\n",
    "- the folder on S3 where you want the data to go\n",
    "\n",
    "### Hit go!\n",
    "\n",
    "Once you have set up your aws credentials, and modified `compute_sync.sh`, submit this script to PBS (either from within the `Geotiff-conversion/compute-sync` folder, or from anywhere on raijin with the full or relative path to the `compute_sync.sh` file included):\n",
    "\n",
    "`qsub compute_sync.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
